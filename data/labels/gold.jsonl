{"id": "ae66f586c6a6eab1", "title": "Unrolling the Codex agent loop", "description": "A technical deep dive into the Codex agent loop, explaining how Codex CLI orchestrates models, tools, prompts, and performance using the Responses API.", "source_url": "https://openai.com/blog/rss.xml", "article_url": "https://openai.com/index/unrolling-the-codex-agent-loop", "pub_date": "2026-01-23T12:00:00+00:00", "label": true, "category": "ai_ml", "notes": "manual: OpenAI model/agent research announcement."}
{"id": "5fcfc6652ceac921", "title": "Wilson Lin on FastRender: a browser built by thousands of parallel agents", "description": "<p>Last week Cursor published <a href=\"https://cursor.com/blog/scaling-agents\">Scaling long-running autonomous coding</a>, an article describing their research efforts into coordinating large numbers of autonomous coding agents. One of the projects mentioned in the article was <a href=\"https://github.com/wilsonzlin/fastrender\">FastRender</a>, a web browser they built from scratch using their agent swarms. I wanted to learn more so I asked Wilson Lin, the engineer behind FastRender, if we could record a conversation about the project. That 47 minute video is <a href=\"https://www.youtube.com/watch?v=bKrAcTf2pL4\">now available on YouTube</a>. I've included some of the highlights below.</p>\n\n \n\n<p>See my <a href=\"https://simonwillison.net/2026/Jan/19/scaling-long-running-autonomous-coding/\">previous post</a> for my notes and screenshots from trying out FastRender myself.</p>\n\n\n<h4 id=\"what-fastrender-can-do-right-now\">What FastRender can do right now</h4>\n<p>We started the conversation with a demo of FastRender loading different pages (<a href=\"https://www.youtube.com/watch?v=bKrAcTf2pL4&amp;t=195s\">03:15</a>). The JavaScript engine isn't working yet so we instead loaded <a href=\"https://github.com/wilsonzlin/fastrender\">github.com/wilsonzlin/fastrender</a>, <a href=\"https://en.wikipedia.org/\">Wikipedia</a> and <a href=\"https://cnn.com\">CNN</a> - all of which were usable, if a little slow to display.</p>\n<p>JavaScript had been disabled by one of the agents, which decided to add a feature flag! <a href=\"https://www.youtube.com/watch?v=bKrAcTf2pL4&amp;t=242s\">04:02</a></p>\n<blockquote>\n<p>JavaScript is disabled right now. The agents made a decision as they were currently still implementing the engine and making progress towards other parts... they decided to turn it off or put it behind a feature flag, technically.</p>\n</blockquote>\n<h4 id=\"from-side-project-to-core-research\">From side-project to core research</h4>\n<p>Wilson started what become FastRender as a personal side-project to explore the capabilities of the latest generation of frontier models - Claude Opus 4.5, GPT-5.1, and GPT-5.2. <a href=\"https://www.youtube.com/watch?v=bKrAcTf2pL4&amp;t=56s\">00:56</a></p>\n<blockquote>\n<p>FastRender was a personal project of mine from, I'd say, November. It was an experiment to see how well frontier models like Opus 4.5 and back then GPT-5.1 could do with much more complex, difficult tasks.</p>\n</blockquote>\n<p>A browser rendering engine was the ideal choice for this, because it's both <em>extremely</em> ambitious and complex but also well specified. And you can visually see how well it's working! <a href=\"https://www.youtube.com/watch?v=bKrAcTf2pL4&amp;t=117s\">01:57</a></p>\n<blockquote>\n<p>As that experiment progressed, I was seeing better and better results from single agents that were able to actually make good progress on this project. And at that point, I wanted to see, well, what's the next level? How do I push this even further?</p>\n</blockquote>\n<p>Once it became clear that this was an opportunity to try multiple agents working together it graduated to an official Cursor research project, and available resources were amplified.</p>\n<p>The goal of FastRender was never to build a browser to compete with the likes of Chrome. <a href=\"https://www.youtube.com/watch?v=bKrAcTf2pL4&amp;t=2512s\">41:52</a></p>\n<blockquote>\n<p>We never intended for it to be a production software or usable, but we wanted to observe behaviors of this harness of multiple agents, to see how they could work at scale.</p>\n</blockquote>\n<p>The great thing about a browser is that it has such a large scope that it can keep serving experiments in this space for many years to come. JavaScript, then WebAssembly, then WebGPU... it could take many years to run out of new challenges for the agents to tackle.</p>\n<h4 id=\"running-thousands-of-agents-at-once\">Running thousands of agents at once</h4>\n<p>The most interesting thing about FastRender is the way the project used multiple agents working in parallel to build different parts of the browser. I asked how many agents were running at once: <a href=\"https://www.youtube.com/watch?v=bKrAcTf2pL4&amp;t=324s\">05:24</a></p>\n<blockquote>\n<p>At the peak, when we had the stable system running for one week continuously, there were approximately 2,000 agents running concurrently at one time. And they were making, I believe, thousands of commits per hour.</p>\n</blockquote>\n<p>The project has <a href=\"https://github.com/wilsonzlin/fastrender/commits/main/\">nearly 30,000 commits</a>!</p>\n<p>How do you run 2,000 agents at once? They used <em>really big machines</em>. <a href=\"https://www.youtube.com/watch?v=bKrAcTf2pL4&amp;t=356s\">05:56</a></p>\n<blockquote>\n<p>The simple approach we took with the infrastructure was to have a large machine run one of these multi-agent harnesses. Each machine had ample resources, and it would run about 300 agents concurrently on each. This was able to scale and run reasonably well, as agents spend a lot of time thinking, and not just running tools.</p>\n</blockquote>\n<p>At this point we switched to a live demo of the harness running on one of those big machines (<a href=\"https://www.youtube.com/watch?v=bKrAcTf2pL4&amp;t=392s\">06:32</a>). The agents are arranged in a tree structure, with planning agents firing up tasks and worker agents then carrying them out. <a href=\"https://www.youtube.com/watch?v=bKrAcTf2pL4&amp;t=434s\">07:14</a></p>\n<p><img alt=\"Terminal window showing a tmux session running &quot;grind-swarm&quot; task manager with RUNNING status. Header shows &quot;grind-swarm – 45:54:15&quot; with stats &quot;planners: 9 (0 done) | tasks: 111 working, 0 pending, 232 done | 12900.9M↑ 514.1M↓&quot;. Task list includes: p1 Root (main), p2 CSS selector matching performance + bloom filter integration, p3 CSS stylesheet parsing semantics &amp; at-rule handling, p4 Custom properties (@property) + var() resolution + incremental recompute/invalidation, p37 CSS at-rule artifact integration, p50 Selector engine correctness &amp; spec coverage, p51 Computed-value + property coverage across css-cascade, p105 Style sharing / computed style caching in fastrender-style, p289 CSS cascade layers (@layer) global ordering, w5 Fix workspace lockfile drift, w7 Implement computed-style snapshot sharing, w15 Fix css-properties namespace handling, w17 (Stretch) Enable bloom fast-reject in HTML quirks mode, w18 Refactor css-properties stylesheet parsing. Activity log shows shell commands including cargo check, git status, git push origin main, and various test runs. Bottom status bar shows &quot;grind-css0:target/release/grind-swarm*&quot; and &quot;streamyard.com is sharing your screen&quot; notification with timestamp &quot;12:02 22-Jan-26&quot;.\" src=\"https://static.simonwillison.net/static/2026/wilson-lin-agents.jpg\" /></p>\n<blockquote>\n<p>This cluster of agents is working towards building out the CSS aspects of the browser, whether that's parsing, selector engine, those features. We managed to push this even further by splitting out the browser project into multiple instructions or work streams and have each one run one of these harnesses on their own machine, so that was able to further parallelize and increase throughput.</p>\n</blockquote>\n<p>But don't all of these agents working on the same codebase result in a huge amount of merge conflicts? Apparently not: <a href=\"https://www.youtube.com/watch?v=bKrAcTf2pL4&amp;t=501s\">08:21</a></p>\n<blockquote>\n<p>We've noticed that most commits do not have merge conflicts. The reason is the harness itself is able to quite effectively split out and divide the scope and tasks such that it tries to minimize the amount of overlap of work. That's also reflected in the code structure—commits will be made at various times and they don't tend to touch each other at the same time.</p>\n</blockquote>\n<p>This appears to be the key trick for unlocking benefits from parallel agents: if planning agents do a good enough job of breaking up the work into non-overlapping chunks you can bring hundreds or even thousands of agents to bear on a problem at once.</p>\n<p>Surprisingly, Wilson found that GPT-5.1 and GPT-5.2 were a better fit for this work than the coding specialist GPT-5.1-Codex: <a href=\"https://www.youtube.com/watch?v=bKrAcTf2pL4&amp;t=1048s\">17:28</a></p>\n<blockquote>\n<p>Some initial findings were that the instructions here were more expansive than merely coding. For example, how to operate and interact within a harness, or how to operate autonomously without interacting with the user or having a lot of user feedback. These kinds of instructions we found worked better with the general models.</p>\n</blockquote>\n<p>I asked what the longest they've seen this system run without human intervention: <a href=\"https://www.youtube.com/watch?v=bKrAcTf2pL4&amp;t=1108s\">18:28</a></p>\n<blockquote>\n<p>So this system, once you give an instruction, there's actually no way to steer it, you can't prompt it, you're going to adjust how it goes. The only thing you can do is stop it. So our longest run, all the runs are basically autonomous. We don't alter the trajectory while executing. [...]</p>\n<p>And so the longest at the time of the post was about a week and that's pretty close to the longest. Of course the research project itself was only about three weeks so you know we probably can go longer.</p>\n</blockquote>\n<h4 id=\"specifications-and-feedback-loops\">Specifications and feedback loops</h4>\n<p>An interesting aspect of this project design is feedback loops. For agents to work autonomously for long periods of time they need as much useful context about the problem they are solving as possible, combined with effective feedback loops to help them make decisions.</p>\n<p>The FastRender repo <a href=\"https://github.com/wilsonzlin/fastrender/tree/19bf1036105d4eeb8bf3330678b7cb11c1490bdc/specs\">uses git submodules to include relevant specifications</a>, including csswg-drafts, tc39-ecma262 for JavaScript, whatwg-dom, whatwg-html and more. <a href=\"https://www.youtube.com/watch?v=bKrAcTf2pL4&amp;t=846s\">14:06</a></p>\n<blockquote>\n<p>Feedback loops to the system are very important. Agents are working for very long periods continuously, and without guardrails and feedback to know whether what they're doing is right or wrong it can have a big impact over a long rollout. Specs are definitely an important part—you can see lots of comments in the code base that AI wrote referring specifically to specs that they found in the specs submodules.</p>\n</blockquote>\n<p>GPT-5.2 is a vision-capable model, and part of the feedback loop for FastRender included taking screenshots of the rendering results and feeding those back into the model:\n<a href=\"https://www.youtube.com/watch?v=bKrAcTf2pL4&amp;t=983s\">16:23</a></p>\n<blockquote>\n<p>In the earlier evolution of this project, when it was just doing the static renderings of screenshots, this was definitely a very explicit thing we taught it to do. And these models are visual models, so they do have that ability. We have progress indicators to tell it to compare the diff against a golden sample.</p>\n</blockquote>\n<p>The strictness of the Rust compiler helped provide a feedback loop as well: <a href=\"https://www.youtube.com/watch?v=bKrAcTf2pL4&amp;t=952s\">15:52</a></p>\n<blockquote>\n<p>The nice thing about Rust is you can get a lot of verification just from compilation, and that is not as available in other languages.</p>\n</blockquote>\n<h4 id=\"the-agents-chose-the-dependencies\">The agents chose the dependencies</h4>\n<p>We talked about the <a href=\"https://github.com/wilsonzlin/fastrender/blob/19bf1036105d4eeb8bf3330678b7cb11c1490bdc/Cargo.toml\">Cargo.toml dependencies</a> that the project had accumulated, almost all of which had been selected by the agents themselves.</p>\n<p>Some of these, like <a href=\"https://skia.org/\">Skia</a> for 2D graphics rendering or <a href=\"https://github.com/harfbuzz/harfbuzz\">HarfBuzz</a> for text shaping, were obvious choices. Others such as <a href=\"https://github.com/DioxusLabs/taffy\">Taffy</a> felt like they might go against the from-scratch goals of the project, since that library implements CSS flexbox and grid layout algorithms directly. This was not an intended outcome. <a href=\"https://www.youtube.com/watch?v=bKrAcTf2pL4&amp;t=1673s\">27:53</a></p>\n<blockquote>\n<p>Similarly these are dependencies that the agent picked to use for small parts of the engine and perhaps should have actually implemented itself. I think this reflects on the importance of the instructions, because I actually never encoded specifically the level of dependencies we should be implementing ourselves.</p>\n</blockquote>\n<p>The agents vendored in Taffy and <a href=\"https://github.com/wilsonzlin/fastrender/commits/main/vendor/taffy\">applied a stream of changes</a> to that vendored copy.\n<a href=\"https://www.youtube.com/watch?v=bKrAcTf2pL4&amp;t=1878s\">31:18</a></p>\n<blockquote>\n<p>It's currently vendored. And as the agents work on it, they do make changes to it. This was actually an artifact from the very early days of the project before it was a fully fledged browser... it's implementing things like the flex and grid layers, but there are other layout methods like inline, block, and table, and in our new experiment, we're removing that completely.</p>\n</blockquote>\n<p>The inclusion of QuickJS despite the presence of a home-grown ecma-rs implementation has a fun origin story:\n<a href=\"https://www.youtube.com/watch?v=bKrAcTf2pL4&amp;t=2115s\">35:15</a></p>\n<blockquote>\n<p>I believe it mentioned that it pulled in the QuickJS because it knew that other agents were working on the JavaScript engine, and it needed to unblock itself quickly. [...]</p>\n<p>It was like, eventually, once that's finished, let's remove it and replace with the proper engine.</p>\n</blockquote>\n<p>I love how similar this is to the dynamics of a large-scale human engineering team, where you could absolutely see one engineer getting frustrated at another team not having delivered yet and unblocking themselves by pulling in a third-party library.</p>\n<h4 id=\"intermittent-errors-are-ok-actually\">Intermittent errors are OK, actually</h4>\n<p>Here's something I found really surprising: the agents were allowed to introduce small errors into the codebase as they worked! <a href=\"https://www.youtube.com/watch?v=bKrAcTf2pL4&amp;t=2382s\">39:42</a></p>\n<blockquote>\n<p>One of the trade-offs was: if you wanted every single commit to be a hundred percent perfect, make sure it can always compile every time, that might be a synchronization bottleneck. [...]</p>\n<p>Especially as you break up the system into more modularized aspects, you can see that errors get introduced, but small errors, right? An API change or some syntax error, but then they get fixed really quickly after a few commits. So there's a little bit of slack in the system to allow these temporary errors so that the overall system can continue to make progress at a really high throughput. [...]</p>\n<p>People may say, well, that's not correct code. But it's not that the errors are accumulating. It's a stable rate of errors. [...] That seems like a worthwhile trade-off.</p>\n</blockquote>\n<p>If you're going to have thousands of agents working in parallel optimizing for throughput over correctness turns out to be a strategy worth exploring.</p>\n<h4 id=\"a-single-engineer-plus-a-swarm-of-agents-in-january-2026\">A single engineer plus a swarm of agents in January 2026</h4>\n<p>The thing I find most interesting about FastRender is how it demonstrates the extreme edge of what a single engineer can achieve in early 2026 with the assistance of a swarm of agents.</p>\n<p>FastRender may not be a production-ready browser, but it represents over a million lines of Rust code, written in a few weeks, that can already render real web pages to a usable degree.</p>\n<p>A browser really is the ideal research project to experiment with this new, weirdly shaped form of software engineering.</p>\n<p>I asked Wilson how much mental effort he had invested in browser rendering compared to agent co-ordination. <a href=\"https://www.youtube.com/watch?v=bKrAcTf2pL4&amp;t=694s\">11:34</a></p>\n<blockquote>\n<p>The browser and this project were co-developed and very symbiotic, only because the browser was a very useful objective for us to measure and iterate the progress of the harness. The goal was to iterate on and research the multi-agent harness—the browser was just the research example or objective.</p>\n</blockquote>\n<p>FastRender is effectively using a full browser rendering engine as a \"hello world\" exercise for multi-agent coordination!</p>\n    \n        <p>Tags: <a href=\"https://simonwillison.net/tags/browsers\">browsers</a>, <a href=\"https://simonwillison.net/tags/youtube\">youtube</a>, <a href=\"https://simonwillison.net/tags/ai\">ai</a>, <a href=\"https://simonwillison.net/tags/generative-ai\">generative-ai</a>, <a href=\"https://simonwillison.net/tags/llms\">llms</a>, <a href=\"https://simonwillison.net/tags/ai-assisted-programming\">ai-assisted-programming</a>, <a href=\"https://simonwillison.net/tags/coding-agents\">coding-agents</a>, <a href=\"https://simonwillison.net/tags/cursor\">cursor</a>, <a href=\"https://simonwillison.net/tags/parallel-agents\">parallel-agents</a></p>", "source_url": "https://simonwillison.net/atom/everything/", "article_url": "https://simonwillison.net/2026/Jan/23/fastrender/#atom-everything", "pub_date": "2026-01-23T21:26:10+00:00", "label": false, "category": "other", "notes": "manual: Small project highlight; not a major release."}
{"id": "e553391e9dff8f8d", "title": "FLUX.2-klein-4B Pure C Implementation", "description": "<p><strong><a href=\"https://github.com/antirez/flux2.c\">FLUX.2-klein-4B Pure C Implementation</a></strong></p>\nOn 15th January Black Forest Labs, a lab formed by the creators of the original Stable Diffusion, released <a href=\"https://huggingface.co/black-forest-labs/FLUX.2-klein-4B\">black-forest-labs/FLUX.2-klein-4B</a> - an Apache 2.0 licensed 4 billion parameter version of their FLUX.2 family.</p>\n<p>Salvatore Sanfilippo (antirez) decided to build a pure C and dependency-free implementation to run the model, with assistance from Claude Code and Claude Opus 4.5.</p>\n<p>Salvatore shared <a href=\"https://news.ycombinator.com/item?id=46670279#46671233\">this note</a> on Hacker News:</p>\n<blockquote>\n<p>Something that may be interesting for the reader of this thread: this project was possible only once I started to tell Opus that it <em>needed</em> to take a file with all the implementation notes, and also accumulating all the things we discovered during the development process. And also, the file had clear instructions to be taken updated, and to be processed ASAP after context compaction. This kinda enabled Opus to do such a big coding task in a reasonable amount of time without loosing track. Check the file IMPLEMENTATION_NOTES.md in the GitHub repo for more info.</p>\n</blockquote>\n<p>Here's that <a href=\"https://github.com/antirez/flux2.c/blob/main/IMPLEMENTATION_NOTES.md\">IMPLEMENTATION_NOTES.md</a> file.\n\n    <p><small></small>Via <a href=\"https://news.ycombinator.com/item?id=46670279\">Hacker News</a></small></p>\n\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/c\">c</a>, <a href=\"https://simonwillison.net/tags/salvatore-sanfilippo\">salvatore-sanfilippo</a>, <a href=\"https://simonwillison.net/tags/ai\">ai</a>, <a href=\"https://simonwillison.net/tags/stable-diffusion\">stable-diffusion</a>, <a href=\"https://simonwillison.net/tags/generative-ai\">generative-ai</a>, <a href=\"https://simonwillison.net/tags/llms\">llms</a>, <a href=\"https://simonwillison.net/tags/ai-assisted-programming\">ai-assisted-programming</a>, <a href=\"https://simonwillison.net/tags/text-to-image\">text-to-image</a>, <a href=\"https://simonwillison.net/tags/coding-agents\">coding-agents</a>, <a href=\"https://simonwillison.net/tags/claude-code\">claude-code</a></p>", "source_url": "https://simonwillison.net/atom/everything/", "article_url": "https://simonwillison.net/2026/Jan/18/flux2-klein-4b/#atom-everything", "pub_date": "2026-01-18T23:58:58+00:00", "label": false, "category": "other", "notes": "manual: Niche implementation release."}
{"id": "3398ec2692ec54af", "title": "The Product-Minded Engineer: The importance of good errors and warnings", "description": "Product engineers are more in demand than ever, but how do you become one? New book, &#8220;The Product-Minded Engineer&#8221;, offers a guide. An interview with its author and an exclusive excerpt", "source_url": "https://newsletter.pragmaticengineer.com/feed", "article_url": "https://newsletter.pragmaticengineer.com/p/the-product-minded-engineer", "pub_date": "2026-01-20T17:31:16+00:00", "label": false, "category": "other", "notes": "manual: Soft skills/productivity article."}
{"id": "2bf44e703bf8ff82", "title": "Introducing Edu for Countries", "description": "Edu for Countries is a new OpenAI initiative helping governments use AI to modernize education systems and build future-ready workforces.", "source_url": "https://openai.com/blog/rss.xml", "article_url": "https://openai.com/index/edu-for-countries", "pub_date": "2026-01-21T01:00:00+00:00", "label": true, "category": "policy", "notes": "manual: Program launch aimed at governments."}
{"id": "a75c99e63ba96b6d", "title": "Qwen3-TTS Family is Now Open Sourced: Voice Design, Clone, and Generation", "description": "<p><strong><a href=\"https://qwen.ai/blog?id=qwen3tts-0115\">Qwen3-TTS Family is Now Open Sourced: Voice Design, Clone, and Generation</a></strong></p>\nI haven't been paying much attention to the state-of-the-art in speech generation models other than noting that they've got <em>really good</em>, so I can't speak for how notable this new release from Qwen is.</p>\n<p>From <a href=\"https://github.com/QwenLM/Qwen3-TTS/blob/main/assets/Qwen3_TTS.pdf\">the accompanying paper</a>:</p>\n<blockquote>\n<p>In this report, we present the Qwen3-TTS series, a family of advanced multilingual, controllable, robust, and streaming text-to-speech models. Qwen3-TTS supports state-of- the-art 3-second voice cloning and description-based control, allowing both the creation of entirely novel voices and fine-grained manipulation over the output speech. Trained on over 5 million hours of speech data spanning 10 languages, Qwen3-TTS adopts a dual-track LM architecture for real-time synthesis [...]. Extensive experiments indicate state-of-the-art performance across diverse objective and subjective benchmark (e.g., TTS multilingual test set, InstructTTSEval, and our long speech test set). To facilitate community research and development, we release both tokenizers and models under the Apache 2.0 license.</p>\n</blockquote>\n<p>To give an idea of size, <a href=\"https://huggingface.co/Qwen/Qwen3-TTS-12Hz-1.7B-Base\">Qwen/Qwen3-TTS-12Hz-1.7B-Base</a> is 4.54GB on Hugging Face and <a href=\"https://huggingface.co/Qwen/Qwen3-TTS-12Hz-0.6B-Base\">Qwen/Qwen3-TTS-12Hz-0.6B-Base</a> is 2.52GB.</p>\n<p>The <a href=\"https://huggingface.co/spaces/Qwen/Qwen3-TTS\">Hugging Face demo</a> lets you try out the 0.6B and 1.7B models for free in your browser, including voice cloning:</p>\n<p><img alt=\"Screenshot of a Qwen3-TTS voice cloning web interface with three tabs at top: &quot;Voice Design&quot;, &quot;Voice Clone (Base)&quot; (selected), and &quot;TTS (CustomVoice)&quot;. The page is titled &quot;Clone Voice from Reference Audio&quot; and has two main sections. Left section: &quot;Reference Audio (Upload a voice sample clone)&quot; showing an audio waveform player at 0:00/0:34 with playback controls, upload and microphone icons, followed by &quot;Reference Text (Transcript of the reference audio)&quot; containing three paragraphs: &quot;Simon Willison is the creator of Datasette, an open source tool for exploring and publishing data. He currently works full-time building open source tools for data journalism, built around Datasette and SQLite. Prior to becoming an independent open source developer, Simon was an engineering director at Eventbrite. Simon joined Eventbrite through their acquisition of Lanyrd, a Y Combinator funded company he co-founded in 2010. He is a co-creator of the Django Web Framework, and has been blogging about web development and programming since 2002 at simonwillison.net&quot;. Right section: &quot;Target Text (Text to synthesize with cloned voice)&quot; containing text about Qwen3-TTS speech generation capabilities, with &quot;Language&quot; dropdown set to &quot;Auto&quot; and &quot;Model Size&quot; dropdown set to &quot;1.7B&quot;, and a purple &quot;Clone &amp; Generate&quot; button at bottom.\" src=\"https://static.simonwillison.net/static/2026/qwen-voice-clone.jpg\" /></p>\n<p>I tried this out by recording myself reading <a href=\"https://simonwillison.net/about/\">my about page</a> and then having Qwen3-TTS generate audio of me reading the Qwen3-TTS announcement post. Here's the result:</p>\n<p><audio controls=\"controls\" style=\"width: 100%;\">\n  <source src=\"https://static.simonwillison.net/static/2026/qwen-tts-clone.wav\" type=\"audio/wav\" />\n  Your browser does not support the audio element.\n</audio></p>\n<p>It's important that everyone understands that voice cloning is now something that's available to anyone with a GPU and a few GBs of VRAM... or in this case a web browser that can access Hugging Face.</p>\n<p><strong>Update</strong>: Prince Canuma <a href=\"https://x.com/Prince_Canuma/status/2014453857019904423\">got this working</a> with his <a href=\"https://pypi.org/project/mlx-audio/\">mlx-audio</a> library. I <a href=\"https://claude.ai/share/2e01ad60-ca38-4e14-ab60-74eaa45b2fbd\">had Claude</a> turn that into <a href=\"https://github.com/simonw/tools/blob/main/python/q3_tts.py\">a CLI tool</a> which you can run with <code>uv</code> ike this:</p>\n<pre><code>uv run https://tools.simonwillison.net/python/q3_tts.py \\\n  'I am a pirate, give me your gold!' \\\n  -i 'gruff voice' -o pirate.wav\n</code></pre>\n<p>The <code>-i</code> option lets you use a prompt to describe the voice it should use. On first run this downloads a 4.5GB model file from Hugging Face.\n\n    <p><small></small>Via <a href=\"https://news.ycombinator.com/item?id=46719229\">Hacker News</a></small></p>\n\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/text-to-speech\">text-to-speech</a>, <a href=\"https://simonwillison.net/tags/ai\">ai</a>, <a href=\"https://simonwillison.net/tags/generative-ai\">generative-ai</a>, <a href=\"https://simonwillison.net/tags/hugging-face\">hugging-face</a>, <a href=\"https://simonwillison.net/tags/uv\">uv</a>, <a href=\"https://simonwillison.net/tags/qwen\">qwen</a>, <a href=\"https://simonwillison.net/tags/mlx\">mlx</a>, <a href=\"https://simonwillison.net/tags/prince-canuma\">prince-canuma</a>, <a href=\"https://simonwillison.net/tags/ai-in-china\">ai-in-china</a></p>", "source_url": "https://simonwillison.net/atom/everything/", "article_url": "https://simonwillison.net/2026/Jan/22/qwen3-tts/#atom-everything", "pub_date": "2026-01-22T17:42:34+00:00", "label": true, "category": "ai_ml", "notes": "manual: Open-source TTS model family release."}
{"id": "66e11f2f40814d37", "title": "SSH has no Host header", "description": "<p><strong><a href=\"https://blog.exe.dev/ssh-host-header\">SSH has no Host header</a></strong></p>\n<a href=\"https://exe.dev/\">exe.dev</a> is a new hosting service that, for $20/month, gives you up to 25 VMs \"that share 2 CPUs and 8GB RAM\". Everything happens over SSH, including creating new VMs. Once configured you can sign into your exe.dev VMs like this:</p>\n<pre><code>ssh simon.exe.dev\n</code></pre>\n<p>Here's the clever bit: when you run the above command <code>exe.dev</code> signs you into your VM of that name... but they don't assign every VM its own IP address and SSH has no equivalent of the Host header, so how does their load balancer know <em>which</em> of your VMs to forward you on to?</p>\n<p>The answer is that while they don't assign a unique IP to every VM they <em>do</em> have enough IPs that they can ensure each of your VMs has an IP that is unique to your account.</p>\n<p>If I create two VMs they will each resolve to a separate IP address, each of which is shared with many other users. The underlying infrastructure then identifies my user account from my SSH public key and can determine which underlying VM to forward my SSH traffic to.\n\n    <p><small></small>Via <a href=\"https://lobste.rs/s/7oqiqi/ssh_has_no_host_header\">lobste.rs</a></small></p>\n\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/dns\">dns</a>, <a href=\"https://simonwillison.net/tags/hosting\">hosting</a>, <a href=\"https://simonwillison.net/tags/ssh\">ssh</a></p>", "source_url": "https://simonwillison.net/atom/everything/", "article_url": "https://simonwillison.net/2026/Jan/22/ssh-has-no-host-header/#atom-everything", "pub_date": "2026-01-22T23:57:50+00:00", "label": false, "category": "other", "notes": "manual: Technical note; limited scope."}
{"id": "07c6b2cfd9a76cc4", "title": "AssetOpsBench: Bridging the Gap Between AI Agent Benchmarks and Industrial Reality", "description": "", "source_url": "https://huggingface.co/blog/feed.xml", "article_url": "https://huggingface.co/blog/ibm-research/assetopsbench-playground-on-hugging-face", "pub_date": "2026-01-21T06:25:31+00:00", "label": true, "category": "research", "notes": "manual: New benchmark/research release."}
{"id": "9e260bc7581d610d", "title": "Electricity use of AI coding agents", "description": "<p><strong><a href=\"https://www.simonpcouch.com/blog/2026-01-20-cc-impact/\">Electricity use of AI coding agents</a></strong></p>\nPrevious work estimating the energy and water cost of LLMs has generally focused on the cost per prompt using a consumer-level system such as ChatGPT.</p>\n<p>Simon P. Couch notes that coding agents such as Claude Code use <em>way</em> more tokens in response to tasks, often burning through many thousands of tokens of many tool calls.</p>\n<p>As a heavy Claude Code user, Simon estimates his own usage at the equivalent of 4,400 \"typical queries\" to an LLM, for an equivalent of around $15-$20 in daily API token spend. He figures that to be about the same as running a dishwasher once or the daily energy used by a domestic refrigerator.\n\n    <p><small></small>Via <a href=\"https://news.ycombinator.com/item?id=46695415\">Hacker News</a></small></p>\n\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/ai\">ai</a>, <a href=\"https://simonwillison.net/tags/generative-ai\">generative-ai</a>, <a href=\"https://simonwillison.net/tags/llms\">llms</a>, <a href=\"https://simonwillison.net/tags/ai-ethics\">ai-ethics</a>, <a href=\"https://simonwillison.net/tags/ai-energy-usage\">ai-energy-usage</a>, <a href=\"https://simonwillison.net/tags/coding-agents\">coding-agents</a>, <a href=\"https://simonwillison.net/tags/claude-code\">claude-code</a></p>", "source_url": "https://simonwillison.net/atom/everything/", "article_url": "https://simonwillison.net/2026/Jan/20/electricity-use-of-ai-coding-agents/#atom-everything", "pub_date": "2026-01-20T23:11:57+00:00", "label": false, "category": "other", "notes": "manual: Exploratory discussion; not a major event."}
{"id": "822840c69cd95145", "title": "Fragments: January 22", "description": "<p>My colleagues here at Thoughtworks have announced <a href=\"https://www.thoughtworks.com/ai/works\">AI/works™</a>, a platform for our work using AI-enabled software development. The platform is in its early days, and is currently intended to support Thoughtworks consultants in their client work. I’m looking forward to sharing what we learn from using and further developing the platform in future months.</p>\n\n<p> ❄                ❄                ❄                ❄                ❄</p>\n\n<p>Simon Couch <a href=\"https://www.simonpcouch.com/blog/2026-01-20-cc-impact/\">examines the electricity consumption</a> of using AI. He’s a heavy user: “usually programming for a few hours, and driving 2 or 3 Claude Code instances at a time”. He finds his usage of electricity is orders of magnitude more than typical estimates based on the “typical query”.</p>\n\n<blockquote>\n  <p>On a median day, I estimate I consume 1,300 Wh through Claude Code—4,400 “typical queries” worth.</p>\n</blockquote>\n\n<p>But it’s still not a massive amount of power - similar to that of running a dishwasher.</p>\n\n<p>A caveat to this is that this is “napkin math” because we don’t have decent data about how these models use resources. I agree with him that we ought to.</p>\n\n<p> ❄                ❄                ❄                ❄                ❄</p>\n\n<p>My namesake Chad Fowler (no relation) considers that the movement to agentic coding creates a similar <a href=\"https://aicoding.leaflet.pub/3mbrvhyye4k2e\">shift in rigor and discipline</a> as appeared in Extreme Programming, dynamic languages, and continuous deployment.</p>\n\n<p>In Extreme Programming’s case, this meant a lot of discipline around testing, continuous integration, and keeping the code-base healthy. My current view is that with AI-enabled development we need to be rigorous about evaluating the software, both for its observable behavior and its internal quality.</p>\n\n<blockquote>\n  <p>The engineers who thrive in this environment will be the ones who relocate discipline rather than abandon it. They’ll treat generation as a capability that demands more precision in specification, not less. They’ll build evaluation systems that are harder to fool than the ones they replaced. They’ll refuse the temptation to mistake velocity for progress.</p>\n</blockquote>\n\n<p> ❄                ❄                ❄                ❄                ❄</p>\n\n<p>There’s been much written about the dreadful events in Minnesota, and I’ve not felt I’ve had anything useful to add to them. But I do want to pass on an excellent post from <a href=\"https://www.noahpinion.blog/p/why-are-federal-agents-gunning-down\">Noah Smith</a> that captures many of my thoughts. He points out that there is a “consistent record of brutality, aggression, dubious legality, and unprofessionalism” from ICE (and CBP) who seem to be turning into MAGA’s <a href=\"https://en.wikipedia.org/wiki/Sturmabteilung\">SD</a>.</p>\n\n<blockquote>\n  <p>Is this America now? A country where unaccountable and poorly trained government agents go door to door, arresting and beating people on pure suspicion, and shooting people who don’t obey their every order or who try to get away? “When a federal officer gives you instructions, you abide by them and then you get to keep your life” is a perfect description of an authoritarian police state. None of this is Constitutional, every bit of it is deeply antithetical to the American values we grew up taking for granted.</p>\n</blockquote>\n\n<p>My worries about these kinds of developments were what animated me to urge against voting for Trump in the <a href=\"https://martinfowler.com/articles/vote-against-trump.html\">2016 election</a>. Mostly those worries didn’t come to fruition because enough constitutional Republicans were in a position to stop them from happening, so even when Trump attempted a coup in 2020, he wasn’t able to get very far. But now those constitutional Republicans are absent or quiescent. I fear that what we’ve seen in Minneapolis will be a harbinger of worse to come.</p>\n\n<p>I also second John Gruber’s <a href=\"https://daringfireball.net/2026/01/lets_call_a_murder_a_murder\">praise of bystander Caitlin Callenson</a>:</p>\n\n<blockquote>\n  <p>But then, after the murderous agent fired three shots — just 30 or 40 feet in front of Callenson — Callenson had the courage and conviction to stay with the scene and keep filming. Not to run away, but instead to follow the scene. To keep filming. To continue documenting with as best clarity as she could, what was unfolding.</p>\n</blockquote>\n\n<p>The recent activity in  Venezuala reminds me that I’ve long felt that Trump is a Hugo Chávez figure - a charismatic populist who’s keen on wrecking institutions and norms. Trump is old, so won’t be with us for that much longer - but the question is: “who is Trump’s Maduro?”</p>\n\n<p> ❄                ❄                ❄                ❄                ❄</p>\n\n<p>With all the drama at home, we shouldn’t ignore the terrible things that happened in Iran. The people there again suffered again the consequences of an entrenched authoritarian police state.</p>", "source_url": "https://martinfowler.com/feed.atom", "article_url": "https://martinfowler.com/fragments/2026-01-22.html", "pub_date": "2026-01-22T14:30:00+00:00", "label": false, "category": "other", "notes": "manual: Newsletter roundup; low impact."}
{"id": "9336d3eba565cf62", "title": "Don't \"Trust the Process\"", "description": "<p><strong><a href=\"https://www.youtube.com/watch?v=4u94juYwLLM\">Don&#x27;t &quot;Trust the Process&quot;</a></strong></p>\nJenny Wen, Design Lead at Anthropic (and previously Director of Design at Figma) gave a provocative keynote at Hatch Conference in Berlin last September.</p>\n<p><img alt=\"Don't &quot;Trust the process&quot; slide, speaker shown on the left\" src=\"https://static.simonwillison.net/static/2026/dont-trust-process.jpg\" /></p>\n<p>Jenny argues that the Design Process - user research leading to personas leading to user journeys leading to wireframes... all before anything gets built - may be outdated for today's world.</p>\n<blockquote>\n<p><strong>Hypothesis</strong>: In a world where anyone can make anything — what matters is your ability to choose and curate what you make.</p>\n</blockquote>\n<p>In place of the Process, designers should lean into prototypes. AI makes these much more accessible and less time-consuming than they used to be.</p>\n<p>Watching this talk made me think about how AI-assisted programming significantly reduces the cost of building the <em>wrong</em> thing. Previously if the design wasn't right you could waste months of development time building in the wrong direction, which was a very expensive mistake. If a wrong direction wastes just a few days instead we can take more risks and be much more proactive in exploring the problem space.</p>\n<p>I've always been a compulsive prototyper though, so this is very much playing into my own existing biases!\n\n    <p><small></small>Via <a href=\"https://twitter.com/jenny_wen/status/2014479445738893649\">@jenny_wen</a></small></p>\n\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/design\">design</a>, <a href=\"https://simonwillison.net/tags/prototyping\">prototyping</a>, <a href=\"https://simonwillison.net/tags/ai\">ai</a>, <a href=\"https://simonwillison.net/tags/generative-ai\">generative-ai</a>, <a href=\"https://simonwillison.net/tags/llms\">llms</a>, <a href=\"https://simonwillison.net/tags/ai-assisted-programming\">ai-assisted-programming</a>, <a href=\"https://simonwillison.net/tags/vibe-coding\">vibe-coding</a></p>", "source_url": "https://simonwillison.net/atom/everything/", "article_url": "https://simonwillison.net/2026/Jan/24/dont-trust-the-process/#atom-everything", "pub_date": "2026-01-24T23:31:03+00:00", "label": false, "category": "other", "notes": "manual: Opinion-style blog post."}
{"id": "88e39f15677c8138", "title": "Claude's new constitution", "description": "<p><strong><a href=\"https://www.anthropic.com/news/claude-new-constitution\">Claude&#x27;s new constitution</a></strong></p>\nLate last year Richard Weiss <a href=\"https://www.lesswrong.com/posts/vpNG99GhbBoLov9og/claude-4-5-opus-soul-document\">found something interesting</a> while poking around with the just-released Claude Opus 4.5: he was able to talk the model into regurgitating a document which was <em>not</em> part of the system prompt but appeared instead to be baked in during training, and which described Claude's core values at great length.</p>\n<p>He called this leak the <strong>soul document</strong>, and Amanda Askell from Anthropic <a href=\"https://simonwillison.net/2025/Dec/2/claude-soul-document/\">quickly confirmed</a> that it was indeed part of Claude's training procedures.</p>\n<p>Today Anthropic made this official, <a href=\"https://www.anthropic.com/news/claude-new-constitution\">releasing that full \"constitution\" document</a> under a CC0 (effectively public domain) license. There's a lot to absorb! It's over 35,000 tokens, more than 10x the length of the <a href=\"https://platform.claude.com/docs/en/release-notes/system-prompts#claude-opus-4-5\">published Opus 4.5 system prompt</a>.</p>\n<p>One detail that caught my eye is the acknowledgements at the end, which include a list of <a href=\"https://www.anthropic.com/constitution#acknowledgements\">external contributors</a> who helped review the document. I was intrigued to note that two of the fifteen listed names are Catholic members of the clergy - <a href=\"https://www.frbrendanmcguire.org/biography\">Father Brendan McGuire</a> is a pastor in Los Altos with a Master’s degree in Computer Science and Math and <a href=\"https://en.wikipedia.org/wiki/Paul_Tighe\">Bishop Paul Tighe</a> is an Irish Catholic bishop with a background in moral theology.\n\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/ai\">ai</a>, <a href=\"https://simonwillison.net/tags/generative-ai\">generative-ai</a>, <a href=\"https://simonwillison.net/tags/llms\">llms</a>, <a href=\"https://simonwillison.net/tags/anthropic\">anthropic</a>, <a href=\"https://simonwillison.net/tags/claude\">claude</a>, <a href=\"https://simonwillison.net/tags/amanda-askell\">amanda-askell</a>, <a href=\"https://simonwillison.net/tags/ai-ethics\">ai-ethics</a>, <a href=\"https://simonwillison.net/tags/ai-personality\">ai-personality</a></p>", "source_url": "https://simonwillison.net/atom/everything/", "article_url": "https://simonwillison.net/2026/Jan/21/claudes-new-constitution/#atom-everything", "pub_date": "2026-01-21T23:39:49+00:00", "label": true, "category": "ai_ml", "notes": "manual: Major model governance/policy update."}
{"id": "d5d963b8c07abecb", "title": "jordanhubbard/nanolang", "description": "<p><strong><a href=\"https://github.com/jordanhubbard/nanolang\">jordanhubbard/nanolang</a></strong></p>\nPlenty of people have mused about what a new programming language specifically designed to be used by LLMs might look like. Jordan Hubbard (<a href=\"https://en.wikipedia.org/wiki/Jordan_Hubbard\">co-founder of FreeBSD</a>, with serious stints at Apple and NVIDIA) just released exactly that.</p>\n<blockquote>\n<p>A minimal, LLM-friendly programming language with mandatory testing and unambiguous syntax.</p>\n<p>NanoLang transpiles to C for native performance while providing a clean, modern syntax optimized for both human readability and AI code generation.</p>\n</blockquote>\n<p>The syntax strikes me as an interesting mix between C, Lisp and Rust.</p>\n<p>I decided to see if an LLM could produce working code in it directly, given the necessary context. I started with this <a href=\"https://github.com/jordanhubbard/nanolang/blob/main/MEMORY.md\">MEMORY.md</a> file, which begins:</p>\n<blockquote>\n<p><strong>Purpose:</strong> This file is designed specifically for Large Language Model consumption. It contains the essential knowledge needed to generate, debug, and understand NanoLang code. Pair this with <code>spec.json</code> for complete language coverage.</p>\n</blockquote>\n<p>I ran that using <a href=\"https://llm.datasette.io/\">LLM</a> and <a href=\"https://github.com/simonw/llm-anthropic\">llm-anthropic</a> like this:</p>\n<pre><code>llm -m claude-opus-4.5 \\\n  -s https://raw.githubusercontent.com/jordanhubbard/nanolang/refs/heads/main/MEMORY.md \\\n  'Build me a mandelbrot fractal CLI tool in this language' \n  &gt; /tmp/fractal.nano\n</code></pre>\n<p>The <a href=\"https://gist.github.com/simonw/7847f022566d11629ec2139f1d109fb8#mandelbrot-fractal-cli-tool-in-nano\">resulting code</a>... <a href=\"https://gist.github.com/simonw/7847f022566d11629ec2139f1d109fb8?permalink_comment_id=5947465#gistcomment-5947465\">did not compile</a>.</p>\n<p>I may have been too optimistic expecting a one-shot working program for a new language like this. So I ran a clone of the actual project, copied in my program and had Claude Code take a look at the failing compiler output.</p>\n<p>... and it worked! Claude happily grepped its way through the various <code>examples/</code> and built me a working program.</p>\n<p>Here's <a href=\"https://gisthost.github.io/?9696da6882cb6596be6a9d5196e8a7a5/index.html\">the Claude Code transcript</a> - you can see it <a href=\"https://gisthost.github.io/?9696da6882cb6596be6a9d5196e8a7a5/page-001.html#msg-2026-01-19T23-43-09-675Z\">reading relevant examples here</a> - and here's <a href=\"https://gist.github.com/simonw/e7f3577adcfd392ab7fa23b1295d00f2\">the finished code plus its output</a>.</p>\n<p>I've suspected <a href=\"https://simonwillison.net/2025/Nov/7/llms-for-new-programming-languages/\">for a while</a> that LLMs and coding agents might significantly reduce the friction involved in launching a new language. This result reinforces my opinion.\n\n    <p><small></small>Via <a href=\"https://news.ycombinator.com/item?id=46684958\">Hacker News</a></small></p>\n\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/programming-languages\">programming-languages</a>, <a href=\"https://simonwillison.net/tags/ai\">ai</a>, <a href=\"https://simonwillison.net/tags/generative-ai\">generative-ai</a>, <a href=\"https://simonwillison.net/tags/llms\">llms</a>, <a href=\"https://simonwillison.net/tags/ai-assisted-programming\">ai-assisted-programming</a>, <a href=\"https://simonwillison.net/tags/llm\">llm</a>, <a href=\"https://simonwillison.net/tags/coding-agents\">coding-agents</a>, <a href=\"https://simonwillison.net/tags/claude-code\">claude-code</a></p>", "source_url": "https://simonwillison.net/atom/everything/", "article_url": "https://simonwillison.net/2026/Jan/19/nanolang/#atom-everything", "pub_date": "2026-01-19T23:58:56+00:00", "label": false, "category": "other", "notes": "manual: Small developer tool release."}
{"id": "233ce97d697e5b25", "title": "Scaling long-running autonomous coding", "description": "<p><strong><a href=\"https://cursor.com/blog/scaling-agents\">Scaling long-running autonomous coding</a></strong></p>\nWilson Lin at Cursor has been doing some experiments to see how far you can push a large fleet of \"autonomous\" coding agents:</p>\n<blockquote>\n<p>This post describes what we've learned from running hundreds of concurrent agents on a single project, coordinating their work, and watching them write over a million lines of code and trillions of tokens.</p>\n</blockquote>\n<p>They ended up running planners and sub-planners to create tasks, then having workers execute on those tasks - similar to how Claude Code uses sub-agents. Each cycle ended with a judge agent deciding if the project was completed or not.</p>\n<p>In my predictions for 2026 <a href=\"https://simonwillison.net/2026/Jan/8/llm-predictions-for-2026/#3-years-someone-will-build-a-new-browser-using-mainly-ai-assisted-coding-and-it-won-t-even-be-a-surprise\">the other day</a> I said that by 2029:</p>\n<blockquote>\n<p>I think somebody will have built a full web browser mostly using AI assistance, and it won’t even be surprising. Rolling a new web browser is one of the most complicated software projects I can imagine[...] the cheat code is the conformance suites. If there are existing tests that it’ll get so much easier.</p>\n</blockquote>\n<p>I may have been off by three years, because Cursor chose \"building a web browser from scratch\" as their test case for their agent swarm approach:</p>\n<blockquote>\n<p>To test this system, we pointed it at an ambitious goal: building a web browser from scratch. The agents ran for close to a week, writing over 1 million lines of code across 1,000 files. You can explore <a href=\"https://github.com/wilsonzlin/fastrender\">the source code on GitHub</a>.</p>\n</blockquote>\n<p>But how well did they do? Their initial announcement a couple of days ago was met with <a href=\"https://embedding-shapes.github.io/cursor-implied-success-without-evidence/\">unsurprising skepticism</a>, especially when it became apparent that their GitHub Actions CI was failing and there were no build instructions in the repo.</p>\n<p>It looks like they addressed that within the past 24 hours. The <a href=\"https://github.com/wilsonzlin/fastrender/blob/main/README.md#build-requirements\">latest README</a> includes build instructions which I followed on macOS like this:</p>\n<pre><code>cd /tmp\ngit clone https://github.com/wilsonzlin/fastrender\ncd fastrender\ngit submodule update --init vendor/ecma-rs\ncargo run --release --features browser_ui --bin browser\n</code></pre>\n<p>This got me a working browser window! Here are screenshots I took of google.com and my own website:</p>\n<p><img alt=\"The browser chrome is neat but has a garbled tab name at the top. The Google homepage looks mostly correct but the buttons are not styled correctly and the Google Search one has a huge plus icon floating near it.\" src=\"https://static.simonwillison.net/static/2026/cursor-google.png\" /></p>\n<p><img alt=\"My blog looks mostly correct, but the right closing quotation mark on a quotation (which is implemented as a background image on the final paragraph) is displayed incorrectly multiple times.\" src=\"https://static.simonwillison.net/static/2026/cursor-simonwillison.jpg\" /></p>\n<p>Honestly those are very impressive! You can tell they're not just wrapping an existing rendering engine because of those very obvious rendering glitches, but the pages are legible and look mostly correct.</p>\n<p>The FastRender repo even uses Git submodules <a href=\"https://github.com/wilsonzlin/fastrender/tree/main/specs\">to include various WhatWG and CSS-WG specifications</a> in the repo, which is a smart way to make sure the agents have access to the reference materials that they might need.</p>\n<p>This is the second attempt I've seen at building a full web browser using AI-assisted coding in the past two weeks - the first was <a href=\"https://github.com/hiwavebrowser/hiwave\">HiWave browser</a>, a new browser engine in Rust first announced <a href=\"https://www.reddit.com/r/Anthropic/comments/1q4xfm0/over_christmas_break_i_wrote_a_fully_functional/\">in this Reddit thread</a>.</p>\n<p>When I made my 2029 prediction this is more-or-less the quality of result I had in mind. I don't think we'll see projects of this nature compete with Chrome or Firefox or WebKit any time soon but I have to admit I'm very surprised to see something this capable emerge so quickly.</p>\n<p><strong>Update 23rd January 2026</strong>: I recorded a 47 minute conversation with Wilson about this project and published it on YouTube. Here's <a href=\"https://simonwillison.net/2026/Jan/23/fastrender/\">the video and accompanying highlights</a>.\n\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/browsers\">browsers</a>, <a href=\"https://simonwillison.net/tags/ai\">ai</a>, <a href=\"https://simonwillison.net/tags/generative-ai\">generative-ai</a>, <a href=\"https://simonwillison.net/tags/llms\">llms</a>, <a href=\"https://simonwillison.net/tags/ai-assisted-programming\">ai-assisted-programming</a>, <a href=\"https://simonwillison.net/tags/coding-agents\">coding-agents</a>, <a href=\"https://simonwillison.net/tags/cursor\">cursor</a>, <a href=\"https://simonwillison.net/tags/parallel-agents\">parallel-agents</a>, <a href=\"https://simonwillison.net/tags/conformance-suites\">conformance-suites</a></p>", "source_url": "https://simonwillison.net/atom/everything/", "article_url": "https://simonwillison.net/2026/Jan/19/scaling-long-running-autonomous-coding/#atom-everything", "pub_date": "2026-01-19T05:12:51+00:00", "label": true, "category": "ai_ml", "notes": "manual: Agent scaling techniques with broader impact."}
{"id": "653e579ee89926a3", "title": "The Pulse #160: Why it’s so dramatic that “writing code by hand is dead”", "description": "Also: New trend of staff+ engineers and managers using AI a lot, replacing a $120/year micro-SaaS with LLM-generated code, and more", "source_url": "https://newsletter.pragmaticengineer.com/feed", "article_url": "https://newsletter.pragmaticengineer.com/p/the-pulse-160-why-its-so-dramatic", "pub_date": "2026-01-22T18:02:45+00:00", "label": true, "category": "ai_ml", "notes": "manual: AI-focused column with industry implications."}
{"id": "afbe4a42a40fda04", "title": "How AWS S3 is built", "description": "A behind-the-scenes look at how Amazon S3 is designed for durability and correctness at massive scale, drawing on over a decade of operating one of the world&#8217;s largest distributed systems with Mai-Lan Tomsen Bukovec at AWS.", "source_url": "https://newsletter.pragmaticengineer.com/feed", "article_url": "https://newsletter.pragmaticengineer.com/p/how-aws-s3-is-built", "pub_date": "2026-01-21T18:00:17+00:00", "label": true, "category": "technology", "notes": "manual: Infrastructure deep dive with broad relevance."}
{"id": "fceb519afc684fbd", "title": "Scaling PostgreSQL to power 800 million ChatGPT users", "description": "An inside look at how OpenAI scaled PostgreSQL to millions of queries per second using replicas, caching, rate limiting, and workload isolation.", "source_url": "https://openai.com/blog/rss.xml", "article_url": "https://openai.com/index/scaling-postgresql", "pub_date": "2026-01-22T12:00:00+00:00", "label": true, "category": "technology", "notes": "manual: Scaling database infrastructure for major product."}
{"id": "d20341d7197c1657", "title": "Inside Praktika's conversational approach to language learning", "description": "How Praktika uses GPT-4.1 and GPT-5.2 to build adaptive AI tutors that personalize lessons, track progress, and help learners achieve real-world language fluency", "source_url": "https://openai.com/blog/rss.xml", "article_url": "https://openai.com/index/praktika", "pub_date": "2026-01-22T05:00:00+00:00", "label": false, "category": "other", "notes": "manual: Customer case study; promotional."}
{"id": "0534826e5c0c23d9", "title": "Route leak incident on January 22, 2026", "description": "An automated routing policy configuration error caused us to leak some Border Gateway Protocol prefixes unintentionally from a router at our Miami data center. We discuss the impact and the changes we are implementing as a result.", "source_url": "https://blog.cloudflare.com/rss/", "article_url": "https://blog.cloudflare.com/route-leak-incident-january-22-2026/", "pub_date": "2026-01-23T14:00:00+00:00", "label": true, "category": "security", "notes": "manual: Internet infrastructure incident report."}
{"id": "9684d9b80c73f34b", "title": "How we mitigated a vulnerability in Cloudflare’s ACME validation logic", "description": "A vulnerability was recently identified in Cloudflare’s automation of certificate validation. Here we explain the vulnerability and outline the steps we’ve taken to mitigate it.", "source_url": "https://blog.cloudflare.com/rss/", "article_url": "https://blog.cloudflare.com/acme-path-vulnerability/", "pub_date": "2026-01-19T14:00:00+00:00", "label": true, "category": "security", "notes": "manual: Security vulnerability mitigation details."}
