{"id": "53353c411a7b9dc0", "title": "Horizon 1000: Advancing AI for primary healthcare", "description": "OpenAI and the Gates Foundation launch Horizon 1000, a $50M pilot advancing AI capabilities for healthcare in Africa. The initiative aims to reach 1,000 clinics by 2028.", "source_url": "https://openai.com/blog/rss.xml", "article_url": "https://openai.com/index/horizon-1000", "pub_date": "2026-01-20T21:00:00+00:00", "label": true, "category": "ai_ml", "notes": "heuristic label"}
{"id": "dc94b48e70643f30", "title": "The Pulse #155: 5-day RTO looms for those at Big Tech?", "description": "Also: senior engineers might be more efficient with AI tools than juniors, Anthropic and Claude Code on a roll, and more.", "source_url": "https://newsletter.pragmaticengineer.com/feed", "article_url": "https://newsletter.pragmaticengineer.com/p/the-pulse-155", "pub_date": "2025-12-04T17:29:35+00:00", "label": false, "category": "other", "notes": "heuristic label"}
{"id": "762d1724effcb999", "title": "How Google Got Its Groove Back and Edged Ahead of OpenAI", "description": "<p><strong><a href=\"https://www.wsj.com/tech/ai/google-ai-openai-gemini-chatgpt-b766e160\">How Google Got Its Groove Back and Edged Ahead of OpenAI</a></strong></p>\nI picked up a few interesting tidbits from this Wall Street Journal piece on Google's recent hard won success with Gemini.</p>\n<p>Here's the origin of the name \"Nano Banana\":</p>\n<blockquote>\n<p>Naina Raisinghani, known inside Google for working late into the night, needed a name for the new tool to complete the upload. It was 2:30 a.m., though, and nobody was around. So she just made one up, a mashup of two nicknames friends had given her: Nano Banana.</p>\n</blockquote>\n<p>The WSJ credit OpenAI's Daniel Selsam with un-retiring Sergei Brin:</p>\n<blockquote>\n<p>Around that time, Google co-founder Sergey Brin, who had recently retired, was at a party chatting with a researcher from OpenAI named Daniel Selsam, according to people familiar with the conversation. Why, Selsam asked him, wasn‚Äôt he working full time on AI. Hadn‚Äôt the launch of ChatGPT captured his imagination as a computer scientist?</p>\n<p>ChatGPT was on its way to becoming a household name in AI chatbots, while Google was still fumbling to get its product off the ground. Brin decided Selsam had a point and returned to work.</p>\n</blockquote>\n<p>And we get some rare concrete user numbers:</p>\n<blockquote>\n<p>By October, Gemini had more than 650 million monthly users, up from 450 million in July.</p>\n</blockquote>\n<p>The LLM usage number I see cited most often is OpenAI's 800 million weekly active users for ChatGPT. That's from October 6th at OpenAI DevDay so it's comparable to these Gemini numbers, albeit not directly since it's weekly rather than monthly actives.</p>\n<p>I'm also never sure what counts as a \"Gemini user\" - does interacting via Google Docs or Gmail count or do you need to be using a Gemini chat interface directly?</p>\n<p><strong>Update 17th January 2025</strong>: <a href=\"https://twitter.com/lunixa380/status/2012610344741412909\">@LunixA380 pointed out</a> that this 650m user figure comes from the <a href=\"https://abc.xyz/investor/news/news-details/2025/Alphabet-Announces-Third-Quarter-2025-Results-2025-mIRgD3AI4A/default.aspx\">Alphabet 2025 Q3 earnings report</a> which says this (emphasis mine):</p>\n<blockquote>\n<p>\"Alphabet had a terrific quarter, with double-digit growth across every major part of our business. We delivered our first-ever $100 billion quarter,\" said Sundar Pichai, CEO of Alphabet and Google.</p>\n<p>\"[...] In addition to topping leaderboards, our first party models, like Gemini, now process 7 billion tokens per minute, via direct API use by our customers. <strong>The Gemini App now has over 650 million monthly active users</strong>.</p>\n</blockquote>\n<p>Presumably the \"Gemini App\" encompasses the Android and iPhone apps as well as direct visits to <a href=\"https://gemini.google.com/\">gemini.google.com</a> - that seems to be the indication from Google's <a href=\"https://blog.google/products-and-platforms/products/gemini/gemini-3/\">November 18th blog post</a> that also mentioned the 650m number.\n\n    <p><small></small>Via <a href=\"https://news.ycombinator.com/item?id=46528389\">Hacker News</a></small></p>\n\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/google\">google</a>, <a href=\"https://simonwillison.net/tags/ai\">ai</a>, <a href=\"https://simonwillison.net/tags/openai\">openai</a>, <a href=\"https://simonwillison.net/tags/generative-ai\">generative-ai</a>, <a href=\"https://simonwillison.net/tags/llms\">llms</a>, <a href=\"https://simonwillison.net/tags/gemini\">gemini</a>, <a href=\"https://simonwillison.net/tags/nano-banana\">nano-banana</a></p>", "source_url": "https://simonwillison.net/atom/everything/", "article_url": "https://simonwillison.net/2026/Jan/8/how-google-got-its-groove-back/#atom-everything", "pub_date": "2026-01-08T15:32:08+00:00", "label": true, "category": "ai_ml", "notes": "heuristic label"}
{"id": "344dd99e1dbe880e", "title": "The Open Evaluation Standard: Benchmarking NVIDIA Nemotron 3 Nano with NeMo Evaluator", "description": "", "source_url": "https://huggingface.co/blog/feed.xml", "article_url": "https://huggingface.co/blog/nvidia/nemotron-3-nano-evaluation-recipe", "pub_date": "2025-12-17T13:22:18+00:00", "label": true, "category": "business", "notes": "heuristic label"}
{"id": "942b871cda5cdfcb", "title": "Teaching AI to see the world more like we do", "description": "Our new paper analyzes the important ways AI systems organize the visual world differently from humans.", "source_url": "https://deepmind.google/blog/rss.xml", "article_url": "https://deepmind.google/blog/teaching-ai-to-see-the-world-more-like-we-do/", "pub_date": "2025-11-11T11:49:13+00:00", "label": true, "category": "ai_ml", "notes": "heuristic label"}
{"id": "880392bec75da092", "title": "Tokenization in Transformers v5: Simpler, Clearer, and More Modular", "description": "", "source_url": "https://huggingface.co/blog/feed.xml", "article_url": "https://huggingface.co/blog/tokenizers", "pub_date": "2025-12-18T00:00:00+00:00", "label": true, "category": "ai_ml", "notes": "heuristic label"}
{"id": "218027d855441924", "title": "OpenAI‚Äôs Raising Concerns Policy", "description": "We‚Äôre publishing our Raising Concerns Policy, which protects employees‚Äô rights to make protected disclosures.", "source_url": "https://openai.com/blog/rss.xml", "article_url": "https://openai.com/index/openai-raising-concerns-policy", "pub_date": "2026-01-12T00:00:00+00:00", "label": true, "category": "policy", "notes": "heuristic label"}
{"id": "a07573bf564bc947", "title": "Businesses grow revenue on Stripe 27 percentage points faster after accepting financing through Stripe Capital", "description": "In a new study, we found a strong causal relationship between accepting financing and growing revenue on Stripe. Learn which businesses are most likely to benefit, and how greater access to financing could drive significant GDP growth.", "source_url": "https://stripe.com/blog/feed.rss", "article_url": "https://stripe.com/blog/businesses-grow-revenue-on-stripe-faster-after-accepting-financing-through-stripe-capital", "pub_date": "2025-12-17T00:00:00+00:00", "label": false, "category": "other", "notes": "heuristic label"}
{"id": "e7ea3180d005cd05", "title": "The new ChatGPT Images is here", "description": "The new ChatGPT Images is powered by our flagship image generation model, delivering more precise edits, consistent details, and image generation up to 4√ó faster. The upgraded model is rolling out to all ChatGPT users today and is also available in the API as GPT-Image-1.5.", "source_url": "https://openai.com/blog/rss.xml", "article_url": "https://openai.com/index/new-chatgpt-images-is-here", "pub_date": "2025-12-16T00:00:00+00:00", "label": true, "category": "ai_ml", "notes": "heuristic label"}
{"id": "67a26c90358ec21c", "title": "RL without TD learning", "description": "<!-- twitter -->\n\n\n\n\n\n\n\n\n\n\n\n\n<p>In this post, I‚Äôll introduce a reinforcement learning (RL) algorithm based on an ‚Äúalternative‚Äù paradigm: <strong>divide and conquer</strong>. Unlike traditional methods, this algorithm is <em>not</em> based on temporal difference (TD) learning (which has <a href=\"https://seohong.me/blog/q-learning-is-not-yet-scalable/\">scalability challenges</a>), and scales well to long-horizon tasks.</p>\n\n<p style=\"text-align: center;\">\n<img alt=\"\" src=\"https://bair.berkeley.edu/static/blog/rl-without-td-learning/teaser_short.png\" width=\"100%\" />\n<br />\n<i style=\"font-size: 0.9em;\">We can do Reinforcement Learning (RL) based on divide and conquer, instead of temporal difference (TD) learning.</i>\n</p>\n\n<!--more-->\n\n<h2 id=\"problem-setting-off-policy-rl\">Problem setting: off-policy RL</h2>\n\n<p>Our problem setting is <strong>off-policy RL</strong>. Let‚Äôs briefly review what this means.</p>\n\n<p>There are two classes of algorithms in RL: on-policy RL and off-policy RL. On-policy RL means we can <em>only</em> use fresh data collected by the current policy. In other words, we have to throw away old data each time we update the policy. Algorithms like PPO and GRPO (and policy gradient methods in general) belong to this category.</p>\n\n<p>Off-policy RL means we don‚Äôt have this restriction: we can use <em>any</em> kind of data, including old experience, human demonstrations, Internet data, and so on. So off-policy RL is more general and flexible than on-policy RL (and of course harder!). Q-learning is the most well-known off-policy RL algorithm. In domains where data collection is expensive (<em>e.g.</em>, <strong>robotics</strong>, dialogue systems, healthcare, etc.), we often have no choice but to use off-policy RL. That‚Äôs why it‚Äôs such an important problem.</p>\n\n<p>As of 2025, I think we have reasonably good recipes for scaling up on-policy RL (<em>e.g.</em>, PPO, GRPO, and their variants). However, we still haven‚Äôt found a ‚Äúscalable‚Äù <em>off-policy RL</em> algorithm that scales well to complex, long-horizon tasks. Let me briefly explain why.</p>\n\n<h2 id=\"two-paradigms-in-value-learning-temporal-difference-td-and-monte-carlo-mc\">Two paradigms in value learning: Temporal Difference (TD) and Monte Carlo (MC)</h2>\n\n<p>In off-policy RL, we typically train a value function using temporal difference (TD) learning (<em>i.e.</em>, Q-learning), with the following Bellman update rule:</p>\n\n\\[\\begin{aligned} Q(s, a) \\gets r + \\gamma \\max_{a'} Q(s', a'), \\end{aligned}\\]\n\n<p>The problem is this: the error in the next value $Q(s‚Äô, a‚Äô)$ propagates to the current value $Q(s, a)$ through bootstrapping, and these errors <em>accumulate</em> over the entire horizon. This is basically what makes TD learning struggle to scale to long-horizon tasks (see <a href=\"https://seohong.me/blog/q-learning-is-not-yet-scalable/\">this post</a> if you‚Äôre interested in more details).</p>\n\n<p>To mitigate this problem, people have mixed TD learning with Monte Carlo (MC) returns. For example, we can do $n$-step TD learning (TD-$n$):</p>\n\n\\[\\begin{aligned} Q(s_t, a_t) \\gets \\sum_{i=0}^{n-1} \\gamma^i r_{t+i} + \\gamma^n \\max_{a'} Q(s_{t+n}, a'). \\end{aligned}\\]\n\n<p>Here, we use the actual Monte Carlo return (from the dataset) for the first $n$ steps, and then use the bootstrapped value for the rest of the horizon. This way, we can reduce the number of Bellman recursions by $n$ times, so errors accumulate less. In the extreme case of $n = \\infty$, we recover pure Monte Carlo value learning.</p>\n\n<p>While this is a reasonable solution (and often <a href=\"https://arxiv.org/abs/2506.04168\">works well</a>), it is highly unsatisfactory. First, it doesn‚Äôt <em>fundamentally</em> solve the error accumulation problem; it only reduces the number of Bellman recursions by a constant factor ($n$). Second, as $n$ grows, we suffer from high variance and suboptimality. So we can‚Äôt just set $n$ to a large value, and need to carefully tune it for each task.</p>\n\n<p>Is there a fundamentally different way to solve this problem?</p>\n\n<h2 id=\"the-third-paradigm-divide-and-conquer\">The ‚ÄúThird‚Äù Paradigm: Divide and Conquer</h2>\n\n<p>My claim is that a <em>third</em> paradigm in value learning, <strong>divide and conquer</strong>, may provide an ideal solution to off-policy RL that scales to arbitrarily long-horizon tasks.</p>\n\n<p style=\"text-align: center;\">\n<img alt=\"\" src=\"https://bair.berkeley.edu/static/blog/rl-without-td-learning/teaser.png\" width=\"100%\" />\n<br />\n<i style=\"font-size: 0.9em;\">Divide and conquer reduces the number of Bellman recursions logarithmically.</i>\n</p>\n\n<p>The key idea of divide and conquer is to divide a trajectory into two equal-length segments, and combine their values to update the value of the full trajectory. This way, we can (in theory) reduce the number of Bellman recursions <em>logarithmically</em> (not linearly!). Moreover, it doesn‚Äôt require choosing a hyperparameter like $n$, and it doesn‚Äôt necessarily suffer from high variance or suboptimality, unlike $n$-step TD learning.</p>\n\n<p>Conceptually, divide and conquer really has all the nice properties we want in value learning. So I‚Äôve long been excited about this high-level idea. The problem was that it wasn‚Äôt clear how to actually do this in practice‚Ä¶ until recently.</p>\n\n<h2 id=\"a-practical-algorithm\">A practical algorithm</h2>\n\n<p>In a <a href=\"https://arxiv.org/abs/2510.22512\">recent work</a> co-led with <a href=\"https://aober.ai/\">Aditya</a>, we made meaningful progress toward realizing and scaling up this idea. Specifically, we were able to scale up divide-and-conquer value learning to highly complex tasks (as far as I know, this is the first such work!) at least in one important class of RL problems, <em>goal-conditioned RL</em>. Goal-conditioned RL aims to learn a policy that can reach any state from any other state. This provides a natural divide-and-conquer structure. Let me explain this.</p>\n\n<p>The structure is as follows. Let‚Äôs first assume that the dynamics is deterministic, and denote the shortest path distance (‚Äútemporal distance‚Äù) between two states $s$ and $g$ as $d^*(s, g)$. Then, it satisfies the triangle inequality:</p>\n\n\\[\\begin{aligned} d^*(s, g) \\leq d^*(s, w) + d^*(w, g) \\end{aligned}\\]\n\n<p>for all $s, g, w \\in \\mathcal{S}$.</p>\n\n<p>In terms of values, we can equivalently translate this triangle inequality to the following <em>‚Äútransitive‚Äù</em> Bellman update rule:</p>\n\n\\[\\begin{aligned} \nV(s, g) \\gets \\begin{cases}\n\\gamma^0 &amp; \\text{if } s = g, \\\\\\\\ \n\\gamma^1 &amp; \\text{if } (s, g) \\in \\mathcal{E}, \\\\\\\\ \n\\max_{w \\in \\mathcal{S}} V(s, w)V(w, g) &amp; \\text{otherwise}\n\\end{cases} \n\\end{aligned}\\]\n\n<p>where $\\mathcal{E}$ is the set of edges in the environment‚Äôs transition graph, and $V$ is the value function associated with the sparse reward $r(s, g) = 1(s = g)$. <strong>Intuitively</strong>, this means that we can update the value of $V(s, g)$ using two ‚Äúsmaller‚Äù values: $V(s, w)$ and $V(w, g)$, provided that $w$ is the optimal ‚Äúmidpoint‚Äù (subgoal) on the shortest path. This is exactly the divide-and-conquer value update rule that we were looking for!</p>\n\n<h3 id=\"the-problem\">The problem</h3>\n\n<p>However, there‚Äôs one problem here. The issue is that it‚Äôs unclear how to choose the optimal subgoal $w$ in practice. In tabular settings, we can simply enumerate all states to find the optimal $w$ (this is essentially the Floyd-Warshall shortest path algorithm). But in continuous environments with large state spaces, we can‚Äôt do this. Basically, this is why previous works have struggled to scale up divide-and-conquer value learning, even though this idea has been around for decades (in fact, it dates back to the very first work in goal-conditioned RL by <a href=\"https://scholar.google.com/citations?view_op=view_citation&amp;citation_for_view=IcasIiwAAAAJ:hC7cP41nSMkC\">Kaelbling (1993)</a> ‚Äì see <a href=\"https://arxiv.org/abs/2510.22512\">our paper</a> for a further discussion of related works). The main contribution of our work is a practical solution to this issue.</p>\n\n<h3 id=\"the-solution\">The solution</h3>\n\n<p>Here‚Äôs our key idea: we <em>restrict</em> the search space of $w$ to the states that appear in the dataset, specifically, those that lie between $s$ and $g$ in the dataset trajectory. Also, instead of searching for the optimal $\\text{argmax}_w$, we compute a ‚Äúsoft‚Äù $\\text{argmax}$ using <a href=\"https://arxiv.org/abs/2110.06169\">expectile regression</a>. Namely, we minimize the following loss:</p>\n\n\\[\\begin{aligned} \\mathbb{E}\\left[\\ell^2_\\kappa (V(s_i, s_j) - \\bar{V}(s_i, s_k) \\bar{V}(s_k, s_j))\\right], \\end{aligned}\\]\n\n<p>where $\\bar{V}$ is the target value network, $\\ell^2_\\kappa$ is the expectile loss with an expectile $\\kappa$, and the expectation is taken over all $(s_i, s_k, s_j)$ tuples with $i \\leq k \\leq j$ in a randomly sampled dataset trajectory.</p>\n\n<p>This has two benefits. First, we don‚Äôt need to search over the entire state space. Second, we prevent value overestimation from the $\\max$ operator by instead using the ‚Äúsofter‚Äù expectile regression. We call this algorithm <strong>Transitive RL (TRL)</strong>. Check out <a href=\"https://arxiv.org/abs/2510.22512\">our paper</a> for more details and further discussions!</p>\n\n<h2 id=\"does-it-work-well\">Does it work well?</h2>\n\n<div style=\"display: flex; margin: 30px 0;\">\n  <div style=\"text-align: center;\">\n    <video loop=\"\" style=\"width: 350px;\">\n      <source src=\"https://bair.berkeley.edu/static/blog/rl-without-td-learning/humanoidmaze.mp4\" type=\"video/mp4\" />\n      Your browser does not support the video tag.\n    </video>\n    <br />\n    <i style=\"font-size: 0.9em;\">humanoidmaze</i>\n  </div>\n  <div style=\"text-align: center;\">\n    <video loop=\"\" style=\"width: 350px;\">\n      <source src=\"https://bair.berkeley.edu/static/blog/rl-without-td-learning/puzzle.mp4\" type=\"video/mp4\" />\n      Your browser does not support the video tag.\n    </video>\n    <br />\n    <i style=\"font-size: 0.9em;\">puzzle</i>\n  </div>\n</div>\n\n<p>To see whether our method scales well to complex tasks, we directly evaluated TRL on some of the most challenging tasks in <a href=\"https://seohong.me/projects/ogbench/\">OGBench</a>, a benchmark for offline goal-conditioned RL. We mainly used the hardest versions of humanoidmaze and puzzle tasks with large, 1B-sized datasets. These tasks are highly challenging: they require performing combinatorially complex skills across up to <strong>3,000 environment steps</strong>.</p>\n\n<p style=\"text-align: center;\">\n<img alt=\"\" src=\"https://bair.berkeley.edu/static/blog/rl-without-td-learning/table.png\" width=\"100%\" />\n<br />\n<i style=\"font-size: 0.9em;\">TRL achieves the best performance on highly challenging, long-horizon tasks.</i>\n</p>\n\n<p>The results are quite exciting! Compared to many strong baselines across different categories (TD, MC, quasimetric learning, etc.), TRL achieves the best performance on most tasks.</p>\n\n<p style=\"text-align: center;\">\n<img alt=\"\" src=\"https://bair.berkeley.edu/static/blog/rl-without-td-learning/1b.svg\" width=\"100%\" />\n<br />\n<i style=\"font-size: 0.9em;\">TRL matches the best, individually tuned TD-$n$, <b>without needing to set $\\boldsymbol{n}$</b>.</i>\n</p>\n\n<p>This is my favorite plot. We compared TRL with $n$-step TD learning with different values of $n$, from $1$ (pure TD) to $\\infty$ (pure MC). The result is really nice. TRL matches the best TD-$n$ on all tasks, <strong>without needing to set $\\boldsymbol{n}$</strong>! This is exactly what we wanted from the divide-and-conquer paradigm. By recursively splitting a trajectory into smaller ones, it can <em>naturally</em> handle long horizons, without having to arbitrarily choose the length of trajectory chunks.</p>\n\n<p>The paper has a lot of additional experiments, analyses, and ablations. If you‚Äôre interested, check out <a href=\"https://arxiv.org/abs/2510.22512\">our paper</a>!</p>\n\n<h2 id=\"whats-next\">What‚Äôs next?</h2>\n\n<p>In this post, I shared some promising results from our new divide-and-conquer value learning algorithm, Transitive RL. This is just the beginning of the journey. There are many open questions and exciting directions to explore:</p>\n\n<ul>\n  <li>\n    <p>Perhaps the most important question is how to extend TRL to regular, reward-based RL tasks beyond goal-conditioned RL. Would regular RL have a similar divide-and-conquer structure that we can exploit? I‚Äôm quite optimistic about this, given that it is possible to convert any reward-based RL task to a goal-conditioned one at least in theory (see page 40 of <a href=\"https://sites.google.com/view/goalconditioned-rl/\">this book</a>).</p>\n  </li>\n  <li>\n    <p>Another important challenge is to deal with stochastic environments. The current version of TRL assumes deterministic dynamics, but many real-world environments are stochastic, mainly due to partial observability. For this, <a href=\"https://arxiv.org/abs/2406.17098\">‚Äústochastic‚Äù triangle inequalities</a> might provide some hints.</p>\n  </li>\n  <li>\n    <p>Practically, I think there is still a lot of room to further improve TRL. For example, we can find better ways to choose subgoal candidates (beyond the ones from the same trajectory), further reduce hyperparameters, further stabilize training, and simplify the algorithm even more.</p>\n  </li>\n</ul>\n\n<p>In general, I‚Äôm really excited about the potential of the divide-and-conquer paradigm. I <a href=\"https://seohong.me/blog/q-learning-is-not-yet-scalable/\">still</a> think one of the most important problems in RL (and even in machine learning) is to find a <em>scalable</em> off-policy RL algorithm. I don‚Äôt know what the final solution will look like, but I do think divide and conquer, or <strong>recursive</strong> decision-making in general, is one of the strongest candidates toward this holy grail (by the way, I think the other strong contenders are (1) model-based RL and (2) TD learning with some ‚Äúmagic‚Äù tricks). Indeed, several recent works in other fields have shown the promise of recursion and divide-and-conquer strategies, such as <a href=\"https://kvfrans.com/shortcut-models/\">shortcut models</a>, <a href=\"https://arxiv.org/abs/2506.04761\">log-linear attention</a>, and <a href=\"https://alexzhang13.github.io/blog/2025/rlm/\">recursive language models</a> (and of course, classic algorithms like quicksort, segment trees, FFT, and so on). I hope to see more exciting progress in scalable off-policy RL in the near future!</p>\n\n<h3 id=\"acknowledgments\">Acknowledgments</h3>\n\n<p>I‚Äôd like to thank <a href=\"https://kvfrans.com/\">Kevin</a> and <a href=\"https://people.eecs.berkeley.edu/~svlevine/\">Sergey</a> for their helpful feedback on this post.</p>\n\n<hr />\n\n<p><em>This post originally appeared on <a href=\"https://seohong.me/blog/rl-without-td-learning/\">Seohong Park‚Äôs blog</a>.</em></p>", "source_url": "https://bair.berkeley.edu/blog/feed.xml", "article_url": "http://bair.berkeley.edu/blog/2025/11/01/rl-without-td-learning/", "pub_date": "2025-11-01T09:00:00+00:00", "label": false, "category": "other", "notes": "heuristic label"}
{"id": "f29849f7ac317b5d", "title": "My Foreword to \"Frictionless\"", "description": "<div class=\"img-link\"><a href=\"https://martinfowler.com/articles/frictionless-foreword.html\"><img src=\"https://martinfowler.com/articles/frictionless-card.jpg\" width=\"\" /></a></div>\n\n<p>I find most writing on software productivity to be twaddle, but Nicole\n      Forsgren and Abi Noda are notable exceptions. I had a chance to take a\n      look at their new book, published today, and liked it so much I wrote a\n      foreword.</p>\n\n<p><a class=\"more\" href=\"https://martinfowler.com/articles/frictionless-foreword.html\">more‚Ä¶</a></p>", "source_url": "https://martinfowler.com/feed.atom", "article_url": "https://martinfowler.com/articles/frictionless-foreword.html", "pub_date": "2025-11-18T17:19:00+00:00", "label": false, "category": "other", "notes": "heuristic label"}
{"id": "0921ff28a85ee0f2", "title": "How to Build a Healthcare Robot from Simulation to Deployment with NVIDIA Isaac for Healthcare", "description": "", "source_url": "https://huggingface.co/blog/feed.xml", "article_url": "https://huggingface.co/blog/nvidia/nvidia-isaac-for-healthcare", "pub_date": "2025-10-28T20:42:35+00:00", "label": false, "category": "lifestyle", "notes": "heuristic label"}
{"id": "4499d9091c859913", "title": "Addendum to GPT-5 System Card: Sensitive conversations", "description": "This system card details GPT-5‚Äôs improvements in handling sensitive conversations, including new benchmarks for emotional reliance, mental health, and jailbreak resistance.", "source_url": "https://openai.com/blog/rss.xml", "article_url": "https://openai.com/index/gpt-5-system-card-sensitive-conversations", "pub_date": "2025-10-27T10:00:00+00:00", "label": true, "category": "ai_ml", "notes": "heuristic label"}
{"id": "3498cb93782981de", "title": "Brazil‚Äôs AI moment is here", "description": "Brazil is now one of the most engaged countries in the world when it comes to AI. From classrooms to farms and small businesses, Brazilians are using OpenAI products to learn, create, and drive innovation.", "source_url": "https://openai.com/blog/rss.xml", "article_url": "https://openai.com/global-affairs/brazil-ai-moment-is-here", "pub_date": "2025-11-04T15:30:00+00:00", "label": true, "category": "ai_ml", "notes": "heuristic label"}
{"id": "c537cdb80d3dab38", "title": "Introducing OpenAI for Australia", "description": "OpenAI is launching OpenAI for Australia to build sovereign AI infrastructure, upskill more than 1.5 million workers, and accelerate innovation across the country‚Äôs growing AI ecosystem.", "source_url": "https://openai.com/blog/rss.xml", "article_url": "https://openai.com/global-affairs/openai-for-australia", "pub_date": "2025-12-04T19:00:00+00:00", "label": true, "category": "ai_ml", "notes": "heuristic label"}
{"id": "39de9271f408f9b9", "title": "First impressions of Claude Cowork, Anthropic's general agent", "description": "<p>New from Anthropic today is <a href=\"https://claude.com/blog/cowork-research-preview\">Claude Cowork</a>, a \"research preview\" that they describe as \"Claude Code for the rest of your work\". It's currently available only to Max subscribers ($100 or $200 per month plans) as part of the updated Claude Desktop macOS application. <strong>Update 16th January 2026</strong>: it's now also available to $20/month Claude Pro subscribers.</p>\n<p>I've been saying for a while now that Claude Code is a \"general agent\" disguised as a developer tool. It can help you with any computer task that can be achieved by executing code or running terminal commands... which covers almost anything, provided you know what you're doing with it! What it really needs is a UI that doesn't involve the terminal and a name that doesn't scare away non-developers.</p>\n<p>\"Cowork\" is a pretty solid choice on the name front!</p>\n<h4 id=\"what-it-looks-like\">What it looks like</h4>\n<p>The interface for Cowork is a new tab in the Claude desktop app, called Cowork. It sits next to the existing Chat and Code tabs.</p>\n<p>It looks very similar to the desktop interface for regular Claude Code. You start with a prompt, optionally attaching a folder of files. It then starts work.</p>\n<p>I tried it out against my perpetually growing \"blog-drafts\" folder with the following prompt:</p>\n<blockquote>\n<p>Look at my drafts that were started within the last three months and then check that I didn't publish them on simonwillison.net using a search against content on that site and then suggest the ones that are most close to being ready</p>\n</blockquote>\n<p><img alt=\"Screenshot of Claude AI desktop application showing a &quot;Cowork&quot; task interface. Left sidebar shows tabs for &quot;Chat&quot;, &quot;Code&quot;, and &quot;Cowork&quot; (selected), with &quot;+ New task&quot; button and a task titled &quot;Review unpublished drafts for pu...&quot; listed below. Text reads &quot;These tasks run locally and aren't synced across devices&quot;. Main panel header shows &quot;Review unpublished drafts for publication&quot;. User message in green bubble reads: &quot;Look at my drafts that were started within the last three months and then check that I didn't publish them on simonwillison.net using a search against content on that site and then suggest the ones that are most close to being ready&quot;. Claude responds: &quot;I'll help you find drafts from the last three months and check if they've been published. Let me start by looking at your drafts folder.&quot; Below is an expanded &quot;Running command&quot; section showing Request JSON with command: find /sessions/zealous-bold-ramanujan/mnt/blog-drafts -type f \\\\( -name \\&quot;*.md\\&quot; -o -name \\&quot;*.txt\\&quot; -o -name \\&quot;*.html\\&quot; \\\\) -mtime -90 -exec ls -la {} \\\\;, description: Find draft files modified in the last 90 days. Response text begins: &quot;Found 46 draft files. Next let me read the content of each to get their titles/topics, then&quot;. Right sidebar shows Progress section with three circular indicators (two checked, one pending) and text &quot;Steps will show as the task unfolds.&quot;, Artifacts section listing &quot;publish-encouragement.html&quot;, Context section with &quot;Selected folders&quot; showing &quot;blog-drafts&quot; folder, Connectors showing &quot;Web search&quot;, and Working files listing &quot;llm-digest-october-2025.md&quot;, &quot;tests-not-optional-coding-agen...&quot;, and &quot;digest-november-2025.md&quot;. Bottom shows reply input field, &quot;Opus 4.5&quot; model selector, user &quot;Simon Willison&quot; with &quot;Max plan&quot;, and disclaimer &quot;Claude is AI and can make mistakes. Please double-check responses.&quot;\" src=\"https://static.simonwillison.net/static/2026/claude-cowork.jpg\" /></p>\n<p>It started by running this command:</p>\n<div class=\"highlight highlight-source-shell\"><pre>find /sessions/zealous-bold-ramanujan/mnt/blog-drafts \\\n  -type f <span class=\"pl-cce\">\\(</span> -name <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>*.md<span class=\"pl-pds\">\"</span></span> -o -name <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>*.txt<span class=\"pl-pds\">\"</span></span> -o -name <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>*.html<span class=\"pl-pds\">\"</span></span> <span class=\"pl-cce\">\\)</span> \\\n  -mtime -90 -exec ls -la {} <span class=\"pl-cce\">\\;</span></pre></div>\n<p>That <code>/sessions/zealous-bold-ramanujan/mnt/blog-drafts</code> path instantly caught my eye. Anthropic say that Cowork can only access files you grant it access to - it looks to me like they're mounting those files into a containerized environment, which should mean we can trust Cowork not to be able to access anything outside of that sandbox.</p>\n<p>It turns out I have 46 draft files from the past three months. Claude then went to work with its search tool, running 44 individual searches against <code>site:simonwillison.net</code> to figure out which of my drafts had already been published.</p>\n<p>Here's the eventual reply:</p>\n<blockquote>\n<p>Based on my analysis, here are <strong>your unpublished drafts that appear closest to being ready for publication</strong>:</p>\n<hr />\n<p><strong>üî• Most Ready to Publish (substantial content, not yet published)</strong></p>\n<ol>\n<li>\n<strong><code>freq-argued-llms.md</code></strong> - \"Frequently Argued Questions about LLMs\" (22,602 bytes)\n<ul>\n<li>This is a meaty piece documenting common arguments about LLMs with your counterpoints</li>\n<li>Well-structured with a TL;DR and multiple sections</li>\n<li>No matching published article found on your site</li>\n<li>\n<strong>Very close to ready</strong> - just needs a final review pass</li>\n</ul>\n</li>\n<li>\n<strong><code>claude-code-timeline.md</code></strong> - \"Claude Code Timeline and Codex Timeline\" (3,075 bytes)\n<ul>\n<li>About viewing JSONL session logs from Claude Code and Codex</li>\n<li>You published <code>claude-code-transcripts</code> on Dec 25, but this appears to be a different/earlier piece about timeline viewing tools</li>\n<li>Shorter but seems complete</li>\n</ul>\n</li>\n<li>\n<strong><code>datasette-1a20-upgrade-instructions.md</code></strong> - Plugin Upgrade Guide (3,147 bytes)\n<ul>\n<li>Technical guide for plugin authors</li>\n<li>You published the main 1.0a20 announcement but this companion upgrade guide appears unpublished</li>\n<li>Would be valuable for plugin maintainers</li>\n</ul>\n</li>\n</ol>\n</blockquote>\n<p>That's a good response! It found exactly what I needed to see, although those upgrade instructions are actually published elsewhere now (<a href=\"https://docs.datasette.io/en/latest/upgrade_guide.html\">in the Datasette docs</a>) and weren't actually intended for my blog.</p>\n<p>Just for fun, and because I <a href=\"https://simonwillison.net/2024/Oct/21/claude-artifacts/\">really like artifacts</a>, I asked for a follow-up:</p>\n<blockquote>\n<p>Make me an artifact with exciting animated encouragements to get me to do it</p>\n</blockquote>\n<p>Here's what I got:</p>\n<p><img alt=\"Screenshot of the same Claude AI desktop application Cowork interface, now showing completed task results. Left panel shows &quot;1 step &gt;&quot; with link &quot;View your animated encouragement page&quot;. Claude's response reads: &quot;I created an over-the-top animated encouragement page with:&quot; followed by bullet points: &quot;üöÄ Pulsing rockets and bouncing stats&quot;, &quot;‚ú® Falling emoji rain and confetti&quot;, &quot;üî• Dancing fire emojis around your draft title&quot;, &quot;üí´ Sparkles that follow your mouse&quot;, &quot;üìä An animated '95% ready' progress bar&quot;, &quot;üí¨ Rotating motivational quotes&quot;, &quot;üéâ A 'I'M GONNA DO IT!' button that triggers an explosion of confetti when clicked&quot;. Center shows an artifact preview of the generated HTML page with dark background featuring animated rocket emojis, large white text &quot;PUBLISH TIME!&quot;, stats showing &quot;22,602 bytes of wisdom waiting&quot;, &quot;95% ready to ship&quot;, infinity symbol with &quot;future arguments saved&quot;, and a fire emoji with yellow text &quot;Frequently&quot; (partially visible). Top toolbar shows &quot;Open in Firefox&quot; button. Right sidebar displays Progress section with checkmarks, Artifacts section with &quot;publish-encouragement.html&quot; selected, Context section showing &quot;blog-drafts&quot; folder, &quot;Web search&quot; connector, and Working files listing &quot;llm-digest-october-2025.md&quot;, &quot;tests-not-optional-coding-agen...&quot;, and &quot;digest-november-2025.md&quot;. Bottom shows reply input, &quot;Opus 4.5&quot; model selector, and disclaimer text.\" src=\"https://static.simonwillison.net/static/2026/claude-cowork-artifact.jpg\" /></p>\n<p>I couldn't figure out how to close the right sidebar so the artifact ended up cramped into a thin column but it did work. I expect Anthropic will fix that display bug pretty quickly.</p>\n<h4 id=\"isn-t-this-just-claude-code-\">Isn't this just Claude Code?</h4>\n<p>I've seen a few people ask what the difference between this and regular Claude Code is. The answer is <em>not a lot</em>. As far as I can tell Claude Cowork is regular Claude Code wrapped in a less intimidating default interface and with a filesystem sandbox configured for you without you needing to know what a \"filesystem sandbox\" is.</p>\n<p><strong>Update</strong>: It's more than just a filesystem sandbox - I had Claude Code reverse engineer the Claude app and <a href=\"https://gist.github.com/simonw/35732f187edbe4fbd0bf976d013f22c8\">it found out</a> that Claude uses VZVirtualMachine - the Apple Virtualization Framework - and downloads and boots a custom Linux root filesystem.</p>\n<p>I think that's a really smart product. Claude Code has an enormous amount of value that hasn't yet been unlocked for a general audience, and this seems like a pragmatic approach.</p>\n\n<h4 id=\"the-ever-present-threat-of-prompt-injection\">The ever-present threat of prompt injection</h4>\n<p>With a feature like this, my first thought always jumps straight to security. How big is the risk that someone using this might be hit by hidden malicious instruction somewhere that break their computer or steal their data?</p>\n<p>Anthropic touch on that directly in the announcement:</p>\n<blockquote>\n<p>You should also be aware of the risk of \"<a href=\"https://www.anthropic.com/research/prompt-injection-defenses\">prompt injections</a>\": attempts by attackers to alter Claude's plans through content it might encounter on the internet. We've built sophisticated defenses against prompt injections, but agent safety---that is, the task of securing Claude's real-world actions---is still an active area of development in the industry.</p>\n<p>These risks aren't new with Cowork, but it might be the first time you're using a more advanced tool that moves beyond a simple conversation. We recommend taking precautions, particularly while you learn how it works. We provide more detail in our <a href=\"https://support.claude.com/en/articles/13364135-using-cowork-safely\">Help Center</a>.</p>\n</blockquote>\n<p>That help page includes the following tips:</p>\n<blockquote>\n<p>To minimize risks:</p>\n<ul>\n<li>Avoid granting access to local files with sensitive information, like financial documents.</li>\n<li>When using the Claude in Chrome extension, limit access to trusted sites.</li>\n<li>If you chose to extend Claude‚Äôs default internet access settings, be careful to only extend internet access to sites you trust.</li>\n<li>Monitor Claude for suspicious actions that may indicate prompt injection.</li>\n</ul>\n</blockquote>\n<p>I do not think it is fair to tell regular non-programmer users to watch out for \"suspicious actions that may indicate prompt injection\"!</p>\n<p>I'm sure they have some impressive mitigations going on behind the scenes. I recently learned that the summarization applied by the WebFetch function in Claude Code and now in Cowork is partly intended as a prompt injection protection layer via <a href=\"https://x.com/bcherny/status/1989025306980860226\">this tweet</a> from Claude Code creator Boris Cherny:</p>\n<blockquote>\n<p>Summarization is one thing we do to reduce prompt injection risk. Are you running into specific issues with it?</p>\n</blockquote>\n<p>But Anthropic are being honest here with their warnings: they can attempt to filter out potential attacks all they like but the one thing they can't provide is guarantees that no future attack will be found that sneaks through their defenses and steals your data (see <a href=\"https://simonwillison.net/2025/Jun/16/the-lethal-trifecta/\">the lethal trifecta</a> for more on this.)</p>\n<p>The problem with prompt injection remains that until there's a high profile incident it's really hard to get people to take it seriously. I myself have all sorts of Claude Code usage that could cause havoc if a malicious injection got in. Cowork does at least run in a filesystem sandbox by default, which is more than can be said for my <code>claude --dangerously-skip-permissions</code> habit!</p>\n<p>I wrote more about this in my 2025 round-up: <a href=\"https://simonwillison.net/2025/Dec/31/the-year-in-llms/#the-year-of-yolo-and-the-normalization-of-deviance\">The year of YOLO and the Normalization of Deviance</a>.</p>\n<h4 id=\"this-is-still-a-strong-signal-of-the-future\">This is still a strong signal of the future</h4>\n<p>Security worries aside, Cowork represents something really interesting. This is a general agent that looks well positioned to bring the wildly powerful capabilities of Claude Code to a wider audience.</p>\n<p>I would be very surprised if Gemini and OpenAI don't follow suit with their own offerings in this category.</p>\n<p>I imagine OpenAI are already regretting burning the name \"ChatGPT Agent\" on their janky, experimental and mostly forgotten browser automation tool <a href=\"https://simonwillison.net/2025/Aug/4/chatgpt-agents-user-agent/\">back in August</a>!</p>\n<h4 id=\"bonus-and-a-silly-logo\">Bonus: and a silly logo</h4>\n<p>bashtoni <a href=\"https://news.ycombinator.com/item?id=46593022#46593553\">on Hacker News</a>:</p>\n<blockquote>\n<p>Simple suggestion: logo should be a cow and and orc to match how I originally read the product name.</p>\n</blockquote>\n<p>I couldn't resist <a href=\"https://gist.github.com/simonw/d06dec3d62dee28f2bd993eb78beb2ce\">throwing that one at Nano Banana</a>:</p>\n<p><img alt=\"An anthropic style logo with a cow and an ork on it\" src=\"https://static.simonwillison.net/static/2026/cow-ork.jpg\" /></p>\n    \n        <p>Tags: <a href=\"https://simonwillison.net/tags/sandboxing\">sandboxing</a>, <a href=\"https://simonwillison.net/tags/ai\">ai</a>, <a href=\"https://simonwillison.net/tags/prompt-injection\">prompt-injection</a>, <a href=\"https://simonwillison.net/tags/generative-ai\">generative-ai</a>, <a href=\"https://simonwillison.net/tags/llms\">llms</a>, <a href=\"https://simonwillison.net/tags/anthropic\">anthropic</a>, <a href=\"https://simonwillison.net/tags/claude\">claude</a>, <a href=\"https://simonwillison.net/tags/ai-agents\">ai-agents</a>, <a href=\"https://simonwillison.net/tags/claude-code\">claude-code</a>, <a href=\"https://simonwillison.net/tags/lethal-trifecta\">lethal-trifecta</a>, <a href=\"https://simonwillison.net/tags/claude-cowork\">claude-cowork</a></p>", "source_url": "https://simonwillison.net/atom/everything/", "article_url": "https://simonwillison.net/2026/Jan/12/claude-cowork/#atom-everything", "pub_date": "2026-01-12T21:46:13+00:00", "label": true, "category": "ai_ml", "notes": "heuristic label"}
{"id": "de79048745755144", "title": "OpenAI partners with Cerebras", "description": "OpenAI partners with Cerebras to add 750MW of high-speed AI compute, reducing inference latency and making ChatGPT faster for real-time AI workloads.", "source_url": "https://openai.com/blog/rss.xml", "article_url": "https://openai.com/index/cerebras-partnership", "pub_date": "2026-01-14T14:00:00+00:00", "label": true, "category": "ai_ml", "notes": "heuristic label"}
{"id": "4e4cfd65503e8cd9", "title": "The state of enterprise AI", "description": "Key findings from OpenAI‚Äôs enterprise data show accelerating AI adoption, deeper integration, and measurable productivity gains across industries in 2025.", "source_url": "https://openai.com/blog/rss.xml", "article_url": "https://openai.com/index/the-state-of-enterprise-ai-2025-report", "pub_date": "2025-12-08T04:00:00+00:00", "label": true, "category": "ai_ml", "notes": "heuristic label"}
{"id": "62286e49d21a677a", "title": "Fighting the New York Times‚Äô invasion of user privacy", "description": "OpenAI is fighting the New York Times‚Äô demand for 20 million private ChatGPT conversations and accelerating new security and privacy protections to protect your data.", "source_url": "https://openai.com/blog/rss.xml", "article_url": "https://openai.com/index/fighting-nyt-user-privacy-invasion", "pub_date": "2025-11-12T06:00:00+00:00", "label": true, "category": "other", "notes": "heuristic label"}
{"id": "1c164e28d04f5478", "title": "Expanding Stargate to Michigan", "description": "OpenAI is expanding Stargate to Michigan with a new one-gigawatt campus that strengthens America‚Äôs AI infrastructure. The project will create jobs, drive investment, and support economic growth across the Midwest.", "source_url": "https://openai.com/blog/rss.xml", "article_url": "https://openai.com/index/expanding-stargate-to-michigan", "pub_date": "2025-10-30T13:30:00+00:00", "label": false, "category": "other", "notes": "heuristic label"}
{"id": "a5bb17987f7195e6", "title": "Quoting Linus Torvalds", "description": "<blockquote cite=\"https://github.com/torvalds/AudioNoise/blob/71b256a7fcb0aa1250625f79838ab71b2b77b9ff/README.md\"><p>Also note that the python visualizer tool has been basically written by vibe-coding. I know more about analog filters -- and that's not saying much -- than I do about python. It started out as my typical \"google and do the monkey-see-monkey-do\" kind of programming, but then I cut out the middle-man -- me -- and just used Google Antigravity to do the audio sample visualizer.</p></blockquote>\n<p class=\"cite\">&mdash; <a href=\"https://github.com/torvalds/AudioNoise/blob/71b256a7fcb0aa1250625f79838ab71b2b77b9ff/README.md\">Linus Torvalds</a>, Another silly guitar-pedal-related repo</p>\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/ai\">ai</a>, <a href=\"https://simonwillison.net/tags/vibe-coding\">vibe-coding</a>, <a href=\"https://simonwillison.net/tags/linus-torvalds\">linus-torvalds</a>, <a href=\"https://simonwillison.net/tags/python\">python</a>, <a href=\"https://simonwillison.net/tags/llms\">llms</a>, <a href=\"https://simonwillison.net/tags/generative-ai\">generative-ai</a></p>", "source_url": "https://simonwillison.net/atom/everything/", "article_url": "https://simonwillison.net/2026/Jan/11/linus-torvalds/#atom-everything", "pub_date": "2026-01-11T02:29:58+00:00", "label": false, "category": "other", "notes": "heuristic label"}
{"id": "ca760d04a82f42a0", "title": "Introducing swift-huggingface: The Complete Swift Client for Hugging Face", "description": "", "source_url": "https://huggingface.co/blog/feed.xml", "article_url": "https://huggingface.co/blog/swift-huggingface", "pub_date": "2025-12-05T00:00:00+00:00", "label": true, "category": "other", "notes": "heuristic label"}
{"id": "7529ccf31d3ea467", "title": "Deepening our collaboration with the U.S. Department of Energy", "description": "OpenAI and the U.S. Department of Energy have signed a memorandum of understanding to deepen collaboration on AI and advanced computing in support of scientific discovery. The agreement builds on ongoing work with national laboratories and helps establish a framework for applying AI to high-impact research across the DOE ecosystem.", "source_url": "https://openai.com/blog/rss.xml", "article_url": "https://openai.com/index/us-department-of-energy-collaboration", "pub_date": "2025-12-18T11:00:00+00:00", "label": false, "category": "other", "notes": "heuristic label"}
{"id": "3559b332c460f233", "title": "The Pulse #159: Long-running agents and orchestrating lots of them", "description": "One trend this year may well be experimentation with agent orchestration and long-running agents &#8211; which is already underway. Also: Claude Code bans OpenCode while Codex embraces it, and more", "source_url": "https://newsletter.pragmaticengineer.com/feed", "article_url": "https://newsletter.pragmaticengineer.com/p/the-pulse-159-long-running-agents", "pub_date": "2026-01-15T17:09:41+00:00", "label": true, "category": "ai_ml", "notes": "heuristic label"}
{"id": "781e1e42d9462d8d", "title": "OpenAI and Target team up on new AI-powered experiences", "description": "OpenAI and Target are partnering to bring a new Target app to ChatGPT, offering personalized shopping and faster checkout. Target will also expand its use of ChatGPT Enterprise to boost productivity and guest experiences.", "source_url": "https://openai.com/blog/rss.xml", "article_url": "https://openai.com/index/target-partnership", "pub_date": "2025-11-19T06:00:00+00:00", "label": true, "category": "ai_ml", "notes": "heuristic label"}
{"id": "068696d559e57e5a", "title": "Introducing Aardvark: OpenAI‚Äôs agentic security researcher", "description": "OpenAI introduces Aardvark, an AI-powered security researcher that autonomously finds, validates, and helps fix software vulnerabilities at scale. The system is in private beta‚Äîsign up to join early testing.", "source_url": "https://openai.com/blog/rss.xml", "article_url": "https://openai.com/index/introducing-aardvark", "pub_date": "2025-10-30T11:00:00+00:00", "label": true, "category": "security", "notes": "heuristic label"}
{"id": "9fa9b98f102e76f9", "title": "Inside JetBrains‚Äîthe company reshaping how the world writes code", "description": "JetBrains is integrating GPT-5 across its coding tools, helping millions of developers design, reason, and build software faster.", "source_url": "https://openai.com/blog/rss.xml", "article_url": "https://openai.com/index/jetbrains-2025", "pub_date": "2025-11-25T00:00:00+00:00", "label": true, "category": "ai_ml", "notes": "heuristic label"}
{"id": "0d715a616dfc3840", "title": "Early experiments in accelerating science with GPT-5", "description": "OpenAI introduces the first research cases showing how GPT-5 accelerates scientific progress across math, physics, biology, and computer science. Explore how AI and researchers collaborate to generate proofs, uncover new insights, and reshape the pace of discovery.", "source_url": "https://openai.com/blog/rss.xml", "article_url": "https://openai.com/index/accelerating-science-gpt-5", "pub_date": "2025-11-20T00:00:00+00:00", "label": true, "category": "ai_ml", "notes": "heuristic label"}
{"id": "ae66f586c6a6eab1", "title": "Unrolling the Codex agent loop", "description": "A technical deep dive into the Codex agent loop, explaining how Codex CLI orchestrates models, tools, prompts, and performance using the Responses API.", "source_url": "https://openai.com/blog/rss.xml", "article_url": "https://openai.com/index/unrolling-the-codex-agent-loop", "pub_date": "2026-01-23T12:00:00+00:00", "label": true, "category": "ai_ml", "notes": "manual: OpenAI model/agent research announcement."}
{"id": "1fb5d12e07585775", "title": "Codex is Open Sourcing AI models", "description": "", "source_url": "https://huggingface.co/blog/feed.xml", "article_url": "https://huggingface.co/blog/hf-skills-training-codex", "pub_date": "2025-12-11T00:00:00+00:00", "label": true, "category": "ai_ml", "notes": "heuristic label"}
{"id": "17fddf08e4324610", "title": "1 million business customers putting AI to work", "description": "More than 1 million business customers around the world now use OpenAI. Across healthcare, life sciences, financial services, and more, ChatGPT and our APIs are driving a new era of intelligent, AI-powered work.", "source_url": "https://openai.com/blog/rss.xml", "article_url": "https://openai.com/index/1-million-businesses-putting-ai-to-work", "pub_date": "2025-11-05T05:00:00+00:00", "label": true, "category": "ai_ml", "notes": "heuristic label"}
{"id": "f3d4409b5d77c773", "title": "Holiday gift ideas for techies", "description": "Gift ideas and inspiration for the tech workers in your life (and maybe yourself) this holiday season", "source_url": "https://newsletter.pragmaticengineer.com/feed", "article_url": "https://newsletter.pragmaticengineer.com/p/copy-holiday-gift-ideas-for-techies-2025", "pub_date": "2025-11-25T17:30:00+00:00", "label": false, "category": "other", "notes": "heuristic label"}
{"id": "607717d221f2e6c9", "title": "How Temporal Powers Reliable Cloud Operations at Netflix", "description": "<p>By <a href=\"https://www.linkedin.com/in/jacobmeyers35/\">Jacob Meyers</a> and <a href=\"https://www.linkedin.com/in/robzienert/\">Rob¬†Zienert</a></p><p><a href=\"https://temporal.io/\">Temporal</a> is a <a href=\"https://docs.temporal.io/evaluate/understanding-temporal#durable-execution\">Durable Execution</a> platform which allows you to write code ‚Äúas if failures don‚Äôt exist‚Äù. It‚Äôs become increasingly critical to Netflix since its initial adoption in 2021, with users ranging from the operators of our <a href=\"https://about.netflix.com/en/news/how-netflix-works-with-isps-around-the-globe-to-deliver-a-great-viewing-experience\">Open Connect</a> global CDN to our <a href=\"https://medium.com/netflix-techblog/behind-the-streams-live-at-netflix-part-1-d23f917c2f40\">Live</a> reliability teams now depending on Temporal to operate their business-critical services. In this post, I‚Äôll give a high-level overview of what Temporal offers users, the problems we were experiencing operating Spinnaker that motivated its initial adoption at Netflix, and how Temporal helped us reduce the number of transient deployment failures at Netflix from <strong>4% to¬†0.0001%</strong>.</p><h3>A Crash Course on (some of) Spinnaker</h3><p><a href=\"https://netflixtechblog.com/global-continuous-delivery-with-spinnaker-2a6896c23ba7\">Spinnaker</a> is a multi-cloud continuous delivery platform that powers the vast majority of Netflix‚Äôs software deployments. It‚Äôs composed of several (mostly nautical themed) microservices. Let‚Äôs double-click on two in particular to understand the problems we were facing that led us to adopting Temporal.</p><p>In case you‚Äôre completely new to Spinnaker, Spinnaker‚Äôs fundamental tool for deployments is the <em>Pipeline</em>. A Pipeline is composed of a sequence of steps called <em>Stages</em>, which themselves can be decomposed into one or more <em>Tasks</em>, or other Stages. An example deployment pipeline for a production service may consist of these stages: Find Image -&gt; Run Smoke Tests -&gt; Run Canary -&gt; Deploy to us-east-2 -&gt; Wait -&gt; Deploy to us-east-1.</p><figure><img alt=\"An example Spinnaker Pipeline\" src=\"https://cdn-images-1.medium.com/max/1024/1*7sGhc8LhyqQlW9Uiq76TWQ.png\" /><figcaption>An example Spinnaker Pipeline for a Netflix¬†service</figcaption></figure><p>Pipeline configuration is extremely flexible. You can have Stages run completely serially, one after another, or you can have a mix of concurrent and serial Stages. Stages can also be executed conditionally based on the result of previous stages. This brings us to our first Spinnaker service: <em>Orca</em>. Orca is the <a href=\"https://raw.githubusercontent.com/spinnaker/orca/refs/heads/master/logo.jpg\">orca-stration</a> engine of Spinnaker. It‚Äôs responsible for managing the execution of the Stages and Tasks that a Pipeline unrolls into and coordinating with other Spinnaker services to actually execute¬†them.</p><p>One of those collaborating services is called <em>Clouddriver</em>. In the example Pipeline above, some of the Stages will require interfacing with cloud infrastructure. For example, the canary deployment involves creating ephemeral hosts to run an experiment, and a full deployment of a new version of the service may involve spinning up new servers and then tearing down the old ones. We call these sorts of operations that mutate cloud infrastructure <em>Cloud Operations</em>. Clouddriver‚Äôs job is to decompose and execute Cloud Operations sent to it by Orca as part of a deployment. Cloud Operations sent from Orca to Clouddriver are relatively high level (for example: createServerGroup), so Clouddriver understands how to translate these into lower-level cloud provider API¬†calls.</p><p>Pain points in the interaction between Orca and Clouddriver and the implementation details of Cloud Operation execution in Clouddriver are what led us to look for new solutions and ultimately migrate to Temporal, so we‚Äôll next look at the anatomy of a Cloud Operation. Cloud Operations in the OSS version of Spinnaker still work as described below, so motivated readers can follow along in <a href=\"https://github.com/spinnaker/clouddriver\">source code</a>, however our migration to Temporal is entirely closed-source following a fork from OSS in 2020 to allow Netflix to make larger pivots to the product such as this¬†one.</p><h4><strong>The Original Cloud Operation Flow</strong></h4><p>A Cloud Operation‚Äôs execution goes something like¬†this:</p><ol><li>Orca, in orchestrating a Pipeline execution, decides a particular Cloud Operation needs to be performed. It sends a POST request to Clouddriver‚Äôs /ops endpoint with an untyped bag-of-fields.</li><li>Clouddriver attempts to resolve the operation Orca sent into a set of AtomicOperation s‚Äî internal operations that only Clouddriver understands.</li><li>If the payload was valid and Clouddriver successfully resolved the operation, it will immediately return a Task ID to¬†Orca.</li><li>Orca will immediately begin polling Clouddriver‚Äôs GET /task/&lt;id&gt; endpoint to keep track of the status of the Cloud Operation.</li><li>Asynchronously, Clouddriver begins executing AtomicOperations using <em>its own</em> internal orchestration engine. Ultimately, the AtomicOperations resolve into cloud provider API calls. As the Cloud Operation progresses, Clouddriver updates an internal state store to surface progress to¬†Orca.</li><li>Eventually, if all went well, Clouddriver will mark the Cloud Operation complete, which eventually surfaces to Orca in its polling. Orca considers the Cloud Operation finished, and the deployment can progress.</li></ol><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*Y57y00EsM2YGRph9IRNmLQ.png\" /><figcaption>A sequence diagram of a Cloud Operation execution</figcaption></figure><p>This works well enough on the happy path, but veer off the happy path and dragons begin to¬†emerge:</p><ol><li>Clouddriver has its own internal orchestration system independent of Orca to allow Orca to query the progress of Cloud Operation. This is largely undifferentiated lifting relative to Clouddriver‚Äôs goal of actuating cloud infrastructure changes, and ultimately adds complexity and surface area for bugs to the application. Additionally, Orca is tightly coupled to Clouddriver‚Äôs orchestration system‚Ää‚Äî‚Ääit must understand how to poll Clouddriver, interpret the status, and handle errors returned by Clouddriver.</li><li>Distributed systems are messy‚Ää‚Äî‚Äänetworks and external services are unreliable. While executing a Cloud Operation, Clouddriver could experience transient network issues, or the cloud provider it‚Äôs attempting to call into may be having an outage, or any number of issues in between. Despite all of this, Clouddriver must be as reliable as reasonably possible as a core platform service. To deal with this shape of issue, Clouddriver internally evolved complex retry logic, further adding cognitive complexity to the¬†system.</li><li>Remember how a Cloud Operation gets decomposed by Clouddriver into AtomicOperations? Sometimes, if there‚Äôs a failure in the middle of a Cloud Operation, we need to be able to roll back what was done in AtomicOperations prior to the failure. This led to a homegrown Saga framework being implemented inside Clouddriver. While this did result in a big step forward in reliability of Cloud Operations facing transient failures because the Saga framework <em>also</em> allowed replaying partially-failed Cloud Operations, it added yet more undifferentiated lifting inside the¬†service.</li><li>The task state kept by Clouddriver was <em>instance-local</em>. In other words, if the Clouddriver instance carrying out a Cloud Operation crashed, that Cloud Operation state was lost, and Orca would eventually time out polling for the task status. The Saga implementation mentioned above mitigated this for certain operations, but was not widely adopted across all cloud providers supported by Spinnaker.</li></ol><p>We introduced a <em>lot</em> of incidental complexity into Clouddriver in an effort to keep Cloud Operation execution reliable, and despite all this deployments still failed around 4% of the time due to transient Cloud Operation failures.</p><p>Now, I can already hear you saying: ‚ÄúSo what? Can‚Äôt people re-try their deployments if they fail?‚Äù While true, some pipelines take <em>days</em> to complete for complex deployments, and a failed Cloud Operation mid-way through requires re-running the <em>whole</em> thing. This was detrimental to engineering productivity at Netflix in a non-trivial way. Rather than continue trying to build a faster horse, we began to look elsewhere for our reliable orchestration requirements, which is where Temporal comes¬†in.</p><h3>Temporal: Basic¬†Concepts</h3><p>Temporal is an open source product that offers a durable execution platform for your applications. Durable execution means that the platform will ensure your programs run to completion despite adverse conditions. With Temporal, you organize your business logic into <em>Workflows</em>, which are a deterministic series of steps. The steps inside of Workflows are called <em>Activities</em>, which is where you encapsulate all your non-deterministic logic that needs to happen in the course of executing your Workflows. As your Workflows execute in processes called <em>Workers</em>, the Temporal server durably stores their execution state so that in the event of failures your Workflows can be retried or even migrated to a different Worker. This makes Workflows incredibly resilient to the sorts of transient failures Clouddriver was susceptible to. Here‚Äôs a simple example Workflow in Java that runs an Activity to send an email once every 30¬†days:</p><pre>@WorkflowInterface<br />public interface SleepForDaysWorkflow {<br />    @WorkflowMethod<br />    void run();<br />}<br /><br />public class SleepForDaysWorkflowImpl implements SleepForDaysWorkflow {<br /><br />    private final SendEmailActivities emailActivities =<br />            Workflow.newActivityStub(<br />                    SendEmailActivities.class,<br />                    ActivityOptions.newBuilder()<br />                            .setStartToCloseTimeout(Duration.ofSeconds(10))<br />                            .build());<br /><br />    @Override<br />    public void run() {<br />        while (true) {<br />            // Activities already carry retries/timeouts via options.<br />            emailActivities.sendEmail();<br /><br />            // Pause the workflow for 30 days before sending the next email.<br />            Workflow.sleep(Duration.ofDays(30));<br />        }<br />    }<br />}<br /><br />@ActivityInterface<br />public interface SendEmailActivities {<br />    void sendEmail();<br />}</pre><p>There‚Äôs some interesting things to note about this Workflow:</p><ol><li>Workflows and Activities are just code, so you can test them using the same techniques and processes as the rest of your codebase.</li><li>Activities are automatically retried by Temporal with configurable exponential backoff.</li><li>Temporal manages all the execution state of the Workflow, including timers (like the one used by Workflow.sleep). If the Worker executing this workflow were to have its power cable unplugged, Temporal would ensure another Worker continues to execute it (even during the 30 day¬†sleep).</li><li>Workflow sleeps are not compute-intensive, and they don‚Äôt tie up the¬†process.</li></ol><p>You might already begin to see how Temporal solves a lot of the problems we had with Clouddriver. Ultimately, we decided to pull the trigger on migrating Cloud Operation execution to Temporal.</p><h3>Cloud Operations with¬†Temporal</h3><p>Today, we execute Cloud Operations as Temporal workflows. Here‚Äôs what that looks¬†like.</p><ol><li>Orca, using a Temporal client, sends a request to Temporal to execute an UntypedCloudOperationRunner Workflow. The contract of the Workflow looks something like¬†this:</li></ol><pre>@WorkflowInterface<br />interface UntypedCloudOperationRunner {<br />  /**<br />   * Runs a cloud operation given an untyped payload.<br />   *<br />   * WorkflowResult is a thin wrapper around OutputType providing a standard contract for<br />   * clients to determine if the CloudOperation was successful and fetching any errors.<br />   */<br />  @WorkflowMethod<br />  fun &lt;OutputType : CloudOperationOutput&gt; run(stageContext: Map&lt;String, Any?&gt;, operationType: String): WorkflowResult&lt;OutputType&gt;<br />}</pre><p>2. The Clouddriver Temporal worker is constantly polling Temporal for work. A worker will eventually see a task for an UntypedCloudOperationRunner Workflow and start executing it.</p><p>3. Similar to before with resolution into AtomicOperations, Clouddriver does some pre-processing of the bag-of-fields in stageContext and resolves it to a strongly typed implementation of the CloudOperation Workflow interface based on the operationType input and the stageContext:</p><pre>interface CloudOperation&lt;I : CloudOperationInput, O : CloudOperationOutput&gt; {<br />  @WorkflowMethod<br />  fun operate(input: I, credentials: AccountCredentials&lt;out Any&gt;): O<br />}</pre><p>4. Clouddriver starts a <a href=\"https://docs.temporal.io/child-workflows\">Child Workflow</a> execution of the CloudOperation implementation it resolved. The child workflow will execute Activities which handle the actual cloud provider API calls to mutate infrastructure.</p><p>5. Orca uses its Temporal Client to await completion of the UntypedCloudOperationRunner Workflow. Once it‚Äôs complete, Temporal notifies the client and sends the result and Orca can continue progressing the deployment.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*leM3bH8iyb65_cmtl3vm4A.png\" /><figcaption>Sequence diagram of a Cloud Operation execution with¬†Temporal</figcaption></figure><h3>Results and Lessons Learned from the Migration</h3><p>A shiny new architecture is great, but equally important is the non-glamorous work of refactoring legacy systems to fit the new architecture. How did we integrate Temporal into critical dependencies of all Netflix engineers transparently?</p><p>The answer, of course, is a combination of abstraction and dynamic configuration. We built a CloudOperationRunner interface in Orca to encapsulate whether the Cloud Operation was being executed via the legacy path or Temporal. At runtime, <a href=\"https://netflixtechblog.com/announcing-archaius-dynamic-properties-in-the-cloud-bc8c51faf675\">Fast Properties</a> (Netflix‚Äôs dynamic configuration system) determined which path a stage that needed to execute a Cloud Operation would take. We could set these properties quite granularly‚Ää‚Äî‚Ääby Stage type, cloud provider account, Spinnaker application, Cloud Operation type (createServerGroup), and cloud provider (either AWS or <a href=\"https://netflix.github.io/titus/\">Titus</a> in our case). The Spinnaker services themselves were the first to be deployed using Temporal, and within two quarters, all applications at Netflix were onboarded.</p><h4>Impact</h4><p>What did we have to show for it all? With Temporal as the orchestration engine for Cloud Operations, the percentage of deployments that failed due to transient Cloud Operation failures dropped from 4% to 0.0001%. For those keeping track at home, that‚Äôs a four and a half order of magnitude reduction. Virtually eliminating this failure mode for deployments was a huge win for developer productivity, especially for teams with long and complex deployment pipelines.</p><p>Beyond the improvement in deployment success metrics, we saw a number of other benefits:</p><ol><li>Orca no longer needs to directly communicate with Clouddriver to start Cloud Operations or poll their status with Temporal as the intermediary. The services are less coupled, which is a win for maintainability.</li><li>Speaking of maintainability, with Temporal doing the heavy lifting of orchestration and retries inside of Clouddriver, we got to remove a lot of the homegrown logic we‚Äôd built up over the years for the same¬†purpose.</li><li>Since Temporal manages execution state, Clouddriver instances became stateless and Cloud Operation execution can bounce between instances with impunity. We can treat Clouddriver instances more like cattle and enable things like <a href=\"https://netflixtechblog.com/the-netflix-simian-army-16e57fbab116\">Chaos Monkey</a> for the service which we were previously prevented from¬†doing.</li><li>Migrating Cloud Operation steps into Activities was a forcing function to re-write the logic to be idempotent. Since Temporal retries activities by default, it‚Äôs generally recommended they be idempotent. This alone fixed a number of issues that existed previously when operations were retried in Clouddriver.</li><li>We set the retry timeout for Activities in Clouddriver to be two hours by default. This gives us a long leash to fix-forward or rollback Clouddriver if we introduce a regression before customer deployments fail‚Ää‚Äî‚Ääto them, it might just look like a deployment is taking longer than¬†usual.</li><li>Cloud Operations are much easier to introspect than before. Temporal ships with a great UI to help visualize Workflow and Activity executions, which is a huge boon for debugging live Workflows executing in production. The Temporal SDKs and server also emit a lot of useful¬†metrics.</li></ol><figure><img alt=\"A Cloud Operation Workflow as seen from the Temporal UI. This operation executes 3 Activities: DescribeAutoScalingGroup, GetHookConfigurations, and ResizeServerGroup\" src=\"https://cdn-images-1.medium.com/max/1024/1*zmCyjwzTXji921mulJjmTw.png\" /><figcaption>Execution of a resizeServerGroup Cloud Operation as seen from the Temporal UI. This operation executes 3 Activities: DescribeAutoScalingGroup, GetHookConfigurations, and ResizeServerGroup</figcaption></figure><h4>Lessons Learned</h4><p>With the benefit of hindsight, there are also some lessons we can share from this migration:</p><p>1. <strong>Avoid unnecessary Child Workflows</strong>: Structuring Cloud Operations as an UntypedCloudOperationRunner Workflow that starts Child Workflows to actually execute the Cloud Operation‚Äôs logic was unnecessary and the indirection made troubleshooting more difficult. There are <a href=\"https://community.temporal.io/t/purpose-of-child-workflows/652\">situations</a> where Child Workflows are appropriate, but in this case we were using them as a tool for code organization, which is generally unnecessary. We could‚Äôve achieved the same effect with class composition in the top-level parent Workflow.</p><p>2. <strong>Use single argument objects</strong>: At first, we structured Workflow and Activity functions with variable arguments, much as you‚Äôd write normal functions. This can be problematic for Temporal because of Temporal‚Äôs <a href=\"https://community.temporal.io/t/workflow-determinism/4027\">determinism constraints</a>. Adding or removing an argument from a function signature is <strong>not</strong> a backward-compatible change, and doing so can break long-running workflows‚Ää‚Äî‚Ääand it‚Äôs not immediately obvious in code review your change is problematic. The preferred pattern is to use a single serializable class to host all your arguments for Workflows and Activities‚Ää‚Äî‚Ääthese can be more freely changed without breaking determinism.</p><p>3. <strong>Separate business failures from workflow failures</strong>: We like the pattern of the WorkflowResult type that UntypedCloudOperationRunner returns in the interface above. It allows us to communicate business process failures without failing the Workflow itself and have more overall nuance in error handling. This is a pattern we‚Äôve carried over to other Workflows we‚Äôve implemented since.</p><h3>Temporal at Netflix¬†Today</h3><p>Temporal adoption has skyrocketed at Netflix since its initial introduction for Spinnaker. Today, we have hundreds of use cases, and we‚Äôve seen adoption double in the last year with no signs of slowing¬†down.</p><p>One major difference between initial adoption and today is that Netflix migrated from an on-prem Temporal deployment to using <a href=\"https://temporal.io/cloud\">Temporal Cloud</a>, which is Temporal‚Äôs SaaS offering of the Temporal server. This has let us scale Temporal adoption while running a lean team. We‚Äôve also built up a robust internal platform around Temporal Cloud to integrate with Netflix‚Äôs internal ecosystem and make onboarding for our developers as easy as possible. Stay tuned for a future post digging into more specifics of our Netflix Temporal platform.</p><h3>Acknowledgement</h3><p>We all stand on the shoulders of giants in software. I want to call out that I‚Äôm retelling the work of my two stunning colleagues <a href=\"https://www.linkedin.com/in/chris-smalley/\">Chris Smalley</a> and <a href=\"https://www.linkedin.com/in/robzienert/\">Rob Zienert</a> in this post, who were the two aforementioned engineers who introduced Temporal and carried out the migration.</p><img alt=\"\" height=\"1\" src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=73c69ccb5953\" width=\"1\" /><hr /><p><a href=\"https://netflixtechblog.com/how-temporal-powers-reliable-cloud-operations-at-netflix-73c69ccb5953\">How Temporal Powers Reliable Cloud Operations at Netflix</a> was originally published in <a href=\"https://netflixtechblog.com\">Netflix TechBlog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>", "source_url": "https://netflixtechblog.com/feed", "article_url": "https://netflixtechblog.com/how-temporal-powers-reliable-cloud-operations-at-netflix-73c69ccb5953?source=rss----2615bd06b42e---4", "pub_date": "2025-12-15T23:51:59+00:00", "label": false, "category": "other", "notes": "heuristic label"}
{"id": "dcb9afae1c73ef07", "title": "Easily Build and Share ROCm Kernels with Hugging Face", "description": "", "source_url": "https://huggingface.co/blog/feed.xml", "article_url": "https://huggingface.co/blog/build-rocm-kernels", "pub_date": "2025-11-17T00:00:00+00:00", "label": false, "category": "other", "notes": "heuristic label"}
{"id": "a1d675b7e83cc573", "title": "The Pragmatic Engineer in 2025", "description": "The most-read articles of the year, plus some personal favorites, and a look back at a busy year in tech", "source_url": "https://newsletter.pragmaticengineer.com/feed", "article_url": "https://newsletter.pragmaticengineer.com/p/the-pragmatic-engineer-in-2025", "pub_date": "2025-12-23T14:03:04+00:00", "label": true, "category": "ai_ml", "notes": "heuristic label"}
{"id": "24f2861111d8d050", "title": "Quoting Jeremy Daer", "description": "<blockquote cite=\"https://twitter.com/dhh/status/2012543705161326941\"><p><em>[On agents using CLI tools in place of REST APIs]</em> To save on context window, yes, but moreso to improve accuracy and success rate when multiple tool calls are involved, particularly when calls must be correctly chained e.g. for pagination, rate-limit backoff, and recognizing authentication failures.</p>\n<p>Other major factor: which models can wield the skill? Using the CLI lowers the bar so cheap, fast models (gpt-5-nano, haiku-4.5) can reliably succeed. Using the raw APl is something only the costly \"strong\" models (gpt-5.2, opus-4.5) can manage, and it squeezes a ton of thinking/reasoning out of them, which means multiple turns/iterations, which means accumulating a ton of context, which means burning loads of expensive tokens. For one-off API requests and ad hoc usage driven by a developer, this is reasonable and even helpful, but for an autonomous agent doing repetitive work, it's a disaster.</p></blockquote>\n<p class=\"cite\">&mdash; <a href=\"https://twitter.com/dhh/status/2012543705161326941\">Jeremy Daer</a>, 37signals</p>\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/prompt-engineering\">prompt-engineering</a>, <a href=\"https://simonwillison.net/tags/skills\">skills</a>, <a href=\"https://simonwillison.net/tags/generative-ai\">generative-ai</a>, <a href=\"https://simonwillison.net/tags/37-signals\">37-signals</a>, <a href=\"https://simonwillison.net/tags/ai\">ai</a>, <a href=\"https://simonwillison.net/tags/llms\">llms</a></p>", "source_url": "https://simonwillison.net/atom/everything/", "article_url": "https://simonwillison.net/2026/Jan/17/jeremy-daer/#atom-everything", "pub_date": "2026-01-17T17:06:41+00:00", "label": false, "category": "other", "notes": "heuristic label"}
{"id": "1418f4733c7aeb08", "title": "React2Shell and related RSC vulnerabilities threat brief: early exploitation activity and threat actor techniques", "description": "Early activity indicates that threat actors quickly integrated this vulnerability into their scanning and reconnaissance routines and targeted critical infrastructure including nuclear fuel, uranium and rare earth elements. We outline the tactics they appear to be using and how Cloudflare is protecting customers.", "source_url": "https://blog.cloudflare.com/rss/", "article_url": "https://blog.cloudflare.com/react2shell-rsc-vulnerabilities-exploitation-threat-brief/", "pub_date": "2025-12-11T16:20:00+00:00", "label": true, "category": "security", "notes": "heuristic label"}
{"id": "12d27151ccc69de9", "title": "Why We Use Separate Tech Stacks for Personalization and Experimentation", "description": "<p>The technical and practical rationale for a clear separation between these domains.</p>\n<p>The post <a href=\"https://engineering.atspotify.com/2026/1/why-we-use-separate-tech-stacks-for-personalization-and-experimentation/\">Why We Use Separate Tech Stacks for Personalization and Experimentation</a> appeared first on <a href=\"https://engineering.atspotify.com\">Spotify Engineering</a>.</p>", "source_url": "https://engineering.atspotify.com/feed/", "article_url": "https://engineering.atspotify.com/2026/1/why-we-use-separate-tech-stacks-for-personalization-and-experimentation/", "pub_date": "2026-01-07T14:41:11+00:00", "label": false, "category": "other", "notes": "heuristic label"}
{"id": "e4d0f0c65945cccb", "title": "New features to help SaaS platforms manage risk and stay compliant", "description": "We recently launched three new features that give you more control to fine-tune your risk and compliance strategy, allowing you to use more of Stripe‚Äôs data to inform your approach. Here‚Äôs what‚Äôs new.", "source_url": "https://stripe.com/blog/feed.rss", "article_url": "https://stripe.com/blog/new-features-to-help-saas-platforms-manage-risk-and-stay-compliant", "pub_date": "2025-12-02T00:00:00+00:00", "label": false, "category": "other", "notes": "heuristic label"}
{"id": "cafad22182ef065f", "title": "gpt-oss-safeguard technical report", "description": "gpt-oss-safeguard-120b and gpt-oss-safeguard-20b are two open-weight reasoning models post-trained from the gpt-oss models and trained to reason from a provided policy in order to label content under that policy. In this report, we describe gpt-oss-safeguard‚Äôs capabilities and provide our baseline safety evaluations on the gpt-oss-safeguard models, using the underlying gpt-oss models as a baseline. For more information about the development and architecture of the underlying gpt-oss models, see the original gpt-oss model model card‚Å†.", "source_url": "https://openai.com/blog/rss.xml", "article_url": "https://openai.com/index/gpt-oss-safeguard-technical-report", "pub_date": "2025-10-29T00:00:00+00:00", "label": true, "category": "ai_ml", "notes": "heuristic label"}
{"id": "074dbcba1fe30fbd", "title": "Increasing revenue 300% by bringing AI to SMBs", "description": "Discover how Podium used OpenAI‚Äôs GPT-5 to build ‚ÄúJerry,‚Äù an AI teammate driving 300% growth and transforming how Main Street businesses serve customers.", "source_url": "https://openai.com/blog/rss.xml", "article_url": "https://openai.com/index/podium", "pub_date": "2025-12-11T00:00:00+00:00", "label": true, "category": "business", "notes": "heuristic label"}
{"id": "3f720fc81cc1d479", "title": "Building AI fluency at scale with ChatGPT Enterprise", "description": "Commonwealth Bank of Australia partners with OpenAI to roll out ChatGPT Enterprise to 50,000 employees, building AI fluency at scale to improve customer service and fraud response.", "source_url": "https://openai.com/blog/rss.xml", "article_url": "https://openai.com/index/commonwealth-bank-of-australia", "pub_date": "2025-12-09T00:00:00+00:00", "label": true, "category": "ai_ml", "notes": "heuristic label"}
{"id": "10d34bbec2778ecd", "title": "Granite 4.0 Nano: Just how small can you go?", "description": "", "source_url": "https://huggingface.co/blog/feed.xml", "article_url": "https://huggingface.co/blog/ibm-granite/granite-4-nano", "pub_date": "2025-10-28T14:59:38+00:00", "label": false, "category": "other", "notes": "heuristic label"}
{"id": "3c80e145448e7c85", "title": "Astro is joining Cloudflare", "description": "The Astro Technology Company team ‚Äî the creators of the Astro web framework ‚Äî is joining Cloudflare. We‚Äôre doubling down on making Astro the best framework for content-driven websites, today and in the years to come.", "source_url": "https://blog.cloudflare.com/rss/", "article_url": "https://blog.cloudflare.com/astro-joins-cloudflare/", "pub_date": "2026-01-16T14:00:00+00:00", "label": true, "category": "technology", "notes": "heuristic label"}
{"id": "2f650d7e66c0fd31", "title": "Code Orange: Fail Small ‚Äî our resilience plan following recent incidents", "description": "We have declared ‚ÄúCode Orange: Fail Small‚Äù to focus everyone at Cloudflare on a set of high-priority workstreams with one simple goal: ensure that the cause of our last two global outages never happens again.", "source_url": "https://blog.cloudflare.com/rss/", "article_url": "https://blog.cloudflare.com/fail-small-resilience-plan/", "pub_date": "2025-12-19T22:35:30+00:00", "label": true, "category": "security", "notes": "heuristic label"}
{"id": "2d3460a9e256b0ba", "title": "Introducing ChatGPT Health", "description": "ChatGPT Health is a dedicated experience that securely connects your health data and apps, with privacy protections and a physician-informed design.", "source_url": "https://openai.com/blog/rss.xml", "article_url": "https://openai.com/index/introducing-chatgpt-health", "pub_date": "2026-01-07T00:00:00+00:00", "label": true, "category": "ai_ml", "notes": "heuristic label"}
{"id": "bd597a37c75ce208", "title": "Inside Mirakl's agentic commerce vision", "description": "Mirakl is redefining commerce through AI agents and ChatGPT Enterprise‚Äîachieving faster documentation, smarter customer support, and building toward agent-native commerce with Mirakl Nexus.", "source_url": "https://openai.com/blog/rss.xml", "article_url": "https://openai.com/index/mirakl", "pub_date": "2025-12-01T22:00:00+00:00", "label": true, "category": "ai_ml", "notes": "heuristic label"}
{"id": "cbc6af947a81e23d", "title": "How We Used Codex to Ship Sora for Android in 28 Days", "description": "OpenAI shipped Sora for Android in 28 days using Codex. AI-assisted planning, translation, and parallel coding workflows helped a nimble team deliver rapid, reliable development.", "source_url": "https://openai.com/blog/rss.xml", "article_url": "https://openai.com/index/shipping-sora-for-android-with-codex", "pub_date": "2025-12-12T00:00:00+00:00", "label": false, "category": "other", "notes": "heuristic label"}
{"id": "5633db15c361278b", "title": "Introducing ChatGPT Go, now available worldwide", "description": "ChatGPT Go is now available worldwide, offering expanded access to GPT-5.2 Instant, higher usage limits, and longer memory‚Äîmaking advanced AI more affordable globally.", "source_url": "https://openai.com/blog/rss.xml", "article_url": "https://openai.com/index/introducing-chatgpt-go", "pub_date": "2026-01-16T00:00:00+00:00", "label": true, "category": "ai_ml", "notes": "heuristic label"}
{"id": "db1b90443be01fc9", "title": "Netflix Live Origin", "description": "<p><a href=\"https://www.linkedin.com/in/xiaomei-liu-b475711/\">Xiaomei Liu</a>, <a href=\"https://www.linkedin.com/in/joseph-lynch-9976a431/\">Joseph Lynch</a>, <a href=\"https://www.linkedin.com/in/chrisnewton2/\">Chris¬†Newton</a></p><h3>Introduction</h3><p><a href=\"https://netflixtechblog.com/building-a-reliable-cloud-live-streaming-pipeline-for-netflix-8627c608c967\">Behind the Streams: Building a Reliable Cloud Live Streaming Pipeline for Netflix</a> introduced the architecture of the streaming pipeline. This blog post looks at the custom Origin Server we built for Live‚Ää‚Äî‚Ääthe Netflix Live Origin. It sits at the demarcation point between the cloud live streaming pipelines on its upstream side and the distribution system, Open Connect, Netflix‚Äôs in-house Content Delivery Network (CDN), on its downstream side, and acts as a broker managing what content makes it out to Open Connect and ultimately to the client¬†devices.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*44sJszKXEHZvSHnEQgYiiw.png\" /><figcaption><strong>Live Streaming Distribution and Origin Architecture</strong></figcaption></figure><p>Netflix Live Origin is a multi-tenant microservice operating on EC2 instances within the AWS cloud. We lean on standard HTTP protocol features to communicate with the Live Origin. The Packager pushes segments to it using PUT requests, which place a file into storage at the particular location named in the URL. The storage location corresponds to the URL that is used when the Open Connect side issues the corresponding GET¬†request.</p><p>Live Origin architecture is influenced by key technical decisions of the live streaming architecture. First, resilience is achieved through redundant regional live streaming pipelines, with failover orchestrated at the server-side to reduce client complexity. The implementation of <a href=\"https://netflixtechblog.com/building-a-reliable-cloud-live-streaming-pipeline-for-netflix-8627c608c967\">epoch locking at the cloud encoder</a> enables the origin to select a segment from either encoding pipeline. Second, Netflix adopted a manifest design with <a href=\"https://netflixtechblog.com/behind-the-streams-live-at-netflix-part-1-d23f917c2f40\">segment templates and constant segment duration</a> to avoid frequent manifest refresh. The constant duration templates enable Origin to predict the segment publishing schedule.</p><h3>Multi-pipeline and multi-region aware¬†origin</h3><p>Live streams inevitably contain defects due to the non-deterministic nature of live contribution feeds and strict real-time segment publishing timelines. Common defects¬†include:</p><ul><li><strong>Short segments:</strong> Missing video frames and audio¬†samples.</li><li><strong>Missing segments:</strong> Entire segments are¬†absent.</li><li><strong>Segment timing discontinuity:</strong> Issues with the Track Fragment Decode¬†Time.</li></ul><p>Communicating segment discontinuity from the server to the client via a segment template-based manifest is impractical, and these defective segments can disrupt client streaming.</p><p>The redundant cloud streaming pipelines operate independently, encompassing distinct cloud regions, contribution feeds, encoder, and packager deployments. This independence substantially mitigates the probability of simultaneous defective segments across the dual pipelines. Owing to its strategic placement within the distribution path, the live origin naturally emerges as a component capable of intelligent candidate selection.</p><p>The Netflix Live Origin features multi-pipeline and multi-region awareness. When a segment is requested, the live origin checks candidates from each pipeline in a deterministic order, selecting the first valid one. Segment defects are detected via lightweight media inspection at the packager. This defect information is provided as metadata when the segment is published to the live origin. In the rare case of concurrent defects at the dual pipeline, the segment defects can be communicated downstream for intelligent client-side error concealment.</p><h3>Open Connect streaming optimization</h3><p>When the Live project started, Open Connect had become highly optimised for VOD content delivery‚Ää‚Äî‚Ää<a href=\"https://freenginx.org/en/\">nginx</a> had been chosen many years ago as the Web Server since it is highly capable in this role, and a number of enhancements had been added to it and to the underlying operating system (BSD). Unlike traditional CDNs, Open Connect is more of a distributed origin server‚Ää‚Äî‚ÄäVOD assets are pre-positioned onto carefully selected server machines (OCAs, or Open Connect Appliances) rather than being filled on¬†demand.</p><p>Alongside the VOD delivery, an on-demand fill system has been used for non-VOD assets‚Ää‚Äî‚Ääthis includes artwork and the downloadable portions of the clients, etc. These are also served out of the same <a href=\"https://freenginx.org/en/\">nginx</a> workers, albeit under a distinct server block, using a distinct set of hostnames.</p><p>Live didn‚Äôt fit neatly into this ‚Äòsmall object delivery‚Äô model, so we extended the proxy-caching functionality of <a href=\"https://freenginx.org/en/\">nginx</a> to address Live-specific needs. We will touch on some of these here related to optimized interactions with the Origin Server. Look for a future blog post that will go into more details on the Open Connect¬†side.</p><p>The segment templates provided to clients are also provided to the OCAs as part of the Live Event Configuration data. Using the Availability Start Time and Initial Segment number, the OCA is able to determine the legitimate range of segments for each event at any point in time‚Ää‚Äî‚Äärequests for objects outside this range can be rejected, preventing unnecessary requests going up through the fill hierarchy to the origin. If a request makes it through to the origin, and the segment isn‚Äôt available yet, the origin server will return a 404 Status Code (indicating File Not Found) with the expiration policy of that error so that it can be cached within Open Connect until just before that segment is expected to be published.</p><p>If the Live Origin knows when segments are being pushed to it, and knows what the live edge is‚Ää‚Äî‚Ääwhen a request is received for the immediately next object, rather than handing back another 404 error (which would go all the way back through Open Connect to the client), the Live Origin can ‚Äòhold open‚Äô the request, and service it once the segment has been published to it. By doing this, the degree of chatter within the network handling requests that arrive early has been significantly reduced. As part of this, millisecond grain caching was added to <a href=\"https://freenginx.org/en/\">nginx</a> to enhance the standard HTTP Cache Control, which only works at second granularity, a long time when segments are generated every 2¬†seconds.</p><h4>Streaming metadata enhancement</h4><p>The HTTP standard allows for the addition of request and response headers that can be used to provide additional information as files move between clients and servers. The HTTP headers provide notifications of events within the stream in a highly scalable way that is independently conveyed to client devices, regardless of their playback position within the¬†stream.</p><p>These notifications are provided to the origin by the live streaming pipeline and are inserted by the origin in the form of headers, appearing on the segments generated at that point in time (and persist to future segments‚Ää‚Äî‚Ääthey are cumulative). Whenever a segment is received at an OCA, this notification information is extracted from the response headers and used to update an in-memory data structure, keyed by event ID; and whenever a segment is served from the OCA, the latest such notification data is attached to the response. This means that, given any flow of segments into an OCA, it will always have the most recent notification data, even if all clients requesting it are behind the live edge. In fact, the notification information can be conveyed on any response, not just those supplying new segments.</p><h4>Cache invalidation and origin¬†mask</h4><p>An invalidation system has been available since the early days of the project. It can be used to ‚Äúflush‚Äù all content associated with an event by altering the key used when looking up objects in cache‚Ää‚Äî‚Ääthis is done by incorporating a version number into the cache key that can then be bumped on demand. This is used during pre-event testing so that the network can be returned to a pristine state for the test with minimal¬†fuss.</p><p>Each segment published by the Live Origin conveys the encoding pipeline it was generated by, as well as the region it was requested from. Any issues that are found after segments make their way into the network can be remedied by an enhanced invalidation system that takes such variants into account. It is possible to invalidate (that is, cause to be considered expired) segments in a range of segment numbers, but only if they were sourced from encoder A, or from Encoder A, but only if retrieved from region¬†X.</p><p>In combination with Open Connect‚Äôs enhanced cache invalidation, the Netflix Live Origin allows <em>selective encoding pipeline masking</em> to exclude a range of segments from a particular pipeline when serving segments to Open Connect. The enhanced cache invalidation and origin masking enable live streaming operations to hide known problematic segments (e.g., segments causing client playback errors) from streaming clients once the bad segments are detected, protecting millions of streaming clients during the DVR playback¬†window.</p><h3>Origin storage architecture</h3><p>Our original storage architecture for the Live Origin was simple: just use <a href=\"https://aws.amazon.com/s3/\">AWS S3</a> like we do for SVOD. This served us well initially for our low-traffic events, but as we scaled up we discovered that Live streaming has unique latency and workload requirements that differ significantly from on-demand where we have significant time ahead-of-time to pre-position content. While S3 met its stated uptime guarantees, our strict 2-second retry budget inherent to Live events (where every write is critical) led us to explore optimizations specifically tailored for real-time delivery at scale. AWS S3 is an amazing object store, but our Live streaming requirements were closer to those of a global low-latency highly-available database. So, we went back to the drawing board and started from the requirements. The Origin required:</p><ol><li>[HA Writes] Extremely high <em>write</em> availability, ideally as close to full write availability within a single AWS region, with low second replication delay to other regions. Any failed write operation within 500ms is considered a bug that must be triaged and prevented from re-occurring.</li><li>[Throughput] High write throughput, with hundreds of MiB replicating across¬†regions</li><li>[Large Partitions] Efficiently support O(MiB) writes that accumulate to O(10k) keys per partition with O(GiB) total size per¬†event.</li><li>[Strong Consistency] Within the same region, we needed read-your-write semantics to hit our &lt;1s read delay requirements (must be able to read published segments)</li><li>[Origin Storm] During worst-case load involving Open Connect edge cases, we may need to handle O(<strong>GiB</strong>) of read throughput <em>without affecting writes</em>.</li></ol><p>Fortunately, Netflix had previously invested in building a <a href=\"https://netflixtechblog.com/introducing-netflixs-key-value-data-abstraction-layer-1ea8a0a11b30\">KeyValue Storage Abstraction</a> that cleverly leveraged <a href=\"https://youtu.be/sQ-_jFgOBng?t=1061\">Apache Cassandra</a> to provide chunked storage of MiB or even GiB values. This abstraction was initially built to support cloud saves of Game state. The Live use case would push the boundaries of this solution, however, in terms of availability for writes (#1), cumulative partition size (#3), and read throughput during Origin Storm¬†(#5).</p><h4>High Availability for Writes of Large¬†Payloads</h4><p>The <a href=\"https://youtu.be/paTtLhZFsGE?t=1077\">KeyValue Payload Chunking and Compression Algorithm</a> breaks O(MiB) work down so each part can be idempotently retried and hedged to maintain strict latency service level objectives, as well as spreading the data across the full cluster. When we combine this algorithm with Apache Cassandra‚Äôs local-quorum consistency model, which allows write availability even with an entire Availability Zone outage, plus a write-optimized <a href=\"https://en.wikipedia.org/wiki/Log-structured_merge-tree\">Log-Structured Merge Tree</a> (LSM) storage engine, we could meet the first four requirements. After iterating on the performance and availability of this solution, we were not only able to achieve the write availability required, but did so with a P99 <em>tail</em> latency that was similar to the status quo‚Äôs P50 <em>average </em>latency while also handling cross-region replication behind the scenes for the Origin. This new solution was significantly more expensive (as expected, databases backed by SSD cost more), but minimizing cost was <em>not</em> a key objective and low latency with high availability was:</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*bUPc4gC-mSDcybayhBJQ8g.png\" /><figcaption><strong>Storage System Write Performance</strong></figcaption></figure><h4>High Availability Reads at Gbps Throughputs</h4><p>Now that we solved the write reliability problem, we had to handle the Origin Storm failure case, where potentially dozens of Open Connect top-tier caches could be requesting multiple O(MiB) video segments at once. Our back-of-the-envelope calculations showed worst-case read throughput in the O(100Gbps) range, which would normally be extremely expensive for a strongly-consistent storage engine like Apache Cassandra. With careful tuning of chunk access, we were able to respond to reads at network line rate (100Gbps) from Apache Cassandra, but we observed unacceptable performance and availability degradation on concurrent writes. To resolve this issue, we introduced write-through caching of chunks using our distributed caching system <a href=\"https://github.com/Netflix/EVCache\">EVCache</a>, which is based on Memcached. This allows almost all reads to be served from a highly scalable cache, allowing us to easily hit 200Gbps and beyond without affecting the write path, achieving read-write separation.</p><h4>Final Storage Architecture</h4><p>In the final storage architecture, the Live Origin writes and reads to KeyValue, which manages a write-through cache to EVCache (memcached) and implements a safe chunking protocol that spreads large values and partitions them out across the storage cluster (Apache Cassandra). This allows almost all read load to be handled from cache, with only misses hitting the storage. This combination of cache and highly available storage has met the demanding needs of our Live Origin for over a year¬†now.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*yA9H-BFemM_-99FXBlVOMg.png\" /><figcaption><strong>Storage System High Level Architecture</strong></figcaption></figure><p>Delivering this consistent low latency for large writes with cross-region replication and consistent write-through caching to a distributed cache required solving numerous hard problems with novel techniques, which we plan to share in detail during a future¬†post.</p><h3>Scalability and scalable architecture</h3><p>Netflix‚Äôs live streaming platform must handle a high volume of diverse stream renditions for each live event. This complexity stems from supporting various video encoding formats (each with multiple encoder ladders), numerous audio options (across languages, formats, and bitrates), and different content versions (e.g., with or without advertisements). The combination of these elements, alongside concurrent event support, leads to a significant number of unique stream renditions per live event. This, in turn, necessitates a high Requests Per Second (RPS) capacity from the multi-tenant live origin service to ensure publishing-side scalability.</p><p>In addition, Netflix‚Äôs global reach presents distinct challenges to the live origin on the retrieval side. During the Tyson vs. Paul fight event in 2024, a historic peak of 65 million concurrent streams was observed. Consequently, a scalable architecture for live origin is essential for the success of large-scale live streaming.</p><h4>Scaling architecture</h4><p>We chose to build a highly scalable origin instead of relying on the traditional origin shields approach for better end-to-end cache consistency control and simpler system architecture. The live origin in this architecture directly connects with top-tier Open Connect nodes, which are geographically distributed across several sites. To minimize the load on the origin, only designated nodes per stream rendition at each site are permitted to directly fill from the¬†origin.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*jW7eBCQtjlna0VKaWrKz_A.png\" /><figcaption><strong>Netflix Live Origin Scalability Architecture</strong></figcaption></figure><p>While the origin service can autoscale horizontally using EC2 instances, there are other system resources that are not autoscalable, such as storage platform capacity and AWS to Open Connect backbone bandwidth capacity. Since in live streaming, not all requests to the live origin are of the same importance, the origin is designed to prioritize more critical requests over less critical requests when system resources are limited. The table below outlines the request categories, their identification, and protection methods.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*dYKFJkq22KI8sDBW_Njmog.png\" /></figure><h4>Publishing isolation</h4><p>Publishing traffic, unlike potentially surging CDN retrieval traffic, is predictable, making path isolation a highly effective solution. As shown in the scalability architecture diagram, the origin utilizes separate EC2 publishing and CDN stacks to protect the latency and failure-sensitive origin writes. In addition, the storage abstraction layer features distinct clusters for key-value (KV) read and KV write operations. Finally, the storage layer itself separates read (EVCache) and write (Cassandra) paths. This comprehensive path isolation facilitates independent cloud scaling of publishing and retrieval, and also prevents CDN-facing traffic surges from impacting the performance and reliability of origin publishing.</p><h4>Priority rate¬†limiting</h4><p>Given Netflix‚Äôs scale, managing incoming requests during a traffic storm is challenging, especially considering non-autoscalable system resources. The Netflix Live Origin implemented priority-based rate limiting when the underlying system is under stress. This approach ensures that requests with greater user impact are prioritized to succeed, while requests with lower user impact are allowed to fail during times of stress in order to protect the streaming infrastructure and are permitted to retry later to¬†succeed.</p><p>Leveraging Netflix‚Äôs microservice platform priority rate limiting feature, the origin prioritizes live edge traffic over DVR traffic during periods of high load on the storage platform. The live edge vs. DVR traffic detection is based on the predictable segment template. The template is further cached in memory on the origin node to enable priority rate limiting without access to the datastore, which is valuable especially during periods of high datastore stress.</p><p>To mitigate traffic surges, TTL cache control is used alongside priority rate limiting. When the low-priority traffic is impacted, the origin instructs Open Connect to slow down and cache identical requests for 5 seconds by setting a max-age = 5s and returns an HTTP 503 error code. This strategy effectively dampens traffic surges by preventing repeated requests to the origin within that 5-second¬†window.</p><p>The following diagrams illustrate origin priority rate limiting with simulated traffic. The nliveorigin_mp41 traffic is the low-priority traffic and is mixed with other high-priority traffic. In the first row: the 1st diagram shows the request RPS, the 2nd diagram shows the percentage of request failure. In the second row, the 1st diagram shows datastore resource utilization, and the 2nd diagram shows the origin retrieval P99 latency. The results clearly show that only the low-priority traffic (nliveorigin_mp41) is impacted at datastore high utilization, and the origin request latency is under¬†control.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*-_YP6H3sEDaw1lS8prH4mQ.png\" /><figcaption><strong>Origin Priority Rate¬†Limiting</strong></figcaption></figure><h4>404 storm and cache optimization</h4><p>Publishing isolation and priority rate limiting successfully protect the live origin from DVR traffic storms. However, the traffic storm generated by requests for non-existent segments presents further challenges and opportunities for optimization.</p><p>The live origin structures metadata hierarchically as event &gt; stream rendition &gt; segment, and the segment publishing template is maintained at the stream rendition level. This hierarchical organization allows the origin to preemptively reject requests with an HTTP 404(not found)/410(Gone) error, leveraging highly cacheable event and stream rendition level metadata, avoiding unnecessary queries to the segment level metadata:</p><ul><li>If the event is unknown, reject the request with¬†404</li><li>If the event is known, but the segment request timing does not match the expected publishing timing, reject the request with 404 and cache control TTL matching the expected publishing time</li><li>If the event is known, the requested segment is never generated or misses the retry deadline, reject the request with a 410 error, preventing the client from repeatedly requesting</li></ul><p>At the storage layer, metadata is stored separately from media data in the control plane datastore. Unlike the media datastore, the control plane datastore does not use a distributed cache to avoid cache inconsistency. Event and rendition level metadata benefits from a high cache hit ratio when in-memory caching is utilized at the live origin instance. During traffic storms involving non-existent segments, the cache hit ratio for control plane access easily exceeds¬†90%.</p><p>The use of in-memory caching for metadata effectively handles 404 storms at the live origin without causing datastore stress. This metadata caching complements the storage system‚Äôs distributed media cache, providing a complete solution for traffic surge protection.</p><h3>Summary</h3><p>The Netflix Live Origin, built upon an optimized storage platform, is specifically designed for live streaming. It incorporates advanced media and segment publishing scheduling awareness and leverages enhanced intelligence to improve streaming quality, optimize scalability, and improve Open Connect live streaming operations.</p><h3>Acknowledgement</h3><p>Many teams and stunning colleagues contributed to the Netflix live origin. Special thanks to <a href=\"https://www.linkedin.com/in/flavioribeiro/?originalSubdomain=br\">Flavio Ribeiro</a> for advocacy and sponsorship of the live origin project; to <a href=\"https://www.linkedin.com/in/rummadis/\">Raj Ummadisetty</a>, <a href=\"https://www.linkedin.com/in/prudhviraj9/\">Prudhviraj Karumanchi</a> for the storage platform; to <a href=\"https://www.linkedin.com/in/rosanna-lee-197920/\">Rosanna Lee</a>, <a href=\"https://www.linkedin.com/in/hunterford/\">Hunter Ford</a>, and <a href=\"https://www.linkedin.com/in/thiagopnts/\">Thiago Pontes</a> for storage lifecycle management; to <a href=\"https://www.linkedin.com/in/ameya-vasani-8904304/\">Ameya Vasani</a> for e2e test framework; <a href=\"https://www.linkedin.com/in/thomas-symborski-b4216728/\">Thomas Symborski</a> for orchestrator integration; to <a href=\"https://www.linkedin.com/in/jschek/\">James Schek</a> for Open Connect integration; to <a href=\"https://www.linkedin.com/in/kzwang/\">Kevin Wang</a> for platform priority rate limit; to <a href=\"https://www.linkedin.com/in/di-li-09663968/\">Di Li</a>, <a href=\"mailto:nhubbard@netflix.com\">Nathan Hubbard</a> for origin scalability testing.</p><img alt=\"\" height=\"1\" src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=41f1b0ad5371\" width=\"1\" /><hr /><p><a href=\"https://netflixtechblog.com/netflix-live-origin-41f1b0ad5371\">Netflix Live Origin</a> was originally published in <a href=\"https://netflixtechblog.com\">Netflix TechBlog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>", "source_url": "https://netflixtechblog.com/feed", "article_url": "https://netflixtechblog.com/netflix-live-origin-41f1b0ad5371?source=rss----2615bd06b42e---4", "pub_date": "2025-12-15T17:38:16+00:00", "label": false, "category": "other", "notes": "heuristic label"}
{"id": "adbb2ea8709116ec", "title": "Cisco and OpenAI redefine enterprise engineering with AI agents", "description": "Cisco and OpenAI redefine enterprise engineering with Codex, an AI software agent embedded in workflows to speed builds, automate defect fixes, and enable AI-native development.", "source_url": "https://openai.com/blog/rss.xml", "article_url": "https://openai.com/index/cisco", "pub_date": "2026-01-20T11:00:00+00:00", "label": true, "category": "ai_ml", "notes": "heuristic label"}
{"id": "703c05e9514ba9f1", "title": "Quoting Theia Vogel", "description": "<blockquote cite=\"https://twitter.com/voooooogel/status/2014189072647078053\"><p>[...] i was too busy with work to read anything, so i asked chatgpt to summarize some books on state formation, and it suggested circumscription theory. there was already the natural boundary of my computer hemming the towns in, and town mayors played the role of big men to drive conflict. so i just needed a way for them to fight. i slightly tweaked the allocation of claude max accounts to the towns from a demand-based to a fixed allocation system. towns would each get a fixed amount of tokens to start, but i added a soldier role that could attack and defend in raids to steal tokens from other towns. [...]</p></blockquote>\n<p class=\"cite\">&mdash; <a href=\"https://twitter.com/voooooogel/status/2014189072647078053\">Theia Vogel</a>, Gas Town fan fiction</p>\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/parallel-agents\">parallel-agents</a>, <a href=\"https://simonwillison.net/tags/llms\">llms</a>, <a href=\"https://simonwillison.net/tags/ai\">ai</a>, <a href=\"https://simonwillison.net/tags/generative-ai\">generative-ai</a></p>", "source_url": "https://simonwillison.net/atom/everything/", "article_url": "https://simonwillison.net/2026/Jan/23/theia-vogel/#atom-everything", "pub_date": "2026-01-23T09:13:54+00:00", "label": false, "category": "other", "notes": "heuristic label"}
{"id": "b939f1712631a7a8", "title": "Gemma Scope 2: helping the AI safety community deepen understanding of complex language model behavior", "description": "Open interpretability tools for language models are now available across the entire Gemma 3 family with the release of Gemma Scope 2.", "source_url": "https://deepmind.google/blog/rss.xml", "article_url": "https://deepmind.google/blog/gemma-scope-2-helping-the-ai-safety-community-deepen-understanding-of-complex-language-model-behavior/", "pub_date": "2025-12-16T10:14:24+00:00", "label": true, "category": "ai_ml", "notes": "heuristic label"}
{"id": "75aa8ba4cd30db80", "title": "Engineering more resilient crops for a warming climate", "description": "Scientists are using AlphaFold to strengthen a photosynthesis enzyme for resilient, heat-tolerant crops.", "source_url": "https://deepmind.google/blog/rss.xml", "article_url": "https://deepmind.google/blog/engineering-more-resilient-crops-for-a-warming-climate/", "pub_date": "2025-12-04T16:23:24+00:00", "label": false, "category": "other", "notes": "heuristic label"}
{"id": "25b45dbd0fc949b4", "title": "A business that scales with the value of intelligence", "description": "OpenAI‚Äôs business model scales with intelligence‚Äîspanning subscriptions, API, ads, commerce, and compute‚Äîdriven by deepening ChatGPT adoption.", "source_url": "https://openai.com/blog/rss.xml", "article_url": "https://openai.com/index/a-business-that-scales-with-the-value-of-intelligence", "pub_date": "2026-01-18T10:00:00+00:00", "label": true, "category": "technology", "notes": "heuristic label"}
{"id": "7e0171185e0fc4ce", "title": "Building a Healthcare Robot from Simulation to Deployment with NVIDIA Isaac", "description": "", "source_url": "https://huggingface.co/blog/feed.xml", "article_url": "https://huggingface.co/blog/lerobotxnvidia-healthcare", "pub_date": "2025-10-29T00:00:00+00:00", "label": false, "category": "lifestyle", "notes": "heuristic label"}
{"id": "9e3ba8c01dd7bb59", "title": "Don't fall into the anti-AI hype", "description": "<p><strong><a href=\"https://antirez.com/news/158\">Don&#x27;t fall into the anti-AI hype</a></strong></p>\nI'm glad someone was brave enough to say this. There is a <em>lot</em> of anti-AI sentiment in the software development community these days. Much of it is justified, but if you let people convince you that AI isn't genuinely useful for software developers or that this whole thing will blow over soon it's becoming clear that you're taking on a very real risk to your future career.</p>\n<p>As Salvatore Sanfilippo puts it:</p>\n<blockquote>\n<p>It does not matter if AI companies will not be able to get their money back and the stock market will crash. All that is irrelevant, in the long run. It does not matter if this or the other CEO of some unicorn is telling you something that is off putting, or absurd. Programming changed forever, anyway.</p>\n</blockquote>\n<p>I do like this hopeful positive outlook on what this could all mean, emphasis mine:</p>\n<blockquote>\n<p>How do I feel, about all the code I wrote that was ingested by LLMs? I feel great to be part of that, because I see this as a continuation of what I tried to do all my life: democratizing code, systems, knowledge. <strong>LLMs are going to help us to write better software, faster, and will allow small teams to have a chance to compete with bigger companies</strong>. The same thing open source software did in the 90s.</p>\n</blockquote>\n<p>This post has been the subject of heated discussions all day today on both <a href=\"https://news.ycombinator.com/item?id=46574276\">Hacker News</a> and <a href=\"https://lobste.rs/s/cmsfbu/don_t_fall_into_anti_ai_hype\">Lobste.rs</a>.\n\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/salvatore-sanfilippo\">salvatore-sanfilippo</a>, <a href=\"https://simonwillison.net/tags/ai\">ai</a>, <a href=\"https://simonwillison.net/tags/generative-ai\">generative-ai</a>, <a href=\"https://simonwillison.net/tags/llms\">llms</a>, <a href=\"https://simonwillison.net/tags/ai-assisted-programming\">ai-assisted-programming</a>, <a href=\"https://simonwillison.net/tags/ai-ethics\">ai-ethics</a></p>", "source_url": "https://simonwillison.net/atom/everything/", "article_url": "https://simonwillison.net/2026/Jan/11/dont-fall-into-the-anti-ai-hype/#atom-everything", "pub_date": "2026-01-11T23:58:43+00:00", "label": true, "category": "ai_ml", "notes": "heuristic label"}
{"id": "5fcfc6652ceac921", "title": "Wilson Lin on FastRender: a browser built by thousands of parallel agents", "description": "<p>Last week Cursor published <a href=\"https://cursor.com/blog/scaling-agents\">Scaling long-running autonomous coding</a>, an article describing their research efforts into coordinating large numbers of autonomous coding agents. One of the projects mentioned in the article was <a href=\"https://github.com/wilsonzlin/fastrender\">FastRender</a>, a web browser they built from scratch using their agent swarms. I wanted to learn more so I asked Wilson Lin, the engineer behind FastRender, if we could record a conversation about the project. That 47 minute video is <a href=\"https://www.youtube.com/watch?v=bKrAcTf2pL4\">now available on YouTube</a>. I've included some of the highlights below.</p>\n\n \n\n<p>See my <a href=\"https://simonwillison.net/2026/Jan/19/scaling-long-running-autonomous-coding/\">previous post</a> for my notes and screenshots from trying out FastRender myself.</p>\n\n\n<h4 id=\"what-fastrender-can-do-right-now\">What FastRender can do right now</h4>\n<p>We started the conversation with a demo of FastRender loading different pages (<a href=\"https://www.youtube.com/watch?v=bKrAcTf2pL4&amp;t=195s\">03:15</a>). The JavaScript engine isn't working yet so we instead loaded <a href=\"https://github.com/wilsonzlin/fastrender\">github.com/wilsonzlin/fastrender</a>, <a href=\"https://en.wikipedia.org/\">Wikipedia</a> and <a href=\"https://cnn.com\">CNN</a> - all of which were usable, if a little slow to display.</p>\n<p>JavaScript had been disabled by one of the agents, which decided to add a feature flag! <a href=\"https://www.youtube.com/watch?v=bKrAcTf2pL4&amp;t=242s\">04:02</a></p>\n<blockquote>\n<p>JavaScript is disabled right now. The agents made a decision as they were currently still implementing the engine and making progress towards other parts... they decided to turn it off or put it behind a feature flag, technically.</p>\n</blockquote>\n<h4 id=\"from-side-project-to-core-research\">From side-project to core research</h4>\n<p>Wilson started what become FastRender as a personal side-project to explore the capabilities of the latest generation of frontier models - Claude Opus 4.5, GPT-5.1, and GPT-5.2. <a href=\"https://www.youtube.com/watch?v=bKrAcTf2pL4&amp;t=56s\">00:56</a></p>\n<blockquote>\n<p>FastRender was a personal project of mine from, I'd say, November. It was an experiment to see how well frontier models like Opus 4.5 and back then GPT-5.1 could do with much more complex, difficult tasks.</p>\n</blockquote>\n<p>A browser rendering engine was the ideal choice for this, because it's both <em>extremely</em> ambitious and complex but also well specified. And you can visually see how well it's working! <a href=\"https://www.youtube.com/watch?v=bKrAcTf2pL4&amp;t=117s\">01:57</a></p>\n<blockquote>\n<p>As that experiment progressed, I was seeing better and better results from single agents that were able to actually make good progress on this project. And at that point, I wanted to see, well, what's the next level? How do I push this even further?</p>\n</blockquote>\n<p>Once it became clear that this was an opportunity to try multiple agents working together it graduated to an official Cursor research project, and available resources were amplified.</p>\n<p>The goal of FastRender was never to build a browser to compete with the likes of Chrome. <a href=\"https://www.youtube.com/watch?v=bKrAcTf2pL4&amp;t=2512s\">41:52</a></p>\n<blockquote>\n<p>We never intended for it to be a production software or usable, but we wanted to observe behaviors of this harness of multiple agents, to see how they could work at scale.</p>\n</blockquote>\n<p>The great thing about a browser is that it has such a large scope that it can keep serving experiments in this space for many years to come. JavaScript, then WebAssembly, then WebGPU... it could take many years to run out of new challenges for the agents to tackle.</p>\n<h4 id=\"running-thousands-of-agents-at-once\">Running thousands of agents at once</h4>\n<p>The most interesting thing about FastRender is the way the project used multiple agents working in parallel to build different parts of the browser. I asked how many agents were running at once: <a href=\"https://www.youtube.com/watch?v=bKrAcTf2pL4&amp;t=324s\">05:24</a></p>\n<blockquote>\n<p>At the peak, when we had the stable system running for one week continuously, there were approximately 2,000 agents running concurrently at one time. And they were making, I believe, thousands of commits per hour.</p>\n</blockquote>\n<p>The project has <a href=\"https://github.com/wilsonzlin/fastrender/commits/main/\">nearly 30,000 commits</a>!</p>\n<p>How do you run 2,000 agents at once? They used <em>really big machines</em>. <a href=\"https://www.youtube.com/watch?v=bKrAcTf2pL4&amp;t=356s\">05:56</a></p>\n<blockquote>\n<p>The simple approach we took with the infrastructure was to have a large machine run one of these multi-agent harnesses. Each machine had ample resources, and it would run about 300 agents concurrently on each. This was able to scale and run reasonably well, as agents spend a lot of time thinking, and not just running tools.</p>\n</blockquote>\n<p>At this point we switched to a live demo of the harness running on one of those big machines (<a href=\"https://www.youtube.com/watch?v=bKrAcTf2pL4&amp;t=392s\">06:32</a>). The agents are arranged in a tree structure, with planning agents firing up tasks and worker agents then carrying them out. <a href=\"https://www.youtube.com/watch?v=bKrAcTf2pL4&amp;t=434s\">07:14</a></p>\n<p><img alt=\"Terminal window showing a tmux session running &quot;grind-swarm&quot; task manager with RUNNING status. Header shows &quot;grind-swarm ‚Äì 45:54:15&quot; with stats &quot;planners: 9 (0 done) | tasks: 111 working, 0 pending, 232 done | 12900.9M‚Üë 514.1M‚Üì&quot;. Task list includes: p1 Root (main), p2 CSS selector matching performance + bloom filter integration, p3 CSS stylesheet parsing semantics &amp; at-rule handling, p4 Custom properties (@property) + var() resolution + incremental recompute/invalidation, p37 CSS at-rule artifact integration, p50 Selector engine correctness &amp; spec coverage, p51 Computed-value + property coverage across css-cascade, p105 Style sharing / computed style caching in fastrender-style, p289 CSS cascade layers (@layer) global ordering, w5 Fix workspace lockfile drift, w7 Implement computed-style snapshot sharing, w15 Fix css-properties namespace handling, w17 (Stretch) Enable bloom fast-reject in HTML quirks mode, w18 Refactor css-properties stylesheet parsing. Activity log shows shell commands including cargo check, git status, git push origin main, and various test runs. Bottom status bar shows &quot;grind-css0:target/release/grind-swarm*&quot; and &quot;streamyard.com is sharing your screen&quot; notification with timestamp &quot;12:02 22-Jan-26&quot;.\" src=\"https://static.simonwillison.net/static/2026/wilson-lin-agents.jpg\" /></p>\n<blockquote>\n<p>This cluster of agents is working towards building out the CSS aspects of the browser, whether that's parsing, selector engine, those features. We managed to push this even further by splitting out the browser project into multiple instructions or work streams and have each one run one of these harnesses on their own machine, so that was able to further parallelize and increase throughput.</p>\n</blockquote>\n<p>But don't all of these agents working on the same codebase result in a huge amount of merge conflicts? Apparently not: <a href=\"https://www.youtube.com/watch?v=bKrAcTf2pL4&amp;t=501s\">08:21</a></p>\n<blockquote>\n<p>We've noticed that most commits do not have merge conflicts. The reason is the harness itself is able to quite effectively split out and divide the scope and tasks such that it tries to minimize the amount of overlap of work. That's also reflected in the code structure‚Äîcommits will be made at various times and they don't tend to touch each other at the same time.</p>\n</blockquote>\n<p>This appears to be the key trick for unlocking benefits from parallel agents: if planning agents do a good enough job of breaking up the work into non-overlapping chunks you can bring hundreds or even thousands of agents to bear on a problem at once.</p>\n<p>Surprisingly, Wilson found that GPT-5.1 and GPT-5.2 were a better fit for this work than the coding specialist GPT-5.1-Codex: <a href=\"https://www.youtube.com/watch?v=bKrAcTf2pL4&amp;t=1048s\">17:28</a></p>\n<blockquote>\n<p>Some initial findings were that the instructions here were more expansive than merely coding. For example, how to operate and interact within a harness, or how to operate autonomously without interacting with the user or having a lot of user feedback. These kinds of instructions we found worked better with the general models.</p>\n</blockquote>\n<p>I asked what the longest they've seen this system run without human intervention: <a href=\"https://www.youtube.com/watch?v=bKrAcTf2pL4&amp;t=1108s\">18:28</a></p>\n<blockquote>\n<p>So this system, once you give an instruction, there's actually no way to steer it, you can't prompt it, you're going to adjust how it goes. The only thing you can do is stop it. So our longest run, all the runs are basically autonomous. We don't alter the trajectory while executing. [...]</p>\n<p>And so the longest at the time of the post was about a week and that's pretty close to the longest. Of course the research project itself was only about three weeks so you know we probably can go longer.</p>\n</blockquote>\n<h4 id=\"specifications-and-feedback-loops\">Specifications and feedback loops</h4>\n<p>An interesting aspect of this project design is feedback loops. For agents to work autonomously for long periods of time they need as much useful context about the problem they are solving as possible, combined with effective feedback loops to help them make decisions.</p>\n<p>The FastRender repo <a href=\"https://github.com/wilsonzlin/fastrender/tree/19bf1036105d4eeb8bf3330678b7cb11c1490bdc/specs\">uses git submodules to include relevant specifications</a>, including csswg-drafts, tc39-ecma262 for JavaScript, whatwg-dom, whatwg-html and more. <a href=\"https://www.youtube.com/watch?v=bKrAcTf2pL4&amp;t=846s\">14:06</a></p>\n<blockquote>\n<p>Feedback loops to the system are very important. Agents are working for very long periods continuously, and without guardrails and feedback to know whether what they're doing is right or wrong it can have a big impact over a long rollout. Specs are definitely an important part‚Äîyou can see lots of comments in the code base that AI wrote referring specifically to specs that they found in the specs submodules.</p>\n</blockquote>\n<p>GPT-5.2 is a vision-capable model, and part of the feedback loop for FastRender included taking screenshots of the rendering results and feeding those back into the model:\n<a href=\"https://www.youtube.com/watch?v=bKrAcTf2pL4&amp;t=983s\">16:23</a></p>\n<blockquote>\n<p>In the earlier evolution of this project, when it was just doing the static renderings of screenshots, this was definitely a very explicit thing we taught it to do. And these models are visual models, so they do have that ability. We have progress indicators to tell it to compare the diff against a golden sample.</p>\n</blockquote>\n<p>The strictness of the Rust compiler helped provide a feedback loop as well: <a href=\"https://www.youtube.com/watch?v=bKrAcTf2pL4&amp;t=952s\">15:52</a></p>\n<blockquote>\n<p>The nice thing about Rust is you can get a lot of verification just from compilation, and that is not as available in other languages.</p>\n</blockquote>\n<h4 id=\"the-agents-chose-the-dependencies\">The agents chose the dependencies</h4>\n<p>We talked about the <a href=\"https://github.com/wilsonzlin/fastrender/blob/19bf1036105d4eeb8bf3330678b7cb11c1490bdc/Cargo.toml\">Cargo.toml dependencies</a> that the project had accumulated, almost all of which had been selected by the agents themselves.</p>\n<p>Some of these, like <a href=\"https://skia.org/\">Skia</a> for 2D graphics rendering or <a href=\"https://github.com/harfbuzz/harfbuzz\">HarfBuzz</a> for text shaping, were obvious choices. Others such as <a href=\"https://github.com/DioxusLabs/taffy\">Taffy</a> felt like they might go against the from-scratch goals of the project, since that library implements CSS flexbox and grid layout algorithms directly. This was not an intended outcome. <a href=\"https://www.youtube.com/watch?v=bKrAcTf2pL4&amp;t=1673s\">27:53</a></p>\n<blockquote>\n<p>Similarly these are dependencies that the agent picked to use for small parts of the engine and perhaps should have actually implemented itself. I think this reflects on the importance of the instructions, because I actually never encoded specifically the level of dependencies we should be implementing ourselves.</p>\n</blockquote>\n<p>The agents vendored in Taffy and <a href=\"https://github.com/wilsonzlin/fastrender/commits/main/vendor/taffy\">applied a stream of changes</a> to that vendored copy.\n<a href=\"https://www.youtube.com/watch?v=bKrAcTf2pL4&amp;t=1878s\">31:18</a></p>\n<blockquote>\n<p>It's currently vendored. And as the agents work on it, they do make changes to it. This was actually an artifact from the very early days of the project before it was a fully fledged browser... it's implementing things like the flex and grid layers, but there are other layout methods like inline, block, and table, and in our new experiment, we're removing that completely.</p>\n</blockquote>\n<p>The inclusion of QuickJS despite the presence of a home-grown ecma-rs implementation has a fun origin story:\n<a href=\"https://www.youtube.com/watch?v=bKrAcTf2pL4&amp;t=2115s\">35:15</a></p>\n<blockquote>\n<p>I believe it mentioned that it pulled in the QuickJS because it knew that other agents were working on the JavaScript engine, and it needed to unblock itself quickly. [...]</p>\n<p>It was like, eventually, once that's finished, let's remove it and replace with the proper engine.</p>\n</blockquote>\n<p>I love how similar this is to the dynamics of a large-scale human engineering team, where you could absolutely see one engineer getting frustrated at another team not having delivered yet and unblocking themselves by pulling in a third-party library.</p>\n<h4 id=\"intermittent-errors-are-ok-actually\">Intermittent errors are OK, actually</h4>\n<p>Here's something I found really surprising: the agents were allowed to introduce small errors into the codebase as they worked! <a href=\"https://www.youtube.com/watch?v=bKrAcTf2pL4&amp;t=2382s\">39:42</a></p>\n<blockquote>\n<p>One of the trade-offs was: if you wanted every single commit to be a hundred percent perfect, make sure it can always compile every time, that might be a synchronization bottleneck. [...]</p>\n<p>Especially as you break up the system into more modularized aspects, you can see that errors get introduced, but small errors, right? An API change or some syntax error, but then they get fixed really quickly after a few commits. So there's a little bit of slack in the system to allow these temporary errors so that the overall system can continue to make progress at a really high throughput. [...]</p>\n<p>People may say, well, that's not correct code. But it's not that the errors are accumulating. It's a stable rate of errors. [...] That seems like a worthwhile trade-off.</p>\n</blockquote>\n<p>If you're going to have thousands of agents working in parallel optimizing for throughput over correctness turns out to be a strategy worth exploring.</p>\n<h4 id=\"a-single-engineer-plus-a-swarm-of-agents-in-january-2026\">A single engineer plus a swarm of agents in January 2026</h4>\n<p>The thing I find most interesting about FastRender is how it demonstrates the extreme edge of what a single engineer can achieve in early 2026 with the assistance of a swarm of agents.</p>\n<p>FastRender may not be a production-ready browser, but it represents over a million lines of Rust code, written in a few weeks, that can already render real web pages to a usable degree.</p>\n<p>A browser really is the ideal research project to experiment with this new, weirdly shaped form of software engineering.</p>\n<p>I asked Wilson how much mental effort he had invested in browser rendering compared to agent co-ordination. <a href=\"https://www.youtube.com/watch?v=bKrAcTf2pL4&amp;t=694s\">11:34</a></p>\n<blockquote>\n<p>The browser and this project were co-developed and very symbiotic, only because the browser was a very useful objective for us to measure and iterate the progress of the harness. The goal was to iterate on and research the multi-agent harness‚Äîthe browser was just the research example or objective.</p>\n</blockquote>\n<p>FastRender is effectively using a full browser rendering engine as a \"hello world\" exercise for multi-agent coordination!</p>\n    \n        <p>Tags: <a href=\"https://simonwillison.net/tags/browsers\">browsers</a>, <a href=\"https://simonwillison.net/tags/youtube\">youtube</a>, <a href=\"https://simonwillison.net/tags/ai\">ai</a>, <a href=\"https://simonwillison.net/tags/generative-ai\">generative-ai</a>, <a href=\"https://simonwillison.net/tags/llms\">llms</a>, <a href=\"https://simonwillison.net/tags/ai-assisted-programming\">ai-assisted-programming</a>, <a href=\"https://simonwillison.net/tags/coding-agents\">coding-agents</a>, <a href=\"https://simonwillison.net/tags/cursor\">cursor</a>, <a href=\"https://simonwillison.net/tags/parallel-agents\">parallel-agents</a></p>", "source_url": "https://simonwillison.net/atom/everything/", "article_url": "https://simonwillison.net/2026/Jan/23/fastrender/#atom-everything", "pub_date": "2026-01-23T21:26:10+00:00", "label": false, "category": "other", "notes": "manual: Small project highlight; not a major release."}
{"id": "31ca6a56aa2c4f49", "title": "Stargate Community", "description": "Stargate Community plans detail a community-first approach to AI infrastructure, using locally tailored plans shaped by community input, energy needs, and workforce priorities.", "source_url": "https://openai.com/blog/rss.xml", "article_url": "https://openai.com/index/stargate-community", "pub_date": "2026-01-20T19:00:00+00:00", "label": false, "category": "other", "notes": "heuristic label"}
{"id": "bcc7322f22f1c392", "title": "Stripe Atlas startups in 2025: Year in review", "description": "2025 was a breakout year for early-stage startups, as founders launched more companies and generated revenue faster than ever. Three shifts stand out: customer bases are more international than ever, time-to-revenue has compressed, and founders are turning their attention to AI agents over AI infrastructure or copilots.", "source_url": "https://stripe.com/blog/feed.rss", "article_url": "https://stripe.com/blog/stripe-atlas-startups-in-2025-year-in-review", "pub_date": "2025-12-18T00:00:00+00:00", "label": false, "category": "other", "notes": "heuristic label"}
{"id": "d48848e55e160952", "title": "The Pulse #156: Beware of global configuration changes in large systems", "description": "Also: inadequate capacity planning at AWS, how the engineering team at Oxide uses LLMs, Linux now supports Rust in the kernel, and more", "source_url": "https://newsletter.pragmaticengineer.com/feed", "article_url": "https://newsletter.pragmaticengineer.com/p/the-pulse-156", "pub_date": "2025-12-11T17:33:00+00:00", "label": false, "category": "other", "notes": "heuristic label"}
{"id": "82397adc16b43c44", "title": "How Higgsfield turns simple ideas into cinematic social videos", "description": "Discover how Higgsfield gives creators cinematic, social-first video output from simple inputs using OpenAI GPT-4.1, GPT-5, and Sora 2.", "source_url": "https://openai.com/blog/rss.xml", "article_url": "https://openai.com/index/higgsfield", "pub_date": "2026-01-21T10:00:00+00:00", "label": false, "category": "other", "notes": "heuristic label"}
{"id": "28d4cca54f975a62", "title": "Google DeepMind supports U.S. Department of Energy on Genesis: a national mission to accelerate innovation and scientific discovery", "description": "Google DeepMind and the DOE partner on Genesis, a new effort to accelerate science with AI.", "source_url": "https://deepmind.google/blog/rss.xml", "article_url": "https://deepmind.google/blog/google-deepmind-supports-us-department-of-energy-on-genesis/", "pub_date": "2025-11-24T14:12:03+00:00", "label": true, "category": "ai_ml", "notes": "heuristic label"}
{"id": "2f3fd8e3926dd0b9", "title": "Code security for software engineers", "description": "Johannes Dahse, VP of Code Security at Sonar, breaks down where vulnerabilities really come from, how developers can prevent them, and how AI is reshaping the security landscape.", "source_url": "https://newsletter.pragmaticengineer.com/feed", "article_url": "https://newsletter.pragmaticengineer.com/p/code-security", "pub_date": "2025-11-26T16:45:55+00:00", "label": true, "category": "security", "notes": "heuristic label"}
{"id": "f5875fbc0e2d2233", "title": "WeatherNext 2: Our most advanced weather forecasting model", "description": "The new AI model delivers more efficient, more accurate and higher-resolution global weather predictions.", "source_url": "https://deepmind.google/blog/rss.xml", "article_url": "https://deepmind.google/blog/weathernext-2-our-most-advanced-weather-forecasting-model/", "pub_date": "2025-11-17T15:09:23+00:00", "label": true, "category": "ai_ml", "notes": "heuristic label"}
{"id": "e553391e9dff8f8d", "title": "FLUX.2-klein-4B Pure C Implementation", "description": "<p><strong><a href=\"https://github.com/antirez/flux2.c\">FLUX.2-klein-4B Pure C Implementation</a></strong></p>\nOn 15th January Black Forest Labs, a lab formed by the creators of the original Stable Diffusion, released <a href=\"https://huggingface.co/black-forest-labs/FLUX.2-klein-4B\">black-forest-labs/FLUX.2-klein-4B</a> - an Apache 2.0 licensed 4 billion parameter version of their FLUX.2 family.</p>\n<p>Salvatore Sanfilippo (antirez) decided to build a pure C and dependency-free implementation to run the model, with assistance from Claude Code and Claude Opus 4.5.</p>\n<p>Salvatore shared <a href=\"https://news.ycombinator.com/item?id=46670279#46671233\">this note</a> on Hacker News:</p>\n<blockquote>\n<p>Something that may be interesting for the reader of this thread: this project was possible only once I started to tell Opus that it <em>needed</em> to take a file with all the implementation notes, and also accumulating all the things we discovered during the development process. And also, the file had clear instructions to be taken updated, and to be processed ASAP after context compaction. This kinda enabled Opus to do such a big coding task in a reasonable amount of time without loosing track. Check the file IMPLEMENTATION_NOTES.md in the GitHub repo for more info.</p>\n</blockquote>\n<p>Here's that <a href=\"https://github.com/antirez/flux2.c/blob/main/IMPLEMENTATION_NOTES.md\">IMPLEMENTATION_NOTES.md</a> file.\n\n    <p><small></small>Via <a href=\"https://news.ycombinator.com/item?id=46670279\">Hacker News</a></small></p>\n\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/c\">c</a>, <a href=\"https://simonwillison.net/tags/salvatore-sanfilippo\">salvatore-sanfilippo</a>, <a href=\"https://simonwillison.net/tags/ai\">ai</a>, <a href=\"https://simonwillison.net/tags/stable-diffusion\">stable-diffusion</a>, <a href=\"https://simonwillison.net/tags/generative-ai\">generative-ai</a>, <a href=\"https://simonwillison.net/tags/llms\">llms</a>, <a href=\"https://simonwillison.net/tags/ai-assisted-programming\">ai-assisted-programming</a>, <a href=\"https://simonwillison.net/tags/text-to-image\">text-to-image</a>, <a href=\"https://simonwillison.net/tags/coding-agents\">coding-agents</a>, <a href=\"https://simonwillison.net/tags/claude-code\">claude-code</a></p>", "source_url": "https://simonwillison.net/atom/everything/", "article_url": "https://simonwillison.net/2026/Jan/18/flux2-klein-4b/#atom-everything", "pub_date": "2026-01-18T23:58:58+00:00", "label": false, "category": "other", "notes": "manual: Niche implementation release."}
{"id": "6d6c549b7c0b8421", "title": "The Design & Implementation of Sprites", "description": "<p><strong><a href=\"https://fly.io/blog/design-and-implementation/\">The Design &amp; Implementation of Sprites</a></strong></p>\nI <a href=\"https://simonwillison.net/2026/Jan/9/sprites-dev/\">wrote about Sprites last week</a>. Here's Thomas Ptacek from Fly with the insider details on how they work under the hood.</p>\n<p>I like this framing of them as \"disposable computers\":</p>\n<blockquote>\n<p>Sprites are ball-point disposable computers. Whatever mark you mean to make, we‚Äôve rigged it so you‚Äôre never more than a second or two away from having a Sprite to do it with.</p>\n</blockquote>\n<p>I've noticed that new Fly Machines can take a while (up to around a minute) to provision. Sprites solve that by keeping warm pools of unused machines in multiple regions, which is enabled by them all using the same container:</p>\n<blockquote>\n<p>Now, today, under the hood, Sprites are still Fly Machines. But they all run from a standard container. Every physical worker knows exactly what container the next Sprite is going to start with, so it‚Äôs easy for us to keep pools of ‚Äúempty‚Äù Sprites standing by. The result: a Sprite create doesn‚Äôt have any heavy lifting to do; it‚Äôs basically just doing the stuff we do when we start a Fly Machine.</p>\n</blockquote>\n<p>The most interesting detail is how the persistence layer works. Sprites only charge you for data you have written that differs from the base image and provide ~300ms checkpointing and restores - it turns out that's power by a custom filesystem on top of S3-compatible storage coordinated by Litestream-replicated local SQLite metadata:</p>\n<blockquote>\n<p>We still exploit NVMe, but not as the root of storage. Instead, it‚Äôs a read-through cache for a blob on object storage. S3-compatible object stores are the most trustworthy storage technology we have. I can feel my blood pressure dropping just typing the words ‚ÄúSprites are backed by object storage.‚Äù [...]</p>\n<p>The Sprite storage stack is organized around the JuiceFS model (in fact, we currently use a very hacked-up JuiceFS, with a rewritten SQLite metadata backend). It works by splitting storage into data (‚Äúchunks‚Äù) and metadata (a map of where the ‚Äúchunks‚Äù are). Data chunks live on object stores; metadata lives in fast local storage. In our case, that metadata store is <a href=\"https://litestream.io\">kept durable with Litestream</a>. Nothing depends on local storage.</p>\n</blockquote>\n\n    <p><small></small>Via <a href=\"https://twitter.com/tqbf/status/2011823480673624434\">@tqbf</a></small></p>\n\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/architecture\">architecture</a>, <a href=\"https://simonwillison.net/tags/sandboxing\">sandboxing</a>, <a href=\"https://simonwillison.net/tags/sqlite\">sqlite</a>, <a href=\"https://simonwillison.net/tags/thomas-ptacek\">thomas-ptacek</a>, <a href=\"https://simonwillison.net/tags/fly\">fly</a>, <a href=\"https://simonwillison.net/tags/litestream\">litestream</a></p>", "source_url": "https://simonwillison.net/atom/everything/", "article_url": "https://simonwillison.net/2026/Jan/15/the-design-implementation-of-sprites/#atom-everything", "pub_date": "2026-01-15T16:08:27+00:00", "label": false, "category": "other", "notes": "heuristic label"}
{"id": "769d3228fde8ca05", "title": "Introducing OpenAI Academy for News Organizations", "description": "OpenAI is launching the OpenAI Academy for News Organizations, a new learning hub built with the American Journalism Project and The Lenfest Institute to help newsrooms use AI effectively. The Academy offers training, practical use cases, and responsible-use guidance to support journalists, editors, and publishers as they adopt AI in their reporting and operations.", "source_url": "https://openai.com/blog/rss.xml", "article_url": "https://openai.com/index/openai-academy-for-news-organizations", "pub_date": "2025-12-17T06:00:00+00:00", "label": true, "category": "ai_ml", "notes": "heuristic label"}
{"id": "a2ee0c75e8652d9c", "title": "Introducing GPT-5.2-Codex", "description": "GPT-5.2-Codex is OpenAI‚Äôs most advanced coding model, offering long-horizon reasoning, large-scale code transformations, and enhanced cybersecurity capabilities.", "source_url": "https://openai.com/blog/rss.xml", "article_url": "https://openai.com/index/introducing-gpt-5-2-codex", "pub_date": "2025-12-18T00:00:00+00:00", "label": true, "category": "ai_ml", "notes": "heuristic label"}
{"id": "a8aaf131dfec21ab", "title": "Developers can now submit apps to ChatGPT", "description": "Developers can now submit apps for review and publication in ChatGPT, with approved apps appearing in a new in-product directory for easy discovery. Updated tools, guidelines, and the Apps SDK help developers build powerful chat-native experiences that bring real-world actions into ChatGPT.", "source_url": "https://openai.com/blog/rss.xml", "article_url": "https://openai.com/index/developers-can-now-submit-apps-to-chatgpt", "pub_date": "2025-12-17T00:00:00+00:00", "label": true, "category": "ai_ml", "notes": "heuristic label"}
{"id": "8d855ce94ad0d710", "title": "Expanding data residency access to business customers worldwide", "description": "OpenAI expands data residency for ChatGPT Enterprise, ChatGPT Edu, and the API Platform, enabling eligible customers to store data at rest in-region.", "source_url": "https://openai.com/blog/rss.xml", "article_url": "https://openai.com/index/expanding-data-residency-access-to-business-customers-worldwide", "pub_date": "2025-11-25T22:00:00+00:00", "label": false, "category": "other", "notes": "heuristic label"}
{"id": "65b5e94aef8b4945", "title": "Fragments Dec 11", "description": "<p><a href=\"https://www.nytimes.com/2025/12/03/magazine/chatbot-writing-style.html?utm_source=substack&amp;utm_medium=email\">Why does AI write like‚Ä¶ that</a> (NYT, gift link). Sam Kriss delves into the quiet hum of AI writing. AI‚Äôs work is not compelling prose: it‚Äôs phantom text, ghostly scribblings, a spectre woven into our communal tapestry.</p>\n\n<p>¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>\n\n<p><a href=\"https://coding-is-like-cooking.info/2025/12/test-desiderata-2-0/\">Emily Bache</a> has written a set of Test Desiderata, building on some earlier writing from Kent Beck. She lists the characteristics of good tests, and how they support her four ‚Äúmacro desiderata‚Äù - the properties of a sound test suite</p>\n\n<blockquote>\n  <ul>\n    <li>Predict success in production</li>\n    <li>Fast to get feedback</li>\n    <li>Support ongoing code design change</li>\n    <li>Low total cost of ownership</li>\n  </ul>\n</blockquote>\n\n<p>She also has a great list of other writers‚Äô lists of good test characteristics.</p>\n\n<p>¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>\n\n<p><a href=\"https://www.techpolicy.press/the-eus-fine-against-x-is-not-about-speech-or-censorship/\">Daphne Keller</a> explains that the EUs fines on X aren‚Äôt about free speech.</p>\n\n<blockquote>\n  <p>There are three charges against X, which all stem from a multi-year investigation that was launched in 2023. One is about verification ‚Äî X‚Äôs blue checkmarks on user accounts ‚Äî and two are about transparency. These charges have nothing to do with what content is on X, or what user speech the platform should or should not allow.</p>\n</blockquote>\n\n<p>¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>\n\n<p><a href=\"https://pluralistic.net/2025/12/05/pop-that-bubble/#u-washington\">Cory Doctorow</a> The Reverse-Centaur‚Äôs Guide to Criticizing AI</p>\n\n<blockquote>\n  <p>Start with what a reverse centaur is. In automation theory, a ‚Äúcentaur‚Äù is a person who is assisted by a machine. ‚Ä¶ And obviously, a reverse centaur is machine head on a human body, a person who is serving as a squishy meat appendage for an uncaring machine.</p>\n\n  <p>Like an Amazon delivery driver‚Ä¶ the van can‚Äôt drive itself and can‚Äôt get a parcel from the curb to your porch. The driver is a peripheral for a van, and the van drives the driver, at superhuman speed, demanding superhuman endurance.</p>\n</blockquote>", "source_url": "https://martinfowler.com/feed.atom", "article_url": "https://martinfowler.com/articles/2025-12-11-frags.html", "pub_date": "2025-12-11T15:17:00+00:00", "label": false, "category": "other", "notes": "heuristic label"}
{"id": "b88de7639f26c266", "title": "OpenAI for Healthcare", "description": "OpenAI for Healthcare enables secure, enterprise-grade AI that supports HIPAA compliance‚Äîreducing administrative burden and supporting clinical workflows.", "source_url": "https://openai.com/blog/rss.xml", "article_url": "https://openai.com/index/openai-for-healthcare", "pub_date": "2026-01-08T12:00:00+00:00", "label": true, "category": "ai_ml", "notes": "heuristic label"}
{"id": "a68e8a2e230accc3", "title": "Python Workers redux: fast cold starts, packages, and a uv-first workflow", "description": "Recent advancements in Cloudflare Python Workers means fast cold starts, comprehensive package support, and a great developer experience. We explain how they were achieved and show how Python can be used to build serverless applications on Cloudflare.", "source_url": "https://blog.cloudflare.com/rss/", "article_url": "https://blog.cloudflare.com/python-workers-advancements/", "pub_date": "2025-12-08T06:00:00+00:00", "label": false, "category": "other", "notes": "heuristic label"}
{"id": "2132c807986f90c8", "title": "Instacart and OpenAI partner on AI shopping experiences", "description": "OpenAI and Instacart are deepening their longstanding partnership by bringing the first fully integrated grocery shopping and Instant Checkout payment app to ChatGPT.", "source_url": "https://openai.com/blog/rss.xml", "article_url": "https://openai.com/index/instacart-partnership", "pub_date": "2025-12-08T06:00:00+00:00", "label": true, "category": "ai_ml", "notes": "heuristic label"}
{"id": "3398ec2692ec54af", "title": "The Product-Minded Engineer: The importance of good errors and warnings", "description": "Product engineers are more in demand than ever, but how do you become one? New book, &#8220;The Product-Minded Engineer&#8221;, offers a guide. An interview with its author and an exclusive excerpt", "source_url": "https://newsletter.pragmaticengineer.com/feed", "article_url": "https://newsletter.pragmaticengineer.com/p/the-product-minded-engineer", "pub_date": "2026-01-20T17:31:16+00:00", "label": false, "category": "other", "notes": "manual: Soft skills/productivity article."}
{"id": "3a2fe575f483dd5d", "title": "Our approach to advertising and expanding access to ChatGPT", "description": "<p><strong><a href=\"https://openai.com/index/our-approach-to-advertising-and-expanding-access/\">Our approach to advertising and expanding access to ChatGPT</a></strong></p>\nOpenAI's long-rumored introduction of ads to ChatGPT just became a whole lot more concrete:</p>\n<blockquote>\n<p>In the coming weeks, we‚Äôre also planning to start testing ads in the U.S. for the free and Go tiers, so more people can benefit from our tools with fewer usage limits or without having to pay. Plus, Pro, Business, and Enterprise subscriptions will not include ads.</p>\n</blockquote>\n<p>What's \"Go\" tier, you might ask? That's a new $8/month tier that launched today in the USA, see <a href=\"https://openai.com/index/introducing-chatgpt-go/\">Introducing ChatGPT Go, now available worldwide</a>. It's a tier that they first trialed in India in August 2025 (here's a mention <a href=\"https://help.openai.com/en/articles/6825453-chatgpt-release-notes#h_22cae6eb9f\">in their release notes from August</a> listing a price of ‚Çπ399/month, which converts to around $4.40).</p>\n<p>I'm finding the new plan comparison grid on <a href=\"https://chatgpt.com/pricing\">chatgpt.com/pricing</a> pretty confusing. It lists all accounts as having access to GPT-5.2 Thinking, but doesn't clarify the limits that the free and Go plans have to conform to. It also lists different context windows for the different plans - 16K for free, 32K for Go and Plus and 128K for Pro. I had assumed that the 400,000 token window <a href=\"https://platform.openai.com/docs/models/gpt-5.2\">on the GPT-5.2 model page</a> applied to ChatGPT as well, but apparently I was mistaken.</p>\n<p><strong>Update</strong>: I've apparently not been paying attention: here's the Internet Archive ChatGPT pricing page from <a href=\"https://web.archive.org/web/20250906071408/https://chatgpt.com/pricing\">September 2025</a> showing those context limit differences as well.</p>\n<p>Back to advertising: my biggest concern has always been whether ads will influence the output of the chat directly. OpenAI assure us that they will not:</p>\n<blockquote>\n<ul>\n<li><strong>Answer independence</strong>: Ads do not influence the answers ChatGPT gives you. Answers are optimized based on what's most helpful to you. Ads are always separate and clearly labeled.</li>\n<li><strong>Conversation privacy</strong>: We keep your conversations with ChatGPT private from advertisers, and we never sell your data to advertisers.</li>\n</ul>\n</blockquote>\n<p>So what will they look like then? This screenshot from the announcement offers a useful hint:</p>\n<p><img alt=\"Two iPhone screenshots showing ChatGPT mobile app interface. Left screen displays a conversation about Santa Fe, New Mexico with an image of adobe-style buildings and desert landscape, text reading &quot;Santa Fe, New Mexico‚Äîoften called 'The City Different'‚Äîis a captivating blend of history, art, and natural beauty at the foot of the Sangre de Cristo Mountains. As the oldest and highest-elevation state capital in the U.S., founded in 1610, it offers a unique mix of Native American, Spanish, and Anglo cultures.&quot; Below is a sponsored section from &quot;Pueblo &amp; Pine&quot; showing &quot;Desert Cottages - Expansive residences with desert vistas&quot; with a thumbnail image, and a &quot;Chat with Pueblo &amp; Pine&quot; button. Input field shows &quot;Ask ChatGPT&quot;. Right screen shows the Pueblo &amp; Pine chat interface with the same Desert Cottages listing and an AI response &quot;If you're planning a trip to Sante Fe, I'm happy to help. When are you thinking of going?&quot; with input field &quot;Ask Pueblo &amp; Pine&quot; and iOS keyboard visible.\" src=\"https://static.simonwillison.net/static/2026/chatgpt-ads.jpg\" /></p>\n<p>The user asks about trips to Santa Fe, and an ad shows up for a cottage rental business there. This particular example imagines an option to start a direct chat with a bot aligned with that advertiser, at which point presumably the advertiser can influence the answers all they like!\n\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/ads\">ads</a>, <a href=\"https://simonwillison.net/tags/ai\">ai</a>, <a href=\"https://simonwillison.net/tags/openai\">openai</a>, <a href=\"https://simonwillison.net/tags/generative-ai\">generative-ai</a>, <a href=\"https://simonwillison.net/tags/chatgpt\">chatgpt</a>, <a href=\"https://simonwillison.net/tags/llms\">llms</a></p>", "source_url": "https://simonwillison.net/atom/everything/", "article_url": "https://simonwillison.net/2026/Jan/16/chatgpt-ads/#atom-everything", "pub_date": "2026-01-16T21:28:26+00:00", "label": true, "category": "ai_ml", "notes": "heuristic label"}
{"id": "efce9e95877e7ddf", "title": "Quoting Chris Lloyd", "description": "<blockquote cite=\"https://news.ycombinator.com/item?id=46699072#46706040\"><p>Most people's mental model of Claude Code is that \"it's just a TUI\" but it should really be closer to \"a small game engine\".</p>\n<p>For each frame our pipeline constructs a scene graph with React then:</p>\n<p>-&gt; layout elements<br />\n-&gt; rasterize them to a 2d screen<br />\n-&gt; diff that against the previous screen<br />\n-&gt; <em>finally</em> use the diff to generate ANSI sequences to draw</p>\n<p>We have a ~16ms frame budget so we have roughly ~5ms to go from the React scene graph to ANSI written.</p></blockquote>\n<p class=\"cite\">&mdash; <a href=\"https://news.ycombinator.com/item?id=46699072#46706040\">Chris Lloyd</a>, Claude Code team at Anthropic</p>\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/react\">react</a>, <a href=\"https://simonwillison.net/tags/claude-code\">claude-code</a></p>", "source_url": "https://simonwillison.net/atom/everything/", "article_url": "https://simonwillison.net/2026/Jan/22/chris-lloyd/#atom-everything", "pub_date": "2026-01-22T15:34:22+00:00", "label": false, "category": "other", "notes": "heuristic label"}
{"id": "f6cec19b7cfca978", "title": "Voice Cloning with Consent", "description": "", "source_url": "https://huggingface.co/blog/feed.xml", "article_url": "https://huggingface.co/blog/voice-consent-gate", "pub_date": "2025-10-28T00:00:00+00:00", "label": false, "category": "other", "notes": "heuristic label"}
{"id": "2abfef09308ab960", "title": "PayTo available in Australia", "description": "Businesses in Australia can now offer PayTo. Accept one-off and recurring direct debits with PayTo to get real-time payment confirmation and funds deposited into your Stripe balance instantly, 24 hours a day, every day of the year.", "source_url": "https://stripe.com/blog/feed.rss", "article_url": "https://stripe.com/payment-method/payto", "pub_date": "2025-12-15T00:00:00+00:00", "label": true, "category": "ai_ml", "notes": "heuristic label"}
{"id": "3c2ceaab5f3c96ce", "title": "How AWS deals with a major outage", "description": "What happens when there&#8217;s a massive outage at AWS? A member of AWS&#8217;s Incident Response team lifts the lid, after playing a key role in resolving the leading cloud provider&#8217;s most recent major outage", "source_url": "https://newsletter.pragmaticengineer.com/feed", "article_url": "https://newsletter.pragmaticengineer.com/p/how-aws-deals-with-a-major-outage", "pub_date": "2025-12-16T16:58:00+00:00", "label": true, "category": "security", "notes": "heuristic label"}
{"id": "54fb706c5f1f90a7", "title": "We Got Claude to Fine-Tune an Open Source LLM", "description": "", "source_url": "https://huggingface.co/blog/feed.xml", "article_url": "https://huggingface.co/blog/hf-skills-training", "pub_date": "2025-12-04T00:00:00+00:00", "label": true, "category": "ai_ml", "notes": "heuristic label"}
{"id": "c3a1061238270f00", "title": "Writing Fragments", "description": "<p>If you‚Äôre a regular reader of my site, you‚Äôll have noticed that in the\nlast few months I‚Äôve been making a <a href=\"https://martinfowler.com/articles/20251204-frags.html\">number</a> of ‚Äúfragments‚Äù <a href=\"https://martinfowler.com/articles/2025-11-19-frags.html\">posts</a>. Such a post\nis a short post with a bunch of little, unconnected segments. These are\nusually a reference to something I‚Äôve found on the web, sometimes a small\nthought of my own.</p>\n\n<p>A few years ago, I wouldn‚Äôt have covered these topics with posts on my\nown site. Instead I would use Twitter, either retweeting someone else‚Äôs\npoint, or just highlighting something I‚Äôd found. But since the Muskover,\nTwitter has effectively died. I‚Äôm not saying that due to any technical\nissues with the site, which has mostly just been fine, nor directly due to\nany of the policy changes there. The point is that lots of people have left, so that\nthe audience I would have reached with Twitter is now fragmented. Some\nremain on X, but I see more activity on LinkedIn. There‚Äôs also Fediverse/Mastodon\nand Bluesky.</p>\n\n<p>What this means for short posts is that I can no longer just post in one\nplace. When I announce new articles on martinfowler.com, I announce now on\nfour social media sites (X, LinkedIn, Fediverse, and Bluesky). It makes\nsense to do this, but I don‚Äôt want to go through all this hassle for the\nkind of micro-post that Twitter served so well.</p>\n\n<p>So I‚Äôve started to batch them up. When I see something interesting, I\nmake a note. When I have enough notes, I post a fragments post. Initially I\ndid this in a rather ad-hoc way, just using the same mechanisms I use for\nmost articles, but last week I started to put in some more deliberate\nmechanisms into the site. (If you‚Äôre observant, you‚Äôll spot that in the URLs.)</p>\n\n<p>One benefit of all of this, at least in my book, is that it means my material is\nnow fully visible in RSS. I‚Äôm probably showing my age, but I‚Äôm a big fan of RSS\n(or in my case, strictly Atom) feeds. I miss the feel of the heyday of the\n‚Äúblogosphere‚Äù before it got steamrolled by social media, and these fragment\nposts are, of course, just the same as the link blogs from that era. I still use my\nRSS reader every day to keep up with writers I like. (I‚Äôm pleased that Substack\nmakes its content available via RSS.) It bothered me a bit that my micro-founts\nof Twitter knowledge weren‚Äôt visible on RSS, but was too lazy to do something\nabout it. Now I don‚Äôt need to - the fragments are available in my RSS feed.</p>", "source_url": "https://martinfowler.com/feed.atom", "article_url": "https://martinfowler.com/articles/writing-fragments.html", "pub_date": "2025-12-16T15:06:00+00:00", "label": false, "category": "other", "notes": "heuristic label"}
{"id": "806ce95e2832308a", "title": "What we know about Iran‚Äôs Internet shutdown", "description": "Cloudflare Radar data shows Internet traffic from Iran has effectively dropped to zero since January 8, signaling a complete shutdown in the country and disconnection from the global Internet.", "source_url": "https://blog.cloudflare.com/rss/", "article_url": "https://blog.cloudflare.com/iran-protests-internet-shutdown/", "pub_date": "2026-01-13T00:00:00+00:00", "label": false, "category": "other", "notes": "heuristic label"}
{"id": "d773f8b93807f912", "title": "Strengthening our partnership with the UK government to support prosperity and security in the AI era", "description": "Deepening our partnership with the UK government to support prosperity and security in the AI era", "source_url": "https://deepmind.google/blog/rss.xml", "article_url": "https://deepmind.google/blog/strengthening-our-partnership-with-the-uk-government-to-support-prosperity-and-security-in-the-ai-era/", "pub_date": "2025-12-10T14:59:21+00:00", "label": true, "category": "security", "notes": "heuristic label"}
{"id": "8c942b1b0643abf9", "title": "Accelerating discovery with the AI for Math Initiative", "description": "The initiative brings together some of the world's most prestigious research institutions to pioneer the use of AI in mathematical research.", "source_url": "https://deepmind.google/blog/rss.xml", "article_url": "https://deepmind.google/blog/accelerating-discovery-with-the-ai-for-math-initiative/", "pub_date": "2025-10-29T14:31:13+00:00", "label": true, "category": "ai_ml", "notes": "heuristic label"}
{"id": "2bf44e703bf8ff82", "title": "Introducing Edu for Countries", "description": "Edu for Countries is a new OpenAI initiative helping governments use AI to modernize education systems and build future-ready workforces.", "source_url": "https://openai.com/blog/rss.xml", "article_url": "https://openai.com/index/edu-for-countries", "pub_date": "2026-01-21T01:00:00+00:00", "label": true, "category": "policy", "notes": "manual: Program launch aimed at governments."}
{"id": "c41359d9218a2419", "title": "A pragmatic guide to LLM evals for devs", "description": "Evals are a new toolset for any and all AI engineers &#8211; and software engineers should also know about them. Move from guesswork to a systematic engineering process for improving AI quality.", "source_url": "https://newsletter.pragmaticengineer.com/feed", "article_url": "https://newsletter.pragmaticengineer.com/p/evals", "pub_date": "2025-12-02T17:19:57+00:00", "label": true, "category": "ai_ml", "notes": "heuristic label"}
{"id": "3650095d8e8f516b", "title": "Background Coding Agents: Predictable Results Through Strong Feedback Loops (Part 3)", "description": "<p>The system we built to ensure our AI agents produce predictable, trustworthy code.</p>\n<p>The post <a href=\"https://engineering.atspotify.com/2025/12/feedback-loops-background-coding-agents-part-3/\">Background Coding Agents: Predictable Results Through Strong Feedback Loops (Part 3)</a> appeared first on <a href=\"https://engineering.atspotify.com\">Spotify Engineering</a>.</p>", "source_url": "https://engineering.atspotify.com/feed/", "article_url": "https://engineering.atspotify.com/2025/12/feedback-loops-background-coding-agents-part-3/", "pub_date": "2025-12-09T15:14:19+00:00", "label": true, "category": "ai_ml", "notes": "heuristic label"}
{"id": "e578faab43dc4262", "title": "TIL from taking Neon I at the Crucible", "description": "<p><strong><a href=\"https://til.simonwillison.net/neon/neon-1\">TIL from taking Neon I at the Crucible</a></strong></p>\nThings I learned about making neon signs after a week long intensive evening class at <a href=\"https://www.thecrucible.org/\">the Crucible</a> in Oakland.\n\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/art\">art</a>, <a href=\"https://simonwillison.net/tags/til\">til</a></p>", "source_url": "https://simonwillison.net/atom/everything/", "article_url": "https://simonwillison.net/2026/Jan/11/neon-i-at-the-crucible/#atom-everything", "pub_date": "2026-01-11T17:35:57+00:00", "label": false, "category": "other", "notes": "heuristic label"}
{"id": "11aaa5f2f34723b5", "title": "Mixpanel security incident: what OpenAI users need to know", "description": "OpenAI shares details about a Mixpanel security incident involving limited API analytics data. No API content, credentials, or payment details were exposed. Learn what happened and how we‚Äôre protecting users.", "source_url": "https://openai.com/blog/rss.xml", "article_url": "https://openai.com/index/mixpanel-incident", "pub_date": "2025-11-26T19:00:00+00:00", "label": true, "category": "security", "notes": "heuristic label"}
{"id": "dbe8b18b5b32aeb1", "title": "Strengthening the U.S. AI supply chain through domestic manufacturing", "description": "OpenAI launches a new RFP to strengthen the U.S. AI supply chain by accelerating domestic manufacturing, creating jobs, and scaling AI infrastructure.", "source_url": "https://openai.com/blog/rss.xml", "article_url": "https://openai.com/index/strengthening-the-us-ai-supply-chain", "pub_date": "2026-01-15T00:00:00+00:00", "label": true, "category": "ai_ml", "notes": "heuristic label"}
{"id": "d8945b4bc7efa7b3", "title": "Innovating to address streaming abuse ‚Äî and our latest transparency report", "description": "Cloudflare's H1 2025 Transparency Report is here. We discuss our principles on content blocking and our innovative approach to combating unauthorized streaming and copyright abuse.", "source_url": "https://blog.cloudflare.com/rss/", "article_url": "https://blog.cloudflare.com/h1-2025-transparency-report/", "pub_date": "2025-12-19T14:00:00+00:00", "label": false, "category": "other", "notes": "heuristic label"}
{"id": "d497b2447ceded27", "title": "Our approach to age prediction", "description": "ChatGPT is rolling out age prediction to estimate if accounts are under or over 18, applying safeguards for teens and refining accuracy over time.", "source_url": "https://openai.com/blog/rss.xml", "article_url": "https://openai.com/index/our-approach-to-age-prediction", "pub_date": "2026-01-20T00:00:00+00:00", "label": false, "category": "other", "notes": "heuristic label"}
{"id": "a75c99e63ba96b6d", "title": "Qwen3-TTS Family is Now Open Sourced: Voice Design, Clone, and Generation", "description": "<p><strong><a href=\"https://qwen.ai/blog?id=qwen3tts-0115\">Qwen3-TTS Family is Now Open Sourced: Voice Design, Clone, and Generation</a></strong></p>\nI haven't been paying much attention to the state-of-the-art in speech generation models other than noting that they've got <em>really good</em>, so I can't speak for how notable this new release from Qwen is.</p>\n<p>From <a href=\"https://github.com/QwenLM/Qwen3-TTS/blob/main/assets/Qwen3_TTS.pdf\">the accompanying paper</a>:</p>\n<blockquote>\n<p>In this report, we present the Qwen3-TTS series, a family of advanced multilingual, controllable, robust, and streaming text-to-speech models. Qwen3-TTS supports state-of- the-art 3-second voice cloning and description-based control, allowing both the creation of entirely novel voices and fine-grained manipulation over the output speech. Trained on over 5 million hours of speech data spanning 10 languages, Qwen3-TTS adopts a dual-track LM architecture for real-time synthesis [...]. Extensive experiments indicate state-of-the-art performance across diverse objective and subjective benchmark (e.g., TTS multilingual test set, InstructTTSEval, and our long speech test set). To facilitate community research and development, we release both tokenizers and models under the Apache 2.0 license.</p>\n</blockquote>\n<p>To give an idea of size, <a href=\"https://huggingface.co/Qwen/Qwen3-TTS-12Hz-1.7B-Base\">Qwen/Qwen3-TTS-12Hz-1.7B-Base</a> is 4.54GB on Hugging Face and <a href=\"https://huggingface.co/Qwen/Qwen3-TTS-12Hz-0.6B-Base\">Qwen/Qwen3-TTS-12Hz-0.6B-Base</a> is 2.52GB.</p>\n<p>The <a href=\"https://huggingface.co/spaces/Qwen/Qwen3-TTS\">Hugging Face demo</a> lets you try out the 0.6B and 1.7B models for free in your browser, including voice cloning:</p>\n<p><img alt=\"Screenshot of a Qwen3-TTS voice cloning web interface with three tabs at top: &quot;Voice Design&quot;, &quot;Voice Clone (Base)&quot; (selected), and &quot;TTS (CustomVoice)&quot;. The page is titled &quot;Clone Voice from Reference Audio&quot; and has two main sections. Left section: &quot;Reference Audio (Upload a voice sample clone)&quot; showing an audio waveform player at 0:00/0:34 with playback controls, upload and microphone icons, followed by &quot;Reference Text (Transcript of the reference audio)&quot; containing three paragraphs: &quot;Simon Willison is the creator of Datasette, an open source tool for exploring and publishing data. He currently works full-time building open source tools for data journalism, built around Datasette and SQLite. Prior to becoming an independent open source developer, Simon was an engineering director at Eventbrite. Simon joined Eventbrite through their acquisition of Lanyrd, a Y Combinator funded company he co-founded in 2010. He is a co-creator of the Django Web Framework, and has been blogging about web development and programming since 2002 at simonwillison.net&quot;. Right section: &quot;Target Text (Text to synthesize with cloned voice)&quot; containing text about Qwen3-TTS speech generation capabilities, with &quot;Language&quot; dropdown set to &quot;Auto&quot; and &quot;Model Size&quot; dropdown set to &quot;1.7B&quot;, and a purple &quot;Clone &amp; Generate&quot; button at bottom.\" src=\"https://static.simonwillison.net/static/2026/qwen-voice-clone.jpg\" /></p>\n<p>I tried this out by recording myself reading <a href=\"https://simonwillison.net/about/\">my about page</a> and then having Qwen3-TTS generate audio of me reading the Qwen3-TTS announcement post. Here's the result:</p>\n<p><audio controls=\"controls\" style=\"width: 100%;\">\n  <source src=\"https://static.simonwillison.net/static/2026/qwen-tts-clone.wav\" type=\"audio/wav\" />\n  Your browser does not support the audio element.\n</audio></p>\n<p>It's important that everyone understands that voice cloning is now something that's available to anyone with a GPU and a few GBs of VRAM... or in this case a web browser that can access Hugging Face.</p>\n<p><strong>Update</strong>: Prince Canuma <a href=\"https://x.com/Prince_Canuma/status/2014453857019904423\">got this working</a> with his <a href=\"https://pypi.org/project/mlx-audio/\">mlx-audio</a> library. I <a href=\"https://claude.ai/share/2e01ad60-ca38-4e14-ab60-74eaa45b2fbd\">had Claude</a> turn that into <a href=\"https://github.com/simonw/tools/blob/main/python/q3_tts.py\">a CLI tool</a> which you can run with <code>uv</code> ike this:</p>\n<pre><code>uv run https://tools.simonwillison.net/python/q3_tts.py \\\n  'I am a pirate, give me your gold!' \\\n  -i 'gruff voice' -o pirate.wav\n</code></pre>\n<p>The <code>-i</code> option lets you use a prompt to describe the voice it should use. On first run this downloads a 4.5GB model file from Hugging Face.\n\n    <p><small></small>Via <a href=\"https://news.ycombinator.com/item?id=46719229\">Hacker News</a></small></p>\n\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/text-to-speech\">text-to-speech</a>, <a href=\"https://simonwillison.net/tags/ai\">ai</a>, <a href=\"https://simonwillison.net/tags/generative-ai\">generative-ai</a>, <a href=\"https://simonwillison.net/tags/hugging-face\">hugging-face</a>, <a href=\"https://simonwillison.net/tags/uv\">uv</a>, <a href=\"https://simonwillison.net/tags/qwen\">qwen</a>, <a href=\"https://simonwillison.net/tags/mlx\">mlx</a>, <a href=\"https://simonwillison.net/tags/prince-canuma\">prince-canuma</a>, <a href=\"https://simonwillison.net/tags/ai-in-china\">ai-in-china</a></p>", "source_url": "https://simonwillison.net/atom/everything/", "article_url": "https://simonwillison.net/2026/Jan/22/qwen3-tts/#atom-everything", "pub_date": "2026-01-22T17:42:34+00:00", "label": true, "category": "ai_ml", "notes": "manual: Open-source TTS model family release."}
{"id": "a8ee208586f48ae2", "title": "OpenAI to acquire Neptune", "description": "OpenAI is acquiring Neptune to deepen visibility into model behavior and strengthen the tools researchers use to track experiments and monitor training.", "source_url": "https://openai.com/blog/rss.xml", "article_url": "https://openai.com/index/openai-to-acquire-neptune", "pub_date": "2025-12-03T10:00:00+00:00", "label": true, "category": "ai_ml", "notes": "heuristic label"}
{"id": "e76e1fd8bcc71246", "title": "Metronome + Stripe: Building the future of billing", "description": "Together, Metronome and Stripe are building the most flexible and complete billing solution on the market‚Äîone that works for everyone, from engineers in a garage figuring out their business model to public companies monetizing at global scale.", "source_url": "https://stripe.com/blog/feed.rss", "article_url": "https://stripe.com/blog/metronome-stripe-building-the-future-of-billing", "pub_date": "2026-01-23T00:00:00+00:00", "label": false, "category": "other", "notes": "heuristic label"}
{"id": "c88b94de28a7e4e5", "title": "Building MCP servers in the real world", "description": "How engineers and teams use MCP servers: from debugging to working with legacy systems, & giving non-devs more access. Details from 40+ devs &#8211; with some surprises", "source_url": "https://newsletter.pragmaticengineer.com/feed", "article_url": "https://newsletter.pragmaticengineer.com/p/mcp-deepdive", "pub_date": "2025-12-09T16:52:37+00:00", "label": false, "category": "other", "notes": "heuristic label"}
{"id": "33e70746da412ab1", "title": "The Pulse #158: new ways of coding with AI tools", "description": "The &#8220;Ralph method&#8221; and running multiple agents at once gain popularity, AI job cuts at dev tooling and education providers, four nines of uptime after quitting the cloud, and more", "source_url": "https://newsletter.pragmaticengineer.com/feed", "article_url": "https://newsletter.pragmaticengineer.com/p/the-pulse-158-new-ways-of-coding", "pub_date": "2026-01-08T20:17:41+00:00", "label": true, "category": "ai_ml", "notes": "heuristic label"}
{"id": "ef7d3685c5604a5c", "title": "AlphaFold: Five years of impact", "description": "Explore how AlphaFold has accelerated science and fueled a global wave of biological discovery.", "source_url": "https://deepmind.google/blog/rss.xml", "article_url": "https://deepmind.google/blog/alphafold-five-years-of-impact/", "pub_date": "2025-11-25T16:00:12+00:00", "label": false, "category": "other", "notes": "heuristic label"}
{"id": "66e11f2f40814d37", "title": "SSH has no Host header", "description": "<p><strong><a href=\"https://blog.exe.dev/ssh-host-header\">SSH has no Host header</a></strong></p>\n<a href=\"https://exe.dev/\">exe.dev</a> is a new hosting service that, for $20/month, gives you up to 25 VMs \"that share 2 CPUs and 8GB RAM\". Everything happens over SSH, including creating new VMs. Once configured you can sign into your exe.dev VMs like this:</p>\n<pre><code>ssh simon.exe.dev\n</code></pre>\n<p>Here's the clever bit: when you run the above command <code>exe.dev</code> signs you into your VM of that name... but they don't assign every VM its own IP address and SSH has no equivalent of the Host header, so how does their load balancer know <em>which</em> of your VMs to forward you on to?</p>\n<p>The answer is that while they don't assign a unique IP to every VM they <em>do</em> have enough IPs that they can ensure each of your VMs has an IP that is unique to your account.</p>\n<p>If I create two VMs they will each resolve to a separate IP address, each of which is shared with many other users. The underlying infrastructure then identifies my user account from my SSH public key and can determine which underlying VM to forward my SSH traffic to.\n\n    <p><small></small>Via <a href=\"https://lobste.rs/s/7oqiqi/ssh_has_no_host_header\">lobste.rs</a></small></p>\n\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/dns\">dns</a>, <a href=\"https://simonwillison.net/tags/hosting\">hosting</a>, <a href=\"https://simonwillison.net/tags/ssh\">ssh</a></p>", "source_url": "https://simonwillison.net/atom/everything/", "article_url": "https://simonwillison.net/2026/Jan/22/ssh-has-no-host-header/#atom-everything", "pub_date": "2026-01-22T23:57:50+00:00", "label": false, "category": "other", "notes": "manual: Technical note; limited scope."}
{"id": "6c1bff7d772461fa", "title": "Transformers v5: Simple model definitions powering the AI ecosystem", "description": "", "source_url": "https://huggingface.co/blog/feed.xml", "article_url": "https://huggingface.co/blog/transformers-v5", "pub_date": "2025-12-01T00:00:00+00:00", "label": true, "category": "ai_ml", "notes": "heuristic label"}
{"id": "d4dbea5e2e9d1807", "title": "Agentic AI and Security", "description": "<div class=\"img-link\"><a href=\"https://martinfowler.com/articles/agentic-ai-security.html\"><img src=\"https://martinfowler.com/articles/agentic-ai-security/card.png\" width=\"350px\" /></a></div>\n\n<p>Agentic AI systems are amazing, but introduce equally\n      amazing security risks. <b class=\"author\">Korny Sietsma</b> explains that their\n      core architecture opens up <a href=\"https://martinfowler.com/articles/agentic-ai-security.html\">security issues</a> through what Simon Willison\n      named the ‚ÄúLethal Trifecta‚Äù. Korny goes on to talk about how to\n      mitigate this through removing legs of the trifecta and splitting complex\n      tasks.</p>\n\n<p><a class=\"more\" href=\"https://martinfowler.com/articles/agentic-ai-security.html\">more‚Ä¶</a></p>", "source_url": "https://martinfowler.com/feed.atom", "article_url": "https://martinfowler.com/articles/agentic-ai-security.html", "pub_date": "2025-10-28T13:20:00+00:00", "label": true, "category": "security", "notes": "heuristic label"}
{"id": "b38c7ea272ce2018", "title": "Investing in Merge Labs", "description": "OpenAI is investing in Merge Labs to support new brain computer interfaces that bridge biological and artificial intelligence to maximize human ability, agency, and experience.", "source_url": "https://openai.com/blog/rss.xml", "article_url": "https://openai.com/index/investing-in-merge-labs", "pub_date": "2026-01-15T07:00:00+00:00", "label": false, "category": "other", "notes": "heuristic label"}
{"id": "1681e19beb1b1c9c", "title": "A Software Library with No Code", "description": "<p><strong><a href=\"https://www.dbreunig.com/2026/01/08/a-software-library-with-no-code.html\">A Software Library with No Code</a></strong></p>\nProvocative experiment from Drew Breunig, who designed a new library for time formatting (\"3 hours ago\" kind of thing) called \"whenwords\" that has no code at all, just a carefully written specification, an AGENTS.md and a collection of conformance tests in a YAML file.</p>\n<p>Pass that to your coding agent of choice, tell it what language you need and it will write it for you on demand!</p>\n<p>This meshes nearly with my recent <a href=\"https://simonwillison.net/2025/Dec/31/the-year-in-llms/#the-year-of-conformance-suites\">interest in conformance suites</a>. If you publish good enough language-independent tests it's pretty astonishing how far today's coding agents can take you!\n\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/testing\">testing</a>, <a href=\"https://simonwillison.net/tags/ai\">ai</a>, <a href=\"https://simonwillison.net/tags/generative-ai\">generative-ai</a>, <a href=\"https://simonwillison.net/tags/llms\">llms</a>, <a href=\"https://simonwillison.net/tags/ai-assisted-programming\">ai-assisted-programming</a>, <a href=\"https://simonwillison.net/tags/drew-breunig\">drew-breunig</a>, <a href=\"https://simonwillison.net/tags/coding-agents\">coding-agents</a>, <a href=\"https://simonwillison.net/tags/conformance-suites\">conformance-suites</a></p>", "source_url": "https://simonwillison.net/atom/everything/", "article_url": "https://simonwillison.net/2026/Jan/10/a-software-library-with-no-code/#atom-everything", "pub_date": "2026-01-10T23:41:58+00:00", "label": false, "category": "other", "notes": "heuristic label"}
{"id": "08a769bae1802155", "title": "How we‚Äôre bringing AI image verification to the Gemini app", "description": "", "source_url": "https://deepmind.google/blog/rss.xml", "article_url": "https://deepmind.google/blog/how-were-bringing-ai-image-verification-to-the-gemini-app/", "pub_date": "2025-11-20T15:13:19+00:00", "label": true, "category": "ai_ml", "notes": "heuristic label"}
{"id": "44dddd125ce8d2e8", "title": "Start building with Gemini 3", "description": "", "source_url": "https://deepmind.google/blog/rss.xml", "article_url": "https://deepmind.google/blog/start-building-with-gemini-3/", "pub_date": "2025-11-18T17:49:13+00:00", "label": true, "category": "ai_ml", "notes": "heuristic label"}
{"id": "7d3650f9d2306fc7", "title": "The next chapter of the Microsoft‚ÄìOpenAI partnership", "description": "Microsoft and OpenAI sign a new agreement that strengthens its long-term partnership, expands innovation, and ensures responsible AI progress.", "source_url": "https://openai.com/blog/rss.xml", "article_url": "https://openai.com/index/next-chapter-of-microsoft-openai-partnership", "pub_date": "2025-10-28T06:00:00+00:00", "label": true, "category": "ai_ml", "notes": "heuristic label"}
{"id": "d44f708232c43ac7", "title": "Shuffle: Making Random Feel More Human", "description": "<p>Shuffle has always been one of Spotify‚Äôs most-used features, and also one of the most misunderstood. For...</p>\n<p>The post <a href=\"https://engineering.atspotify.com/2025/11/shuffle-making-random-feel-more-human/\">Shuffle: Making Random Feel More Human</a> appeared first on <a href=\"https://engineering.atspotify.com\">Spotify Engineering</a>.</p>", "source_url": "https://engineering.atspotify.com/feed/", "article_url": "https://engineering.atspotify.com/2025/11/shuffle-making-random-feel-more-human/", "pub_date": "2025-11-13T13:00:06+00:00", "label": false, "category": "other", "notes": "heuristic label"}
{"id": "d3d625e8c5e88506", "title": "Updating our Model Spec with teen protections", "description": "OpenAI is updating its Model Spec with new Under-18 Principles that define how ChatGPT should support teens with safe, age-appropriate guidance grounded in developmental science. The update strengthens guardrails, clarifies expected model behavior in higher-risk situations, and builds on our broader work to improve teen safety across ChatGPT.", "source_url": "https://openai.com/blog/rss.xml", "article_url": "https://openai.com/index/updating-model-spec-with-teen-protections", "pub_date": "2025-12-18T11:00:00+00:00", "label": true, "category": "ai_ml", "notes": "heuristic label"}
{"id": "6521c74d060e5b33", "title": "20x Faster TRL Fine-tuning with RapidFire AI", "description": "", "source_url": "https://huggingface.co/blog/feed.xml", "article_url": "https://huggingface.co/blog/rapidfireai", "pub_date": "2025-11-21T00:00:00+00:00", "label": true, "category": "ai_ml", "notes": "heuristic label"}
{"id": "6270bfae82a5924a", "title": "A data model for Git (and other docs updates)", "description": "<p>Hello! This past fall, I decided to take some time to work on Git&rsquo;s\ndocumentation. I&rsquo;ve been thinking about working on open source docs for a long\ntime &ndash; usually if I think the documentation for something could be improved,\nI&rsquo;ll write a blog post or a zine or something. But this time I wondered: could I\ninstead make a few improvements to the official documentation?</p>\n<p>So <a href=\"https://marieflanagan.com/\">Marie</a> and I made a few changes to the Git\ndocumentation!</p>\n<h3 id=\"a-data-model-for-git\">a data model for Git</h3>\n<p>After a while working on the documentation, we noticed that Git uses the terms\n&ldquo;object&rdquo;, &ldquo;reference&rdquo;, or &ldquo;index&rdquo; in its documentation a lot, but that it didn&rsquo;t\nhave a great explanation of what those terms mean or how they relate to other\ncore concepts like &ldquo;commit&rdquo; and &ldquo;branch&rdquo;. So we wrote a new &ldquo;data model&rdquo; document!</p>\n<p>You can <a href=\"https://github.com/git/git/blob/master/Documentation/gitdatamodel.adoc\">read the data model here for now</a>.\nI assume at some point (after the next release?) it&rsquo;ll also be on the <a href=\"https://git-scm.com\">Git website</a>.</p>\n<p>I&rsquo;m excited about this because understanding how Git organizes its commit and\nbranch data has really helped me reason about how Git works over the years,\nand I think it&rsquo;s important to have a short (1600 words!) version of the data\nmodel that&rsquo;s accurate.</p>\n<p>The &ldquo;accurate&rdquo; part turned out to not be that easy: I knew the basics of how\nGit&rsquo;s data model worked, but during the review process I learned some new\ndetails and had to make quite a few changes (for example how merge conflicts are\nstored in the staging area).</p>\n<h3 id=\"updates-to-git-push-git-pull-and-more\">updates to <code>git push</code>, <code>git pull</code>, and more</h3>\n<p>I also worked on updating the introduction to some of Git&rsquo;s core man pages.\nI quickly realized that &ldquo;just try to improve it according to my best judgement&rdquo;\nwas not going to work: why should the maintainers believe me that my version is\nbetter?</p>\n<p>I&rsquo;ve seen a problem a lot when discussing open source documentation changes\nwhere 2 expert users of the software argue about whether an explanation\nis clear or not (&ldquo;I think X would be a good way to explain it! Well, I think Y\nwould be better!&rdquo;)</p>\n<p>I don&rsquo;t think this is very productive (expert users of a piece of software\nare notoriously bad at being able to tell if an explanation will be clear to\nnon-experts), so I needed to find a way to identify problems with the man\npages that was a little more evidence-based.</p>\n<h3 id=\"getting-test-readers-to-identify-problems\">getting test readers to identify problems</h3>\n<p>I asked for test readers on Mastodon to read the current version of\ndocumentation and tell me what they find confusing or what questions they have.\nAbout 80 test readers left comments, and I learned so much!</p>\n<p>People left a huge amount of great feedback, for example:</p>\n<ul>\n<li>terminology they didn&rsquo;t understand (what&rsquo;s a pathspec? what does &ldquo;reference&rdquo; mean? does &ldquo;upstream&rdquo; have a specific meaning in Git?)</li>\n<li>specific confusing sentences</li>\n<li>suggestions of things things to add (&ldquo;I do X all the time, I think it should be included here&rdquo;)</li>\n<li>inconsistencies (&ldquo;here it implies X is the default, but elsewhere it implies Y is the default&rdquo;)</li>\n</ul>\n<p>Most of the test readers had been using Git for at least 5-10 years, which\nI think worked well &ndash; if a group of test readers who have been using Git\nregularly for 5+ years find a sentence or term impossible to understand, it\nmakes it easy to argue that the documentation should be updated to make it\nclearer.</p>\n<p>I thought this &ldquo;get users of the software to comment on the existing\ndocumentation and then fix the problems they find&rdquo; pattern worked really\nwell and I&rsquo;m excited about potentially trying it again in the future.</p>\n<h3 id=\"the-man-page-changes\">the man page changes</h3>\n<p>We ended updating these 4 man pages:</p>\n<ul>\n<li><code>git add</code> (<a href=\"https://github.com/git/git/blob/2b3ae040/Documentation/git-add.adoc\">before</a>, <a href=\"https://github.com/git/git/blob/e0bfec3dfc356f7d808eb5ee546a54116b794397/Documentation/git-add.adoc\">after</a>)</li>\n<li><code>git checkout</code> (<a href=\"https://github.com/git/git/blob/2b3ae040/Documentation/git-checkout.adoc\">before</a>, <a href=\"https://github.com/git/git/blob/e0bfec3dfc356f7d808eb5ee546a54116b794397/Documentation/git-checkout.adoc\">after</a>)</li>\n<li><code>git push</code> (<a href=\"https://github.com/git/git/blob/2b3ae040/Documentation/git-push.adoc\">before</a>, <a href=\"https://github.com/git/git/blob/e0bfec3dfc356f7d808eb5ee546a54116b794397/Documentation/git-push.adoc\">after</a>)</li>\n<li><code>git pull</code> (<a href=\"https://github.com/git/git/blob/2b3ae040/Documentation/git-pull.adoc\">before</a>, <a href=\"https://github.com/git/git/blob/e0bfec3dfc356f7d808eb5ee546a54116b794397/Documentation/git-pull.adoc\">after</a>)</li>\n</ul>\n<p>The <code>git push</code> and <code>git pull</code> changes were the most interesting to me: in\naddition to updating the intro to those pages, we also ended up writing:</p>\n<ul>\n<li><a href=\"https://github.com/git/git/blob/e0bfec3dfc356f7d808eb5ee546a54116b794397/Documentation/urls-remotes.adoc#upstream-branches\">a section describing what the term &ldquo;upstream branch&rdquo; means</a> (which previously wasn&rsquo;t really explained)</li>\n<li><a href=\"https://github.com/git/git/blob/e0bfec3dfc356f7d808eb5ee546a54116b794397/Documentation/git-push.adoc#options\">a cleaned-up description of what a &ldquo;push refspec&rdquo; is</a></li>\n</ul>\n<p>Making those changes really gave me an appreciation for how much work it is\nto maintain open source documentation: it&rsquo;s not easy to write things that are\nboth clear and true, and sometimes we had to make compromises, for example the sentence\n&ldquo;<code>git push</code> may fail if you haven‚Äôt set an upstream for the current branch,\ndepending on what <code>push.default</code> is set to.&rdquo; is a little vague, but the exact\ndetails of what &ldquo;depending&rdquo; means are really complicated and untangling that is\na big project.</p>\n<h3 id=\"on-the-process-for-contributing-to-git\">on the process for contributing to Git</h3>\n<p>It took me a while to understand Git&rsquo;s development process.\nI&rsquo;m not going to try to describe it here (that could be a whole other post!), but a few quick notes:</p>\n<ul>\n<li>Git has a <a href=\"https://git-scm.com/community#discord\">Discord server</a>\nwith a &ldquo;my first contribution&rdquo; channel for help with getting started contributing.\nI found people to be very welcoming on the Discord.</li>\n<li>I used <a href=\"https://gitgitgadget.github.io/\">GitGitGadget</a> to make all of my contributions.\nThis meant that I could make a GitHub pull request (a workflow I&rsquo;m comfortable\nwith) and GitGitGadget would convert my PRs into the system the Git developers\nuse (emails with patches attached). GitGitGadget worked great and I was very\ngrateful to not have to learn how to send patches by email with Git.</li>\n<li>Otherwise I used my normal email client (Fastmail&rsquo;s web interface) to reply\nto emails, wrapping my text to 80 character lines since that&rsquo;s the mailing\nlist norm.</li>\n</ul>\n<p>I also found the mailing list archives on <a href=\"https://lore.kernel.org/git/\">lore.kernel.org</a>\nhard to navigate, so I hacked together <a href=\"https://github.com/jvns/git-list-viewer\">my own git list viewer</a>\nto make it easier to read the long mailing list threads.</p>\n<p>Many people helped me navigate the contribution process and review the changes:\nthanks to Emily Shaffer, Johannes Schindelin (the author of GitGitGadget),\nPatrick Steinhardt, Ben Knoble, Junio Hamano, and more.</p>\n<p>(I&rsquo;m experimenting with <a href=\"https://comments.jvns.ca/post/115861337435768520\">comments on Mastodon, you can see the comments here</a>)</p>", "source_url": "https://jvns.ca/atom.xml", "article_url": "https://jvns.ca/blog/2026/01/08/a-data-model-for-git/", "pub_date": "2026-01-08T00:00:00+00:00", "label": true, "category": "ai_ml", "notes": "heuristic label"}
{"id": "392d7c3bed07093a", "title": "Announcing the initial People-First AI Fund grantees", "description": "The OpenAI Foundation announces the initial recipients of the People-First AI Fund, awarding $40.5M in unrestricted grants to 208 nonprofits supporting community innovation and opportunity.", "source_url": "https://openai.com/blog/rss.xml", "article_url": "https://openai.com/index/people-first-ai-fund-grantees", "pub_date": "2025-12-03T08:00:00+00:00", "label": true, "category": "ai_ml", "notes": "heuristic label"}
{"id": "765a710d140abaac", "title": "Congratulations to the recipients of the 2025 Spotify FOSS Fund", "description": "<p>TL;DR Established in 2022 as a way to help support the great open source ecosystem projects that Spotify...</p>\n<p>The post <a href=\"https://engineering.atspotify.com/2026/1/congratulations-to-the-recipients-of-the-2025-spotify-foss-fund/\">Congratulations to the recipients of the 2025 Spotify FOSS Fund</a> appeared first on <a href=\"https://engineering.atspotify.com\">Spotify Engineering</a>.</p>", "source_url": "https://engineering.atspotify.com/feed/", "article_url": "https://engineering.atspotify.com/2026/1/congratulations-to-the-recipients-of-the-2025-spotify-foss-fund/", "pub_date": "2026-01-22T15:00:25+00:00", "label": false, "category": "other", "notes": "heuristic label"}
{"id": "6c2f0e21e600996f", "title": "Introducing GPT-5.1 for developers", "description": "GPT-5.1 is now available in the API, bringing faster adaptive reasoning, extended prompt caching, improved coding performance, and new apply_patch and shell tools.", "source_url": "https://openai.com/blog/rss.xml", "article_url": "https://openai.com/index/gpt-5-1-for-developers", "pub_date": "2025-11-13T00:00:00+00:00", "label": true, "category": "ai_ml", "notes": "heuristic label"}
{"id": "3b5aac449e77edb5", "title": "Fly's new Sprites.dev addresses both developer sandboxes and API sandboxes at the same time", "description": "<p>New from Fly.io today: <a href=\"https://sprites.dev\">Sprites.dev</a>. Here's their <a href=\"https://fly.io/blog/code-and-let-live/\">blog post</a> and <a href=\"https://www.youtube.com/watch?v=7BfTLlwO4hw\">YouTube demo</a>. It's an interesting new product that's quite difficult to explain - Fly call it \"Stateful sandbox environments with checkpoint &amp; restore\" but I see it as hitting two of my current favorite problems: a safe development environment for running coding agents <em>and</em> an API for running untrusted code in a secure sandbox.</p>\n<p><em>Disclosure: Fly sponsor some of my work. They did not ask me to write about Sprites and I didn't get preview access prior to the launch. My enthusiasm here is genuine.</em></p>\n\n<ul>\n  <li><a href=\"https://simonwillison.net/2026/Jan/9/sprites-dev/#developer-sandboxes\">Developer sandboxes</a></li>\n  <li><a href=\"https://simonwillison.net/2026/Jan/9/sprites-dev/#storage-and-checkpoints\">Storage and checkpoints</a></li>\n  <li><a href=\"https://simonwillison.net/2026/Jan/9/sprites-dev/#really-clever-use-of-claude-skills\">Really clever use of Claude Skills</a></li>\n  <li><a href=\"https://simonwillison.net/2026/Jan/9/sprites-dev/#a-sandbox-api\">A sandbox API</a></li>\n  <li><a href=\"https://simonwillison.net/2026/Jan/9/sprites-dev/#scale-to-zero-billing\">Scale-to-zero billing</a></li>\n  <li><a href=\"https://simonwillison.net/2026/Jan/9/sprites-dev/#two-of-my-favorite-problems-at-once\">Two of my favorite problems at once</a></li>\n</ul>\n\n<h4 id=\"developer-sandboxes\">Developer sandboxes</h4>\n<p>I predicted earlier this week that <a href=\"https://simonwillison.net/2026/Jan/8/llm-predictions-for-2026/#1-year-a-challenger-disaster-for-coding-agent-security\">\"we‚Äôre due a Challenger disaster with respect to coding agent security\"</a> due to the terrifying way most of us are using coding agents like Claude Code and Codex CLI. Running them in <code>--dangerously-skip-permissions</code> mode (aka YOLO mode, where the agent acts without constantly seeking approval first) unlocks so much more power, but also means that a mistake or a malicious prompt injection can cause all sorts of damage to your system and data.</p>\n<p>The safe way to run YOLO mode is in a robust sandbox, where the worst thing that can happen is the sandbox gets messed up and you have to throw it away and get another one.</p>\n<p>That's the first problem Sprites solves:</p>\n<div class=\"highlight highlight-source-shell\"><pre>curl https://sprites.dev/install.sh <span class=\"pl-k\">|</span> bash\n\nsprite login\nsprite create my-dev-environment\nsprite console -s my-dev-environment</pre></div>\n<p>That's all it takes to get SSH connected to a fresh environment, running in an ~8GB RAM, 8 CPU server. And... Claude Code and Codex and Gemini CLI and Python 3.13 and Node.js 22.20 and a bunch of other tools are already installed.</p>\n<p>The first time you run <code>claude</code> it neatly signs you in to your existing account with Anthropic. The Sprites VM is persistent so future runs of <code>sprite console -s</code> will get you back to where you were before.</p>\n<p>... and it automatically sets up port forwarding, so you can run a localhost server on your Sprite and access it from <code>localhost:8080</code> on your machine.</p>\n<p>There's also a command you can run to assign a public URL to your Sprite, so anyone else can access it if they know the secret URL.</p>\n<h4 id=\"storage-and-checkpoints\">Storage and checkpoints</h4>\n<p>In <a href=\"https://fly.io/blog/code-and-let-live/\">the blog post</a> Kurt Mackey argues that ephemeral, disposable sandboxes are not the best fit for coding agents:</p>\n<blockquote>\n<p>The state of the art in agent isolation is a read-only sandbox. At Fly.io, we‚Äôve been selling that story for years, and we‚Äôre calling it: ephemeral sandboxes are obsolete. Stop killing your sandboxes every time you use them. [...]</p>\n<p>If you force an agent to, it‚Äôll work around containerization and do work . But you‚Äôre not helping the agent in any way by doing that. They don‚Äôt want containers. They don‚Äôt want ‚Äúsandboxes‚Äù. They want computers.</p>\n<p>[...] with an actual computer, Claude doesn‚Äôt have to rebuild my entire development environment every time I pick up a PR.</p>\n</blockquote>\n<p>Each Sprite gets a proper filesystem which persists in between sessions, even while the Sprite itself shuts down after inactivity. It sounds like they're doing some clever filesystem tricks here, I'm looking forward to learning more about those in the future.</p>\n<p>There are some clues on <a href=\"https://sprites.dev/\">the homepage</a>:</p>\n<blockquote>\n<p>You read and write to fast, directly attached NVMe storage. Your data then gets written to durable, external object storage. [...]</p>\n<p>You don't pay for allocated filesystem space, just the blocks you write. And it's all TRIM friendly, so your bill goes down when you delete things.</p>\n</blockquote>\n<p>The really clever feature is checkpoints. You (or your coding agent) can trigger a checkpoint which takes around 300ms. This captures the entire disk state and can then be rolled back to later.</p>\n<p>For more on how that works, run this in a Sprite:</p>\n<pre><code>cat /.sprite/docs/agent-context.md\n</code></pre>\n<p>Here's the relevant section:</p>\n<pre><code>## Checkpoints\n- Point-in-time checkpoints and restores available\n- Copy-on-write implementation for storage efficiency\n- Last 5 checkpoints mounted at `/.sprite/checkpoints`\n- Checkpoints capture only the writable overlay, not the base image\n</code></pre>\n<p>Or run this to see the <code>--help</code> for the command used to manage them:</p>\n<div class=\"highlight highlight-source-shell\"><pre>sprite-env checkpoints --help</pre></div>\n<p>Which looks like this:</p>\n<pre><code>sprite-env checkpoints - Manage environment checkpoints\n\nUSAGE:\n    sprite-env checkpoints &lt;subcommand&gt; [options]\n\nSUBCOMMANDS:\n    list [--history &lt;ver&gt;]  List all checkpoints (optionally filter by history version)\n    get &lt;id&gt;                Get checkpoint details (e.g., v0, v1, v2)\n    create                  Create a new checkpoint (auto-versioned)\n    restore &lt;id&gt;            Restore from a checkpoint (e.g., v1)\n\nNOTE:\n    Checkpoints are versioned as v0, v1, v2, etc.\n    Restore returns immediately and triggers an async restore that restarts the environment.\n    The last 5 checkpoints are mounted at /.sprite/checkpoints for direct file access.\n\nEXAMPLES:\n    sprite-env checkpoints list\n    sprite-env checkpoints list --history v1.2.3\n    sprite-env checkpoints get v2\n    sprite-env checkpoints create\n    sprite-env checkpoints restore v1\n</code></pre>\n<h4 id=\"really-clever-use-of-claude-skills\">Really clever use of Claude Skills</h4>\n<p>I'm <a href=\"https://simonwillison.net/2025/Oct/16/claude-skills/\">a big fan of Skills</a>, the mechanism whereby Claude Code (and increasingly other agents too) can be given additional capabilities by describing them in Markdown files in a specific directory structure.</p>\n<p>In a smart piece of design, Sprites uses pre-installed skills to teach Claude how Sprites itself works. This means you can ask Claude on the machine how to do things like open up ports and it will talk you through the process.</p>\n<p>There's all sorts of interesting stuff in the <code>/.sprite</code> folder on that machine - digging in there is a great way to learn more about how Sprites works.</p>\n<h4 id=\"a-sandbox-api\">A sandbox API</h4>\n<p>Also from my predictions post earlier this week: <a href=\"https://simonwillison.net/2026/Jan/8/llm-predictions-for-2026/#1-year-we-re-finally-going-to-solve-sandboxing\">\"We‚Äôre finally going to solve sandboxing\"</a>. I am obsessed with this problem: I want to be able to run untrusted code safely, both on my personal devices and in the context of web services I'm building for other people to use.</p>\n<p>I have <em>so many things</em> I want to build that depend on being able to take untrusted code - from users or from LLMs or from LLMs-driven-by-users - and run that code in a sandbox where I can be confident that the blast radius if something goes wrong is tightly contained.</p>\n<p>Sprites offers a clean <a href=\"https://sprites.dev/api\">JSON API</a> for doing exactly that, plus client libraries in <a href=\"https://github.com/superfly/sprites-go\">Go</a> and <a href=\"https://github.com/superfly/sprites-js\">TypeScript</a> and coming-soon <a href=\"https://github.com/superfly/sprites-py\">Python</a> and <a href=\"https://github.com/superfly/sprites-ex\">Elixir</a>.</p>\n<p>From their quick start:</p>\n<pre><code># Create a new sprite\ncurl -X PUT https://api.sprites.dev/v1/sprites/my-sprite \\\n-H \"Authorization: Bearer $SPRITES_TOKEN\"\n\n# Execute a command\ncurl -X POST https://api.sprites.dev/v1/sprites/my-sprite/exec \\\n-H \"Authorization: Bearer $SPRITES_TOKEN\" \\\n-d '{\"command\": \"echo hello\"}'\n</code></pre>\n<p>You can also checkpoint and rollback via the API, so you can get your environment exactly how you like it, checkpoint it, run a bunch of untrusted code, then roll back to the clean checkpoint when you're done.</p>\n<p>Managing network access is an important part of maintaining a good sandbox. The Sprites API lets you <a href=\"https://sprites.dev/api/sprites/policies\">configure network access policies</a> using a DNS-based allow/deny list like this:</p>\n<div class=\"highlight highlight-source-shell\"><pre>curl -X POST \\\n  <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>https://api.sprites.dev/v1/sprites/{name}/policy/network<span class=\"pl-pds\">\"</span></span> \\\n  -H <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Authorization: Bearer <span class=\"pl-smi\">$SPRITES_TOKEN</span><span class=\"pl-pds\">\"</span></span> \\\n  -H <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Content-Type: application/json<span class=\"pl-pds\">\"</span></span> \\\n  -d <span class=\"pl-s\"><span class=\"pl-pds\">'</span>{</span>\n<span class=\"pl-s\">    \"rules\": [</span>\n<span class=\"pl-s\">      {</span>\n<span class=\"pl-s\">        \"action\": \"allow\",</span>\n<span class=\"pl-s\">        \"domain\": \"github.com\"</span>\n<span class=\"pl-s\">      },</span>\n<span class=\"pl-s\">      {</span>\n<span class=\"pl-s\">        \"action\": \"allow\",</span>\n<span class=\"pl-s\">        \"domain\": \"*.npmjs.org\"</span>\n<span class=\"pl-s\">      }</span>\n<span class=\"pl-s\">    ]</span>\n<span class=\"pl-s\">  }<span class=\"pl-pds\">'</span></span></pre></div>\n\n<h4 id=\"scale-to-zero-billing\">Scale-to-zero billing</h4>\n<p>Sprites have scale-to-zero baked into the architecture. They go to sleep after 30 seconds of inactivity, wake up quickly when needed and bill you for just the CPU hours, RAM hours and GB-hours of storage you use while the Sprite is awake.</p>\n<p>Fly <a href=\"https://sprites.dev/#billing\">estimate</a> a 4 hour intensive coding session as costing around 46 cents, and a low traffic web app with 30 hours of wake time per month at ~$4.</p>\n<p>(I calculate that a web app that consumes all 8 CPUs and all 8GBs of RAM 24/7 for a month would cost ((7 cents * 8 * 24 * 30) + (4.375 cents * 8 * 24 * 30)) / 100  = $655.2 per month, so don't necessarily use these as your primary web hosting solution for an app that soaks up all available CPU and RAM!)</p>\n<h4 id=\"two-of-my-favorite-problems-at-once\">Two of my favorite problems at once</h4>\n<p>I was hopeful that Fly would enter the developer-friendly sandbox API market, especially given other entrants from companies like <a href=\"https://sandbox.cloudflare.com/\">Cloudflare</a> and <a href=\"https://modal.com/docs/guide/sandboxes\">Modal</a> and <a href=\"https://e2b.dev/\">E2B</a>.</p>\n<p>I did not expect that they'd tackle the developer sandbox problem at the same time, and with the same product!</p>\n<p>My one concern here is that it makes the product itself a little harder to explain.</p>\n<p>I'm already spinning up some prototypes of sandbox-adjacent things I've always wanted to build, and early signs are very promising. I'll write more about these as they turn into useful projects.</p>\n\n\n<p><strong>Update</strong>: Here's some <a href=\"https://news.ycombinator.com/item?id=46557825#46560748\">additional colour</a> from Thomas Ptacek on Hacker News:</p>\n<blockquote>\n<p>This has been in the works for quite awhile here. We put a long bet on \"slow create fast start/stop\" --- which is a really interesting and useful shape for execution environments --- but it didn't make sense to sandboxers, so \"fast create\" has been the White Whale at Fly.io for over a year.</p>\n</blockquote>\n    \n        <p>Tags: <a href=\"https://simonwillison.net/tags/sandboxing\">sandboxing</a>, <a href=\"https://simonwillison.net/tags/thomas-ptacek\">thomas-ptacek</a>, <a href=\"https://simonwillison.net/tags/ai\">ai</a>, <a href=\"https://simonwillison.net/tags/fly\">fly</a>, <a href=\"https://simonwillison.net/tags/coding-agents\">coding-agents</a></p>", "source_url": "https://simonwillison.net/atom/everything/", "article_url": "https://simonwillison.net/2026/Jan/9/sprites-dev/#atom-everything", "pub_date": "2026-01-09T23:57:12+00:00", "label": false, "category": "other", "notes": "heuristic label"}
{"id": "6eebe20f0bf681e1", "title": "Introducing Google Antigravity", "description": "", "source_url": "https://deepmind.google/blog/rss.xml", "article_url": "https://deepmind.google/blog/introducing-google-antigravity/", "pub_date": "2025-11-18T16:06:32+00:00", "label": true, "category": "other", "notes": "heuristic label"}
{"id": "4d96df941bcad961", "title": "FACTS Benchmark Suite: Systematically evaluating the factuality of large language models", "description": "Systematically evaluating the factuality of large language models with the FACTS Benchmark Suite.", "source_url": "https://deepmind.google/blog/rss.xml", "article_url": "https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/", "pub_date": "2025-12-09T11:29:03+00:00", "label": true, "category": "ai_ml", "notes": "heuristic label"}
{"id": "6cb83b491d15d2ed", "title": "Introducing OpenAI for Ireland", "description": "OpenAI launches OpenAI for Ireland, partnering with the Irish Government, Dogpatch Labs and Patch to help SMEs, founders and young builders use AI to innovate, boost productivity and build the next generation of Irish tech startups.", "source_url": "https://openai.com/blog/rss.xml", "article_url": "https://openai.com/index/openai-for-ireland", "pub_date": "2025-11-14T04:00:00+00:00", "label": true, "category": "ai_ml", "notes": "heuristic label"}
{"id": "6abafcd0978f609c", "title": "Build with Nano Banana Pro, our Gemini 3 Pro Image model", "description": "", "source_url": "https://deepmind.google/blog/rss.xml", "article_url": "https://deepmind.google/blog/build-with-nano-banana-pro-our-gemini-3-pro-image-model/", "pub_date": "2025-11-20T15:11:14+00:00", "label": true, "category": "policy", "notes": "heuristic label"}
{"id": "698189c4d7d2916b", "title": "Differential Transformer V2", "description": "", "source_url": "https://huggingface.co/blog/feed.xml", "article_url": "https://huggingface.co/blog/microsoft/diff-attn-v2", "pub_date": "2026-01-20T03:20:57+00:00", "label": true, "category": "ai_ml", "notes": "heuristic label"}
{"id": "07c6b2cfd9a76cc4", "title": "AssetOpsBench: Bridging the Gap Between AI Agent Benchmarks and Industrial Reality", "description": "", "source_url": "https://huggingface.co/blog/feed.xml", "article_url": "https://huggingface.co/blog/ibm-research/assetopsbench-playground-on-hugging-face", "pub_date": "2026-01-21T06:25:31+00:00", "label": true, "category": "research", "notes": "manual: New benchmark/research release."}
{"id": "b27675a16945b139", "title": "One in a million: celebrating the customers shaping AI‚Äôs future", "description": "More than one million customers around the world now use OpenAI to empower their teams and unlock new opportunities. This post highlights how companies like PayPal, Virgin Atlantic, BBVA, Cisco, Moderna, and Canva are transforming the way work gets done with AI.", "source_url": "https://openai.com/blog/rss.xml", "article_url": "https://openai.com/index/one-in-a-million-customers", "pub_date": "2025-12-22T00:00:00+00:00", "label": true, "category": "ai_ml", "notes": "heuristic label"}
{"id": "48a50409435d3888", "title": "Superhuman AI Exfiltrates Emails", "description": "<p><strong><a href=\"https://www.promptarmor.com/resources/superhuman-ai-exfiltrates-emails\">Superhuman AI Exfiltrates Emails</a></strong></p>\nClassic prompt injection attack:</p>\n<blockquote>\n<p>When asked to summarize the user‚Äôs recent mail, a prompt injection in an untrusted email manipulated Superhuman AI to submit content from dozens of other sensitive emails (including financial, legal, and medical information) in the user‚Äôs inbox to an attacker‚Äôs Google Form.</p>\n</blockquote>\n<p>To Superhuman's credit they treated this as the high priority incident it is and issued a fix.</p>\n<p>The root cause was a CSP rule that allowed markdown images to be loaded from <code>docs.google.com</code> - it turns out Google Forms on that domain will persist data fed to them via a GET request!\n\n    <p><small></small>Via <a href=\"https://news.ycombinator.com/item?id=46592424\">Hacker News</a></small></p>\n\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/security\">security</a>, <a href=\"https://simonwillison.net/tags/ai\">ai</a>, <a href=\"https://simonwillison.net/tags/prompt-injection\">prompt-injection</a>, <a href=\"https://simonwillison.net/tags/generative-ai\">generative-ai</a>, <a href=\"https://simonwillison.net/tags/llms\">llms</a>, <a href=\"https://simonwillison.net/tags/exfiltration-attacks\">exfiltration-attacks</a>, <a href=\"https://simonwillison.net/tags/content-security-policy\">content-security-policy</a></p>", "source_url": "https://simonwillison.net/atom/everything/", "article_url": "https://simonwillison.net/2026/Jan/12/superhuman-ai-exfiltrates-emails/#atom-everything", "pub_date": "2026-01-12T22:24:54+00:00", "label": true, "category": "ai_ml", "notes": "heuristic label"}
{"id": "ee6820e623e7ce75", "title": "Strengthening cyber resilience as AI capabilities advance", "description": "OpenAI is investing in stronger safeguards and defensive capabilities as AI models become more powerful in cybersecurity. We explain how we assess risk, limit misuse, and work with the security community to strengthen cyber resilience.", "source_url": "https://openai.com/blog/rss.xml", "article_url": "https://openai.com/index/strengthening-cyber-resilience", "pub_date": "2025-12-10T12:00:00+00:00", "label": true, "category": "ai_ml", "notes": "heuristic label"}
{"id": "5384184514e6bac2", "title": "Introducing IndQA", "description": "OpenAI introduces IndQA, a new benchmark for evaluating AI systems in Indian languages. Built with domain experts, IndQA tests cultural understanding and reasoning across 12 languages and 10 knowledge areas.", "source_url": "https://openai.com/blog/rss.xml", "article_url": "https://openai.com/index/introducing-indqa", "pub_date": "2025-11-03T22:30:00+00:00", "label": true, "category": "other", "notes": "heuristic label"}
{"id": "f2a5023d3a887fc3", "title": "Deepening our partnership with the UK AI Security Institute", "description": "Google DeepMind and UK AI Security Institute (AISI) strengthen collaboration on critical AI safety and security research", "source_url": "https://deepmind.google/blog/rss.xml", "article_url": "https://deepmind.google/blog/deepening-our-partnership-with-the-uk-ai-security-institute/", "pub_date": "2025-12-11T00:06:40+00:00", "label": true, "category": "security", "notes": "heuristic label"}
{"id": "053cc7da2e432d15", "title": "Evaluating chain-of-thought monitorability", "description": "OpenAI introduces a new framework and evaluation suite for chain-of-thought monitorability, covering 13 evaluations across 24 environments. Our findings show that monitoring a model‚Äôs internal reasoning is far more effective than monitoring outputs alone, offering a promising path toward scalable control as AI systems grow more capable.", "source_url": "https://openai.com/blog/rss.xml", "article_url": "https://openai.com/index/evaluating-chain-of-thought-monitorability", "pub_date": "2025-12-18T12:00:00+00:00", "label": true, "category": "ai_ml", "notes": "heuristic label"}
{"id": "9e260bc7581d610d", "title": "Electricity use of AI coding agents", "description": "<p><strong><a href=\"https://www.simonpcouch.com/blog/2026-01-20-cc-impact/\">Electricity use of AI coding agents</a></strong></p>\nPrevious work estimating the energy and water cost of LLMs has generally focused on the cost per prompt using a consumer-level system such as ChatGPT.</p>\n<p>Simon P. Couch notes that coding agents such as Claude Code use <em>way</em> more tokens in response to tasks, often burning through many thousands of tokens of many tool calls.</p>\n<p>As a heavy Claude Code user, Simon estimates his own usage at the equivalent of 4,400 \"typical queries\" to an LLM, for an equivalent of around $15-$20 in daily API token spend. He figures that to be about the same as running a dishwasher once or the daily energy used by a domestic refrigerator.\n\n    <p><small></small>Via <a href=\"https://news.ycombinator.com/item?id=46695415\">Hacker News</a></small></p>\n\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/ai\">ai</a>, <a href=\"https://simonwillison.net/tags/generative-ai\">generative-ai</a>, <a href=\"https://simonwillison.net/tags/llms\">llms</a>, <a href=\"https://simonwillison.net/tags/ai-ethics\">ai-ethics</a>, <a href=\"https://simonwillison.net/tags/ai-energy-usage\">ai-energy-usage</a>, <a href=\"https://simonwillison.net/tags/coding-agents\">coding-agents</a>, <a href=\"https://simonwillison.net/tags/claude-code\">claude-code</a></p>", "source_url": "https://simonwillison.net/atom/everything/", "article_url": "https://simonwillison.net/2026/Jan/20/electricity-use-of-ai-coding-agents/#atom-everything", "pub_date": "2026-01-20T23:11:57+00:00", "label": false, "category": "other", "notes": "manual: Exploratory discussion; not a major event."}
{"id": "f2f94d6453c129aa", "title": "Introducing the Teen Safety Blueprint", "description": "Discover OpenAI‚Äôs Teen Safety Blueprint‚Äîa roadmap for building AI responsibly with safeguards, age-appropriate design, and collaboration to protect and empower young people online.", "source_url": "https://openai.com/blog/rss.xml", "article_url": "https://openai.com/index/introducing-the-teen-safety-blueprint", "pub_date": "2025-11-06T00:00:00+00:00", "label": true, "category": "other", "notes": "heuristic label"}
{"id": "246310017310241f", "title": "The Pulse #157: Internal dev tooling at Meta & the ‚Äútrajectories‚Äù feature", "description": "Also: GitHub upsets devs by charging for self-hosted CI/CD, Warsaw could become the EU&#8217;s new &#8220;tech capital&#8221;, hiring juniors is profitable now, and more", "source_url": "https://newsletter.pragmaticengineer.com/feed", "article_url": "https://newsletter.pragmaticengineer.com/p/the-pulse-157-internal-dev-tooling", "pub_date": "2025-12-18T17:28:12+00:00", "label": false, "category": "other", "notes": "heuristic label"}
{"id": "8aac2913d7951fc9", "title": "Supercharging the ML and AI Development Experience at Netflix", "description": "<h3>Supercharging the ML and AI Development Experience at Netflix with¬†Metaflow</h3><p><a href=\"https://www.linkedin.com/in/shashanksrikanth/\"><em>Shashank Srikanth</em></a>, <a href=\"https://www.linkedin.com/in/romain-cledat-4a211a5/\"><em>Romain¬†Cledat</em></a></p><p><a href=\"https://docs.metaflow.org\">Metaflow</a>‚Ää‚Äî‚Ääa framework we started and <a href=\"https://netflixtechblog.com/open-sourcing-metaflow-a-human-centric-framework-for-data-science-fa72e04a5d9\">open-sourced</a> in 2019‚Ää‚Äî‚Äänow powers <a href=\"https://netflixtechblog.com/supporting-diverse-ml-systems-at-netflix-2d2e6b6d205d\">a wide range of ML and AI systems across Netflix</a> and at <a href=\"https://github.com/Netflix/metaflow/blob/master/ADOPTERS.md\">many other companies</a>. It is well loved by users for helping them take their ML/AI workflows from <a href=\"https://docs.metaflow.org/introduction/what-is-metaflow#how-does-metaflow-support-prototyping-and-production-use-cases\">prototype to production</a>, allowing them to focus on building cutting-edge systems that bring joy and entertainment to audiences worldwide.</p><p>Metaflow allows users¬†to:</p><ol><li><strong>Iterate and ship quickly </strong>by minimizing friction</li><li><strong>Operate systems reliably</strong> in production with minimal overhead, at Netflix¬†scale.</li></ol><p>Metaflow works with many battle-hardened tooling to address the second point‚Ää‚Äî‚Ääamong them <a href=\"https://netflixtechblog.com/100x-faster-how-we-supercharged-netflix-maestros-workflow-engine-028e9637f041\">Maestro</a>, our newly open-sourced workflow orchestrator that powers nearly every ML and AI system at Netflix and serves as a backbone for Metaflow¬†itself.</p><p>In this post, we focus on the first point and introduce a new Metaflow functionality, <strong>Spin</strong>, that helps users <strong>accelerate their iterative development process</strong>. By the end, you‚Äôll have a solid understanding of Spin‚Äôs capabilities and learn how to try it out yourself with <strong>Metaflow¬†2.19</strong>.</p><h3>Iterative development in ML and AI workflows</h3><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*0I8DAvXCQEN1RpTTC0ZyaQ.jpeg\" /><figcaption>Developing a Metaflow flow with cards in¬†VSCode</figcaption></figure><p>To understand our approach to improving the ML and AI development experience, it helps to consider how these workflows differ from traditional software engineering.</p><p>ML and AI development revolves not just around code but also around data and models, which are large, mutable, and computationally expensive to process. Iteration cycles can involve long-running data transformations, model training, and stochastic processes that yield slightly different results from run to run. These characteristics make fast, stateful iteration a critical part of productive development.</p><p>This is where notebooks‚Ää‚Äî‚Ääsuch as Jupyter, <a href=\"https://observablehq.com/documentation/notebooks/\">Observable</a>, or <a href=\"https://marimo.io/\">Marimo</a>‚Ää‚Äî‚Ääshine. Their ability to preserve state in memory allows developers to load a dataset once and iteratively explore, transform, and visualize it without reloading or recomputing from scratch. This persistent, interactive environment turns what would otherwise be a slow, rigid loop into a fluid, exploratory workflow‚Ää‚Äî‚Ääperfectly suited to the needs of ML and AI practitioners.</p><p>Because ML and AI development is computationally intensive, stochastic, and data- and model-centric, tools that optimize iteration speed must treat state management as a first-class design concern. Any system aiming to improve the development experience in this domain must therefore enable quick, incremental experimentation without losing continuity between iterations.</p><h3>New: rapid, iterative development with¬†spin</h3><p>At first glance, Metaflow code looks like a workflow‚Ää‚Äî‚Ääsimilar to <a href=\"https://airflow.apache.org/\">Airflow</a>‚Ää‚Äî‚Ääbut there‚Äôs another way to look at it: each Metaflow @step serves as <a href=\"https://docs.metaflow.org/metaflow/basics#what-should-be-a-step\">a checkpoint boundary</a>. At the end of every step, Metaflow automatically persists all instance variables as <em>artifacts</em>, allowing the execution to <a href=\"https://docs.metaflow.org/metaflow/debugging#how-to-use-the-resume-command\">resume</a> seamlessly from that point onward. The below animation shows this behavior in¬†action:</p><figure><img alt=\"An animated GIF showing how resume can be used in Metaflow. The GIF shows how using `flow.py resume join` makes Metaflow clone previously executed steps and resumes the computation from the `join` step and continues executing till the end of the flow.\" src=\"https://cdn-images-1.medium.com/max/1024/1*AEDpnt-YULYV4mcyrwLk7g.gif\" /><figcaption>Using resume in¬†Metaflow</figcaption></figure><p>In a sense, we can consider a @step similar to a notebook cell: it is the smallest unit of execution that updates state upon completion. It does have a few differences that address the issues with notebook¬†cells:</p><ul><li><strong>The execution order is explicit and deterministic: </strong>no surprises due to out-of-order cell execution;</li><li><strong>The state is not hidden: </strong>state is explicitly stored as self. variables as shared state, which can be <a href=\"https://docs.metaflow.org/metaflow/client\">discovered and inspected</a>;</li><li><strong>The state is versioned and persisted</strong> making results more reproducible.</li></ul><p>While <strong>Metaflow</strong>‚Äôs resume feature can approximate the incremental and iterative development approach of notebooks, it restarts execution from the selected step onward, introducing more latency between iterations. In contrast, a <strong>notebook</strong> allows near-instant feedback by letting users tweak and rerun individual cells while seamlessly reusing data from earlier cells held in¬†memory.</p><p>The new spin command in Metaflow 2.19 addresses this gap. Similar to executing a single notebook cell, it quickly executes a single Metaflow @step‚Ää‚Äî‚Ääwith all the state carried over from the parent step. As a result, users can develop and debug Metaflow steps as easily as a cell in a notebook.</p><p>The effect becomes clear when considering the three complementary execution modes‚Ää‚Äî‚Äärun, resume, and spin‚Ää‚Äî‚Ääside by side, mapping them to the corresponding notebook behavior:</p><figure><img alt=\"Diagram showing the various modes of execution in Metaflow: Run, Resume and Spin\" src=\"https://cdn-images-1.medium.com/max/1024/1*DgRIxOu-7keiFoHia9JMrg.png\" /><figcaption>Run, Resume and Spin¬†‚Äúmodes‚Äù</figcaption></figure><p>Another major difference isn‚Äôt just what gets executed, but what gets recorded. Both run and resume create a full, versioned run with complete metadata and artifacts, while spin skips tracking altogether. It‚Äôs built for fast, throw-away iterations during development.</p><p>The one-minute clip below illustrates a typical iterative development workflow that alternates between run and spin. In this example, we are building a flow that reads a dataset from a Parquet file and trains a separate model for each product category, focusing on computer-related categories.</p><a href=\"https://medium.com/media/36b60cb7c79bf5c77ddd1f60cc97ae8b/href\">https://medium.com/media/36b60cb7c79bf5c77ddd1f60cc97ae8b/href</a><p>As shown in the video, we start by creating a flow from scratch and running a minimal version of it to persist test artifacts‚Ää‚Äî‚Ääin this case, a Parquet dataset. From there, we can use spin to iterate on one step at a time, incrementally building out the flow, for example, by adding the parallel training steps demonstrated in the¬†clip.</p><p>Once the flow has been iterated on locally, it can be seamlessly deployed to production orchestrators like Maestro or <a href=\"https://docs.metaflow.org/production/scheduling-metaflow-flows/scheduling-with-argo-workflows\">Argo</a>, and <a href=\"https://docs.metaflow.org/scaling/remote-tasks/requesting-resources\">scaled up</a> on compute platforms such as AWS Batch, Titus, Kubernetes and more. Thus, the experience is as smooth as developing in a notebook, but the outcome is a production-ready, scalable workflow, implemented as an idiomatic Python¬†project!</p><h3>Spin up smooth development in VSCode/Cursor</h3><p>Instead of typing run and spin manually in the terminal, we can bind them to keyboard shortcuts. For example, <a href=\"https://github.com/outerbounds/metaflow-dev-vscode\">the simple metaflow-dev VS Code extension</a> (works with Cursor as well) maps Ctrl+Opt+R to run and Ctrl+Opt+S to spin. Just hack away, hit Ctrl+Opt+S, and the extension will save your file and spin the step you are currently editing.</p><p>One area where spin truly shines is in creating mini-dashboards and reports with <a href=\"https://docs.metaflow.org/metaflow/visualizing-results\">Metaflow Cards</a>. Visualization is another strong point of notebooks but the combination of spin and cards makes Metaflow a very compelling alternative for developing real-time and post-execution visualizations. Developing cards is inherently iterative and visual (much like building web pages) where you want to tweak code and see the results instantly. This workflow is readily available with the combination of VSCode/Cursor, which includes a built-in web-view, <a href=\"https://docs.metaflow.org/metaflow/visualizing-results/effortless-task-inspection-with-default-cards#using-local-card-viewer\">the local card viewer</a>, and¬†spin.</p><p>To see the trio of tools‚Ää‚Äî‚Ääalong with the VS Code extension‚Ää‚Äî‚Ääin action, in this short clip we add observability to the train step that we built in the earlier¬†example:</p><a href=\"https://medium.com/media/e2596ec88a7e65b4bad1b6ba7a2a167c/href\">https://medium.com/media/e2596ec88a7e65b4bad1b6ba7a2a167c/href</a><p>A major benefit of Metaflow Cards is that we don‚Äôt need to deploy any extra services, data streams, and databases for observability. Just develop visual outputs as above, deploy the flow, and wehave a complete system in production with reporting and visualizations included.</p><h3>Spin to the next level: injecting inputs, inspecting outputs</h3><p>Spin does more than just run code‚Ää‚Äî‚Ääit also lets us take full control of a spun @step‚Äôs inputs and outputs, enabling a range of advanced patterns.</p><p>In contrast to notebooks, we can spin any arbitrary @step in a flow using state from any past run, making it easy to test functions with different inputs. For example, if we have multiple models produced by separate runs, we could spin an inference step, supplying a different model run each¬†time.</p><p>We can also override artifact values or inject arbitrary Python objects‚Ää‚Äî‚Ääsimilar to a notebook cell‚Ää‚Äî‚Ääfor spin. Simply specify a Python module with an ARTIFACTS dictionary:</p><pre>ARTIFACTS = {<br />  &quot;model&quot;: &quot;kmeans&quot;,<br />  &quot;k&quot;: 15<br />}</pre><p>and point spin at the¬†module:</p><pre>spin train --artifacts-module artifacts.py</pre><p>By default spin doesn‚Äôt persist artifacts, but we can easily change this by adding --persist. Even in this case, artifacts are not persisted in the usual Metaflow datastore but to a directory-specific location which you can easily clean up after testing. We can access the results with <a href=\"https://docs.metaflow.org/metaflow/client\">the Client API</a> as usual‚Ää‚Äî‚Ääjust specify the directory you want to inspect with inspect_spin:</p><pre>from metaflow import inspect_spin<br /><br />inspect_spin(&quot;.&quot;)<br />Flow(&quot;TrainingFlow&quot;).latest_run[&quot;train&quot;].task[&quot;model&quot;].data</pre><p>Being able to inspect and modify a step‚Äôs inputs and outputs on the fly unlocks a powerful use case:<strong> unit testing individual steps</strong>. We can use spin programmatically through <a href=\"https://docs.metaflow.org/metaflow/managing-flows/runner\">the Runner API</a> and assert the¬†results:</p><pre>from metaflow import Runner<br /><br />with Runner(&quot;flow.py&quot;).spin(&quot;train&quot;, persist=True) as spin:<br />  assert spin.task[&quot;model&quot;].data == &quot;kmeans&quot;</pre><h3>Making AI agents¬†spin</h3><p>In addition to speeding up development for humans, spin turns out to be surprisingly handy for coding agents too. There are two major advantages to teaching AI how to¬†spin:</p><ol><li><strong>It accelerates the development loop</strong>. Agents don‚Äôt naturally understand what‚Äôs slow, or why speed matters, so they need to be nudged to favor faster tools over slower¬†ones.</li><li><strong>It helps surface errors faster </strong>and contextualizes them to a specific piece of code, increasing the chance that the agent is able to fix errors by¬†itself.</li></ol><p>Metaflow users are already <a href=\"https://claude.com/product/claude-code\">using</a> Claude Code; spin makes this even easier. In the example below, we added the following section in a CLAUDE.md file:</p><pre>## Developing Metaflow code<br />Follow this incremental development workflow that ensures quick iterations<br />and correct results. You must create a flow incrementally, step by step<br />following this process:<br />1. Create a flow skeleton with empty `@step`s.<br />2. Add a data loading step.<br />3. `run` the flow.<br />4. Populate the next step and use `spin` to test it with the correct inputs.<br />5. `run` the flow to record outputs from the new step.<br />5. Iterate on (4‚Äì5) until all steps have been implemented and work correctly.<br />6. `run` the whole flow to ensure final correctness.<br /><br />To test a flow, run the flow as follows<br />```<br />python flow.py - environment=pypi run<br />```<br /><br />Do this once before running `spin`.<br />As you are building the flow, you `spin` to test steps quickly.<br />For instance<br />```<br />python flow.py - environment=pypi spin train<br />```</pre><p>Just based on these quick instructions, the agent is able to use spin effectively. Take a look at the following inspirational example that one-shots Claude to create a flow, along the lines of our earlier examples, which trains a classifier to predict product categories:</p><a href=\"https://medium.com/media/706e83950e73a05ca41b4fb463702690/href\">https://medium.com/media/706e83950e73a05ca41b4fb463702690/href</a><p>In the video, we can see Claude using spin around the 45-second mark to test a preprocess step. The step initially fails due to a classic data science pitfall: during testing, Claude samples only a small subset of data, causing some classes to be underrepresented. The first spin surfaces the issue, which Claude then fixes by switching to stratified sampling‚Ää‚Äî‚Ääand finally does another spin to confirm the fix, before proceeding to complete the¬†task.</p><h3>The inner loop of end-to-end ML/AI</h3><p>To circle back to where we started, our motivation for adding spin‚Ää‚Äî‚Ääand for creating Metaflow in the first place‚Ää‚Äî‚Ääis to accelerate development cycles so we can deliver more joy to our subscribers, faster. Ultimately, we believe there‚Äôs no single magic feature that makes this possible. It takes all parts of an ML/AI platform working together coherently‚Ää‚Äî‚Ääspin included.</p><p>From this perspective, it‚Äôs useful to place spin in the context of other Metaflow features. It‚Äôs designed for the innermost loop of model and business-logic development, with the added benefit of supporting unit testing during deployment, as shown in the overall blueprint of the Metaflow toolchain below.</p><figure><img alt=\"Metaflow tool-chain.\" src=\"https://cdn-images-1.medium.com/max/1024/1*9cd6SHFrW7A4iWMZHYkVkw.png\" /><figcaption>Metaflow tool-chain</figcaption></figure><p>In this diagram, the solid blue boxes represent different Metaflow commands, while the blue text denotes decorators and other features. In particular, note the <em>Shared Functionality</em> box‚Ää‚Äî‚Ääanother key focus area for us over the past year‚Ää‚Äî‚Ääwhich includes <a href=\"https://netflixtechblog.com/introducing-configurable-metaflow-d2fb8e9ba1c6\">configuration management</a> and <a href=\"https://docs.metaflow.org/metaflow/composing-flows/introduction\">custom decorators</a>. These capabilities let domain-specific teams and platform providers tailor Metaflow to their own use cases. Following our ethos of composability, all of these features integrate seamlessly with spin as¬†well.</p><p>Another key design philosophy of Metaflow is to let projects start small and simple, adding complexity only when it becomes necessary. So don‚Äôt be overwhelmed by the diagram above. To get started, install Metaflow easily¬†with</p><pre>pip install metaflow</pre><p>and take your first baby @steps for a spin! Check out the <a href=\"https://docs.metaflow.org/metaflow/authoring-flows/introduction\">docs</a> and for questions, support, and feedback, join the friendly <a href=\"http://chat.metaflow.org\">Metaflow Community Slack</a>.</p><h3>Acknowledgments</h3><p>We would like to thank our partners at <a href=\"https://outerbounds.com\">Outerbounds</a>, and particularly <a href=\"https://www.linkedin.com/in/villetuulos/\">Ville Tuulos</a>, <a href=\"https://www.linkedin.com/in/savingoyal/\">Savin Goyal</a>, and <a href=\"https://www.linkedin.com/in/madhur-tandon/\">Madhur Tandon</a>, for their collaboration on this feature, from initial ideation to review, testing and documentation. We would also like to acknowledge the rest of the Model Development and Management team (<a href=\"https://www.linkedin.com/in/maria-alder/\">Maria Alder</a>, <a href=\"https://www.linkedin.com/in/david-j-berg/\">David J. Berg</a>, <a href=\"https://www.linkedin.com/in/shaojingli/\">Shaojing Li</a>, <a href=\"https://www.linkedin.com/in/rui-lin-483a83111/\">Rui Lin</a>, <a href=\"https://www.linkedin.com/in/nissanpow/\">Nissan Pow</a>, <a href=\"https://www.linkedin.com/in/chaoying-wang/\">Chaoying Wang</a>, <a href=\"https://www.linkedin.com/in/reginalw/\">Regina Wang</a>, <a href=\"https://www.linkedin.com/in/shuishiyang/\">Seth Yang</a>, <a href=\"https://www.linkedin.com/in/zitingyu/\">Darin Yu</a>) for their input and comments.</p><img alt=\"\" height=\"1\" src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=b2d5b95c63eb\" width=\"1\" /><hr /><p><a href=\"https://netflixtechblog.com/supercharging-the-ml-and-ai-development-experience-at-netflix-b2d5b95c63eb\">Supercharging the ML and AI Development Experience at Netflix</a> was originally published in <a href=\"https://netflixtechblog.com\">Netflix TechBlog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>", "source_url": "https://netflixtechblog.com/feed", "article_url": "https://netflixtechblog.com/supercharging-the-ml-and-ai-development-experience-at-netflix-b2d5b95c63eb?source=rss----2615bd06b42e---4", "pub_date": "2025-11-04T20:33:44+00:00", "label": true, "category": "ai_ml", "notes": "heuristic label"}
{"id": "d7073e20528dd307", "title": "OpenAI appoints Denise Dresser as Chief Revenue Officer", "description": "Denise Dresser is joining as Chief Revenue Officer, overseeing OpenAI‚Äôs global revenue strategy across enterprise and customer success. She will help more businesses put AI to work in their day-to-day operations as OpenAI continues to scale.", "source_url": "https://openai.com/blog/rss.xml", "article_url": "https://openai.com/index/openai-appoints-denise-dresser", "pub_date": "2025-12-09T00:00:00+00:00", "label": true, "category": "business", "notes": "heuristic label"}
{"id": "1fa96df2fb571873", "title": "Building The Intent Engine: How Instacart is Revamping Query Understanding with LLMs", "description": "<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*i-xL0bOg4Oy0EBh9nhAULg.png\" /></figure><p>Authors: Yuanzheng Zhu, Guanghua Shu, Raochuan Fan, Vinesh Gudla, Tejaswi¬†Tenneti</p><h3>Introduction</h3><p>When people search for items on Instacart, they don‚Äôt always type perfectly worded phrases. They might write <em>‚Äúbread no gluten‚Äù</em> or <em>‚Äúx large zip lock‚Äù</em>‚Ää‚Äî‚Ääand that‚Äôs okay. Our job is to understand what they mean, not just what they type. This process, called Query Understanding (QU), is the intent engine that helps millions of customers find what they need on Instacart. Getting QU right is essential.</p><p>For years, we relied on traditional machine learning models. They worked well for many searches, but we wanted to deliver a truly intelligent experience for the endless variety of uncommon, highly-specific, or creatively phrased queries‚Ää‚Äî‚Ääwhat we call <em>long-tail searches</em>.</p><p>This pursuit led us to a new paradigm. Instead of building another bespoke model from the ground up, we opted to ‚Äústand on the shoulder of giants.‚Äù We turned to Large Language Models (LLMs) for their vast pre-trained knowledge. We saw the opportunity not just to <strong>use</strong> these models, but to <strong>steer</strong> them into becoming deep domain experts for our vertical. This post details that journey. Our strategy was layered, moving from <strong>context-engineering</strong> with <strong>guardrails</strong> to our ultimate goal: <strong>fine-tuning</strong> to distill proprietary knowledge directly into a LLM. This approach transforms a generalist model into a true specialist. It has shifted our core challenge from feature engineering to productionizing these powerful backbones while managing latency and¬†cost.</p><h3>Challenges in Traditional Query Understanding</h3><p>Our journey to LLMs began with examining where traditional QU falls short. While essential for search at Instacart, accurately interpreting user intent is notoriously difficult for several¬†reasons:</p><ul><li><strong>Broad Queries:</strong> Queries like <em>‚Äúhealthy food‚Äù</em> or <em>‚Äúfrozen snacks‚Äù</em> are common but difficult to act on. Their lack of specificity makes it challenging to narrow down relevant results, as they can span dozens of categories.</li><li><strong>Lack of Labeled Data:</strong> QU operates upstream and doesn‚Äôt benefit from direct feedback like clicks or conversions. The pseudo-labels we derive from user behaviors are inherently noisy‚Ää‚Äî‚Ääa user might search for<em> ‚Äúbread‚Äù</em> but ultimately purchase bananas. Generating clean labels requires costly and time-consuming human evaluation.</li><li><strong>Tail Queries:</strong> Highly specific or rare searches like<em> ‚Äúred hot chili pepper spice‚Äù</em> or<em> ‚Äú2% reduced-fat ultra-pasteurized chocolate milk‚Äù</em> suffer from data sparsity. Models trained on engagement data struggle due to limited historical clicks or conversions, leading to poor generalization.</li><li><strong>System Complexity:</strong> To solve these problems, we historically trained and maintained multiple independent models for individual QU tasks. For instance, query classification and query rewrites were handled by entirely separate systems, each with its own logic (Figure 1). Each of these bespoke solutions demanded its own data pipeline, training and serving architecture. This heterogeneity introduced inconsistencies, slowed down development cycles, and made the overall QU system difficult to scale and¬†evolve.</li></ul><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*-C3_1SdXKNF_cMe2E0IsVA.png\" /><figcaption><em>Fig 1. Our previous QU involved multiple independent models for individual QU tasks. For instance, query classification relied on a FastText model for multi-label classification, while query rewrites were generated by a separate system that mined user session behavior.</em></figcaption></figure><h3>The Advantages of¬†LLMs</h3><p>To solve these problems, we turned to LLMs to consolidate and enhance our QU models. They offer several key advantages that improve the accuracy and efficiency of Instacart Search:</p><ul><li><strong>World Knowledge and Inference Capabilities:</strong> Trained on diverse textual data, LLMs possess world knowledge that enables them to make logical inferences from user queries. For example, an LLM already understands that <em>‚ÄúItalian parsley‚Äù</em> is a synonym for <em>‚Äúflat parsley‚Äù</em>, while <em>‚Äúcurly parsley‚Äù</em> is a common substitute. This capability dramatically reduces the manual engineering and specialized data required by conventional models, giving us a powerful head¬†start.</li><li><strong>Simplified System:</strong> Because LLMs possess broad linguistic abilities, they enable us to consolidate numerous bespoke models. By replacing specialized models with a single LLM that can handle multiple NLP tasks, we eliminate the complexity of maintaining separate models and their inconsistencies.</li></ul><h3>LLM as QU: Our Strategy in¬†Action</h3><p>We integrated LLMs by adding Instacart‚Äôs domain context in three¬†ways:</p><ol><li><strong>Context-Engineering</strong>: Our primary method is Retrieval-Augmented Generation (RAG). We build data pipelines that retrieve and inject Instacart-specific context, such as conversion history and catalog data, directly into the prompt. This grounds the model in our business¬†reality.</li><li><strong>Post-Processing Guardrails</strong>: We refine LLM outputs through validation layers. These guardrails filter out hallucinations and enforce alignment with Instacart‚Äôs product taxonomy.</li><li><strong>Fine-Tuning for Deep Expertise</strong>: For the most advanced use cases, we fine-tune models on proprietary data. This embeds deep domain expertise directly into the model‚Äôs weights and represents a key part of our long-term strategy for handling complex, long-tail queries.</li></ol><p>The following examples illustrate how we leverage some of these techniques to transform critical QU components.</p><h3>1. Query Category Classification</h3><p>Instacart‚Äôs catalog is organized into a vast, hierarchical product taxonomy that structures billions of items, from broad departments like ‚ÄúMeat‚Äù down to specific sub-categories like ‚ÄúBeef Ribs &gt; Short Ribs‚Äù. Accurately classifying queries into our product taxonomy is essential. It directly powers recall and ranking, helping us retrieve items from the right categories and intelligently expand the search when a query is broad or ambiguous.</p><p>Our legacy approach treated this as a massive multi-class classification problem. For a given query, the model would predict the top-K most likely categories from a flat list. For example, for <em>‚Äúbutter milk‚Äù</em>, it might predict (‚ÄúDairy‚Äù, 0.95) and (‚ÄúMilk‚Äù, 0.92) as distinct, non-hierarchical outputs.</p><p>This legacy approach suffered from two primary pitfalls. First, being trained on noisy conversion data (e.g., a user searches <em>‚Äúbread‚Äù</em> but buys bananas) means it can produce irrelevant suggestions. Second, it lacked deeper contextual understanding, preventing it from using world knowledge to classify new or nuanced queries like <em>‚Äúvegan roast‚Äù</em> correctly, as shown in Table¬†1.</p><p>Our new LLM-powered approach greatly improves precision and recall through a three-step process: first, we retrieve the top-K converted categories for each query as initial candidates; second, we use an LLM to re-rank them with injected Instacart context; and finally, we apply a post-processing guardrail. This filter computes a semantic similarity score between the embeddings of the original query and the LLM‚Äôs predicted category path, discarding any pair that falls below our relevance threshold.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*6UFFj2Ki4CvFZTGlf1Kqyg.png\" /><figcaption><em>Table 1: Comparison of category classification between the legacy model and the new LLM-based approach.</em></figcaption></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*ZY5ZCQpXfi3gcD2nRrkVBw.png\" /><figcaption><em>Fig 2: Overview of the LLM for Query Category Classification system</em></figcaption></figure><h3>2. Query¬†Rewrites</h3><p>Query rewrites are critical for improving recall, especially when the original query does not return sufficient results. Our legacy system mined candidate rewrites from user session data, but this approach was limited, covering only 50% of search traffic and often failing to generate useful alternatives for product discovery.</p><p>To address this, we turned to LLMs. Our initial attempt involved a simple prompt asking a single model to generate rewrites for recall enhancement. This proved too ambiguous. For example, for <em>‚Äú1% milk‚Äù</em>, the model might return <em>‚Äúone percent milk‚Äù‚Ää</em>‚Äî‚Ääa valid synonym but not a useful rewrite for discovering alternative products.</p><p>This led us to design specialized prompts for three distinct rewrite types: <em>Substitutes</em>, <em>Broader queries</em>, and<em> Synonyms</em>. Each type is handled by a dedicated prompt with advanced prompt engineering‚Ää‚Äî‚Ääincorporating specific instructions, chain-of-thought (COT) reasoning, and few-shot examples. To ensure the results are logical and useful, we apply post-processing guardrails, including filters for semantic relevance. This structured approach increased our query rewrite coverage to over 95% with 90%+ precision across all three¬†types.</p><p>Building on this success, we are now adopting context engineering to make rewrites more convertible, personalized, and session-aware. We achieve this by injecting user engagement signals, such as the top-converting product categories from their subsequent searches in the same¬†session.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*fQCVmfoK_K6rnYxpiF08dA.png\" /><figcaption><em>Table 2: Examples of structured query rewrites generated by specialized LLMs</em></figcaption></figure><h3>3. Semantic Role Labeling¬†(SRL)</h3><p>Semantic Role Labeling (SRL) is the task of extracting structured concepts from a user query, such as <strong>product</strong>, <strong>brand</strong>, and <strong>attributes</strong>. These tags are critical for everything from search retrieval and ranking to ad targeting and¬†filters.</p><p>Our goal was to leverage the power of LLMs to generate high-quality tags. However, the power-law nature of search traffic presents a challenge: we can‚Äôt pre-compute results for every possible query because the ‚Äúlong-tail‚Äù of new and unique searches is effectively infinite, and offline LLM processing is expensive.</p><p>To solve this, we designed a <strong>hybrid system</strong>. A powerful offline process generates high-quality data that serves two purposes: populating a cache for our most common ‚Äúhead‚Äù queries and creating the training data for a fast, real-time model that handles the ‚Äúlong-tail.‚Äù The system‚Äôs flow, shown in the diagram below, is determined simply by a cache-hit.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*wJEg1YYYA01rJ4x55mMDVA.png\" /><figcaption><em>Fig 3. </em><strong><em>Architecture of the hybrid SRL system.</em></strong><em> Live traffic is routed based on a cache-hit. High-frequency ‚Äúhead‚Äù queries are served instantly with cache, while ‚Äútail‚Äù queries are handled by a real-time, fine-tuned model. The entire system is powered by an offline pipeline that generates data to both populate the cache and train the real-time model</em></figcaption></figure><h4>The Offline System (‚ÄúTeacher‚Äù): Generating High Quality Data at¬†Scale</h4><p>For our high-frequency ‚Äúhead‚Äù queries, we run an offline <strong>Retrieval-Augmented Generation (RAG)</strong> and <strong>caching</strong> pipeline. Because latency is not a concern here, we can use complex techniques to ensure the highest possible quality. The core of this is <strong>context-engineering</strong>: enriching the prompt with deep Instacart-specific knowledge.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*7r2UQapr0Ri40DIe4cBJfA.png\" /><figcaption><em>Fig 4. Overview of RAG pipeline for query tagging. Context-engineering injects Instacart domain knowledge to ground the LLM‚Äôs inference and generate far more accurate intent signals. (Note: Brand examples used for illustration are fictitious.)</em></figcaption></figure><p>Consider the query <em>‚Äúverdant machine‚Äù</em>. Without context, an LLM might assume it‚Äôs for machinery. Our offline pipeline, however, automatically enriches the prompt with crucial context from our internal data systems, including:</p><ul><li><strong>Historical Conversion Data:</strong> The top converted brand (<em>MuchPure</em>) and categories (<em>Smoothie Juices</em>).</li><li><strong>Product Catalog Information:</strong> Product brand names with high semantic similarity, ranked by embedding scores.</li></ul><p>Armed with this context, the model correctly infers the users‚Äô intent: they are looking for a smoothie brand. After generation, a post-processing guardrail validates the tags against our catalog. This rigorous process has two critical¬†outputs:</p><ol><li>A low-latency cache containing the validated, high quality tags for our most common¬†queries.</li><li>A high-quality training dataset, which is used to teach a light weight real-time model.</li></ol><h4>The Real-Time System (‚ÄúStudent‚Äù): A Fine-Tuned Model for the Long-Tail</h4><p>When a user‚Äôs query results in a cache miss (indicating a long-tail query), it is routed to our real-time model. This is a language model with a much smaller backbone (like Llama3‚Äì8B) that is fast and cost-effective for live inference.</p><p>Crucially, this model was fine-tuned on the high-quality ‚Äúcurriculum‚Äù dataset produced by our offline ‚Äúteacher‚Äù pipeline. By doing this, the smaller model learns to replicate accuracy of its much larger counterpart, along with the domain context we injected. This allows us to deliver a consistent, high-quality experience for virtually any query a user types. This hybrid approach gives us the best of both worlds: the raw power of massive LLMs, and the speed and efficiency of a lightweight, learnable model.</p><h3>Building a New Foundation: Fine-Tuning for Real-Time Inference</h3><p>The success of the real-time ‚Äústudent‚Äù model in our SRL system was more than just a win for one project; it proved the viability of a new foundational capability for Instacart: <strong>fine-tuning smaller, open-source models to serve our specific needs at¬†scale</strong>.</p><p>While the SRL system was the first production application, the process of building and deploying this model established a blueprint for future innovation across our platform. Here‚Äôs a closer look at how we did¬†it.</p><h3>Distilling Knowledge via Fine-Tuning</h3><p>For the real-time SRL model, we fine-tuned an open-source <strong>Llama-3‚Äì8B</strong> model using LoRA (Low-Rank Adaptation). The model was trained on the dataset from the offline ‚Äúteacher‚Äù pipeline. This process effectively distilled the knowledge and nuanced context from the larger model into the smaller, more efficient one.</p><p>The results were remarkable. Our fine-tuned 8B model performs on par with the much larger frontier model it learned from, achieving a similar F1-score with higher precision.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*WTWYlNwXI73Fm4EC5wzlqQ.png\" /><figcaption><em>Fig 5. Our fine-tuned 8B model achieves performance on par with a much larger foundation model. Compared with baseline (dark blue)¬†,Our production model (orange) has higher precision 96.4% vs 95.4%, lower recall 95% vs 96.2%, and on-par F1 score 95.7% vs¬†95.8%.</em></figcaption></figure><h3>The Path to Production: Taming Real-Time Latency</h3><p>Having a great model is only half the battle; serving it in production with a latency target in the low hundreds of milliseconds was a significant engineering challenge. The out-of-the-box latency was nearly 700ms with A100 GPU. We reduced latency through a series of crucial optimizations:</p><ul><li><strong>Adapter Merging &amp; Hardware Upgrade:</strong> Merging the LoRA adapter weights directly into the base model and upgrading to H100 GPUs got us to our 300ms¬†target.</li><li><strong>Quantization Trade-Offs:</strong> We explored quantization (FP8), which cut latency by another 10% but with a slight drop in recall. We deployed the unquantized model to prioritize quality.</li><li><strong>Cost Management:</strong> We enabled GPU autoscaling to run on less GPUs during off-peak hours, reducing costs without compromising performance.</li></ul><p>A/B testing confirmed the success: the real-time LLM meaningfully improved search quality for the bottom 2% of queries. With the new SRL tagging for the tail queries, we reduce ‚Äúaverage scroll depth‚Äù by 6% (users find items faster), with only a marginal latency increase. The system is now live, serving millions of cold-start queries weekly and reducing user complaints related to poor search results for tail queries by¬†50%.</p><h3>Key Takeaways</h3><p>Here‚Äôs what we learned from putting LLMs into our production search¬†system:</p><ul><li><strong>Context is the Defensible Moat</strong>: A generic LLM is a commodity; your business context is what makes your application defensible, because domain knowledge is the most valuable asset. It‚Äôs vast, noisy, and dynamic. It includes everything from user engagement signals (<em>what products are actually purchased after a search?</em>) to real-world constraints (<em>what‚Äôs on the shelf at a specific store right now?</em>). In the past, injecting this data into traditional ML models was difficult and brittle. The central challenge today is how to effectively encode this knowledge into an LLM. Through our work, we found a clear hierarchy of effectiveness, each with its own engineering trade-offs: <strong>Fine-tuning &gt; Context-Engineering (RAG) &gt; Prompting</strong>. Each method progressively transforms a generalist model into a true domain¬†expert.</li><li><strong>Start Offline, Go Real-Time Strategically</strong>: To manage costs and prove value, we began with an offline LLM pipeline on high-frequency ‚Äúhead‚Äù queries. This cost-effective approach handled the bulk of traffic and generated the data needed to later train a ‚Äústudent‚Äù model for the long¬†tail.</li><li><strong>Consolidate, Don‚Äôt Complicate</strong>: We simplified our stack by replacing numerous legacy models with a single LLM backbone, reducing maintenance and accelerating development.</li><li><strong>The Model is Only Half the Battle</strong>: A great model is useless if it can‚Äôt serve traffic at scale. We turned potential into impact through crucial production engineering: adapter merging cut latency by 30%, smart caching meant only 2% of queries needed real-time inference, and GPU autoscaling managed costs effectively.</li></ul><p>Ultimately, this journey has armed us with more than just a more intelligent QU system; it has laid a new foundation for the future of eCommerce search. Looking ahead, we are expanding beyond single-query search to build a smarter, context-aware system. This means building a system that can understand a user‚Äôs entire journey and distinguish between complex intents‚Ää‚Äî‚Äädifferentiating a search for <em>‚Äúlasagna ingredients‚Äù</em> (item search) from a query for a <em>‚Äúquick lasagna recipe‚Äù</em> (content discovery) or a request for <em>‚Äúlasagna delivery near me‚Äù</em> (restaurant search). By understanding this context, we can guide users to the perfect experience, creating a seamless journey across all of Instacart‚Äôs offerings.</p><p><strong>Acknowledgments</strong></p><p>This project required the collaboration of multiple teams across the company including ML, backend and infra teams to be realized. Special thanks to<strong> Taesik Na, Tina He, Akshay Nair, Xiao Xiao, Mostafa Rashed, Kevin Lei, Callum Wood</strong>, <strong>Sudha Rani Kolavali</strong> and <strong>Jonathan Bender</strong> who also contributed to this work and made this vision a reality. I‚Äôd also like to thank <strong>Naval Shah, Jane Ross </strong>and <strong>Eric Hacke</strong> for their thoughtful and thorough review of the blog¬†post.</p><img alt=\"\" height=\"1\" src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=3ac8051ae7ac\" width=\"1\" /><hr /><p><a href=\"https://tech.instacart.com/building-the-intent-engine-how-instacart-is-revamping-query-understanding-with-llms-3ac8051ae7ac\">Building The Intent Engine: How Instacart is Revamping Query Understanding with LLMs</a> was originally published in <a href=\"https://tech.instacart.com\">tech-at-instacart</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>", "source_url": "https://tech.instacart.com/feed", "article_url": "https://tech.instacart.com/building-the-intent-engine-how-instacart-is-revamping-query-understanding-with-llms-3ac8051ae7ac?source=rss----587883b5d2ee---4", "pub_date": "2025-11-13T23:22:56+00:00", "label": true, "category": "ai_ml", "notes": "heuristic label"}
{"id": "e989e88aaf2b6f0f", "title": "Open Responses: What you need to know", "description": "", "source_url": "https://huggingface.co/blog/feed.xml", "article_url": "https://huggingface.co/blog/open-responses", "pub_date": "2026-01-15T00:00:00+00:00", "label": false, "category": "other", "notes": "heuristic label"}
{"id": "227bad49010eac44", "title": "Fragments Dec 4", "description": "<p><a href=\"https://blog.robbowley.net/2025/12/04/ai-is-still-making-code-worse-a-new-cmu-study-confirms/\">Rob Bowley</a> summarizes a study from Carnegie Mellon looking on the impact of AI on a bunch of open-source software projects. Like any such study, we shouldn‚Äôt take its results as definitive, but there seems enough there to make it a handy data point. The key point is that the AI code probably reduced the quality of the code base - at least if static code analysis can be trusted to determine quality. And perhaps some worrying second-order effects</p>\n\n<blockquote>\n  <p>This study shows more than 800 popular GitHub projects with code quality degrading after adopting AI tools. It‚Äôs hard not to see a form of context collapse playing out in real time. If the public code that future models learn from is becoming more complex and less maintainable, there‚Äôs a real risk that newer models will reinforce and amplify those trends, producing even worse code over time.</p>\n</blockquote>\n\n<p>¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>\n\n<p>Rob‚Äôs post is typical of much of the thoughtful writing on AI. We can see its short-term benefits, but worry about its long-term impact. But on a much deeper note is this lovely story from <a href=\"https://www.linkedin.com/pulse/my-80th-birthday-i-bought-myself-new-brain-jim-highsmith-8qcrc/\">Jim Highsmith</a>. Jim has turned 0x50, and has spent the last decade fighting Parkinson‚Äôs disease. To help him battle it he has two AI assisted allies.</p>\n\n<blockquote>\n  <p>Between my neural implants and Byron‚Äôs digital guidance, I now collaborate with two adaptive systems: one for motion, one for thought. Neither replaces me. Both extend me.</p>\n</blockquote>\n\n<p><strong>If you read anything on AI this week, <a href=\"https://www.linkedin.com/pulse/my-80th-birthday-i-bought-myself-new-brain-jim-highsmith-8qcrc/\">make it be this</a>.</strong> It offers a positive harbinger for our future and opens my mind to a whole different perspective of the role of AI in it</p>\n\n<p>¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>\n\n<p>Anthropic recently announced that it disrupted a Chinese state-sponsored operation abusing Claude Code. Jim Gumbley looks at the core lesson to learn from this, that we have to understand the serious risk of  <a href=\"https://www.thoughtworks.com/insights/blog/security/anthropic-ai-espionage-disclosure-signal-from-noise?utm_source=linkedin&amp;utm_medium=social-organic&amp;utm_campaign=dt_bs_rp-in-clt_tech_thought_leaders_2025-11&amp;gh_src=d1fe17bc1us\">AI Jailbreaking</a></p>\n\n<blockquote>\n  <p>New AI tools are able to analyze your attack surface at the next level of granularity. As a business leader, that means you now have two options: wait for someone else to run AI-assisted vulnerability detection against your attack surface, or run it yourself first.</p>\n</blockquote>\n\n<p>¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>\n\n<p>There‚Äôs plenty of claims that AI Vibe Coding can replace software developers, something that folks like me (perhaps with a bias) think unlikely. <a href=\"https://bsky.app/profile/gergely.pragmaticengineer.com/post/3m75kmb7bh22i\">Gergely Orosz</a> shared this tidbit</p>\n\n<blockquote>\n  <p>Talked with an exec at a tech company who is obsessed with AI and has been for 3 years. Not a developer but company makes software. Uses AI for everything, vibe codes ideas.</p>\n\n  <p>Here‚Äôs the kicker:</p>\n\n  <p>Has a team of several devs to implement his vibe coded prototypes to sg workable</p>\n</blockquote>\n\n<p>I‚Äôd love to hear more about this (and similar stories)</p>\n\n<p>¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>\n\n<p><a href=\"https://checkeagle.com/checklists/njr/a-month-of-chat-oriented-programming/\">Nick Radcliffe</a> writes about a month of using AI</p>\n\n<blockquote>\n  <p>I spent a solid month ‚Äúpair programming‚Äù with Claude Code, trying to suspend disbelief and adopt a this-will-be-productive mindset. More specifically, I got Claude to write well over 99% of the code produced during the month. I found the experience infuriating, unpleasant, and stressful before even worrying about its energy impact. Ideally, I would prefer not to do it again for at least a year or two. The only problem with that is that it ‚Äúworked‚Äù.</p>\n</blockquote>\n\n<p>He stresses that his approach is the ‚Äúpolar opposite‚Äù of Vibe Coding. The post is long, and rambles a bit, but is worthwhile because he talks in detail about his workflow and how he uses the tool. Such posts are important so we can learn the nitty-gritty of how our programming habits are changing.</p>\n\n<p>¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>\n\n<p>Along similar lines is a post of <a href=\"https://brianchambers.substack.com/p/chamber-of-tech-secrets-55-issue\">Brian Chambers</a> on his workflow, that he calls Issue-Driven Development (and yes, I‚Äôm also sick of the ‚Äúsomething-driven‚Äù phraseology). As with much of the better stuff I‚Äôve heard about AI assisted work, it‚Äôs all about carefully managing the context window, ensuring the AI is focused on the right things and not distracted by textual squirrels.</p>", "source_url": "https://martinfowler.com/feed.atom", "article_url": "https://martinfowler.com/articles/20251204-frags.html", "pub_date": "2025-12-04T15:59:00+00:00", "label": false, "category": "other", "notes": "heuristic label"}
{"id": "7c59bff458ca3c2f", "title": "Zenken boosts a lean sales team with ChatGPT Enterprise", "description": "By rolling out ChatGPT Enterprise company-wide, Zenken has boosted sales performance, cut preparation time, and increased proposal success rates. AI-supported workflows are helping a lean team deliver more personalized, effective customer engagement.", "source_url": "https://openai.com/blog/rss.xml", "article_url": "https://openai.com/index/zenken", "pub_date": "2026-01-13T16:00:00+00:00", "label": true, "category": "ai_ml", "notes": "heuristic label"}
{"id": "138ba130e55437db", "title": "Our approach to advertising and expanding access to ChatGPT", "description": "OpenAI plans to test advertising in the U.S. for ChatGPT‚Äôs free and Go tiers to expand affordable access to AI worldwide, while protecting privacy, trust, and answer quality.", "source_url": "https://openai.com/blog/rss.xml", "article_url": "https://openai.com/index/our-approach-to-advertising-and-expanding-access", "pub_date": "2026-01-16T00:00:00+00:00", "label": true, "category": "ai_ml", "notes": "heuristic label"}
{"id": "d5deaa104f8e3203", "title": "Quoting Jasmine Sun", "description": "<blockquote cite=\"https://jasmi.news/p/claude-code\"><p><strong>If you tell a friend they can now instantly create any app, they‚Äôll probably say ‚ÄúCool! Now I need to think of an idea.‚Äù</strong> Then they will forget about it, and never build a thing. The problem is not that your friend is horribly uncreative. It‚Äôs that most people‚Äôs problems are not software-shaped, and most won‚Äôt notice even when they are. [...]</p>\n<p>Programmers are trained to see everything as a software-shaped problem: if you do a task three times, you should probably automate it with a script. <em>Rename every IMG_*.jpg file from the last week to hawaii2025_*.jpg</em>, they tell their terminal, while the rest of us painfully click and copy-paste. We are blind to the solutions we were never taught to see, asking for faster horses and never dreaming of cars.</p></blockquote>\n<p class=\"cite\">&mdash; <a href=\"https://jasmi.news/p/claude-code\">Jasmine Sun</a></p>\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/vibe-coding\">vibe-coding</a>, <a href=\"https://simonwillison.net/tags/coding-agents\">coding-agents</a>, <a href=\"https://simonwillison.net/tags/claude-code\">claude-code</a>, <a href=\"https://simonwillison.net/tags/generative-ai\">generative-ai</a>, <a href=\"https://simonwillison.net/tags/ai\">ai</a>, <a href=\"https://simonwillison.net/tags/llms\">llms</a></p>", "source_url": "https://simonwillison.net/atom/everything/", "article_url": "https://simonwillison.net/2026/Jan/24/jasmine-sun/#atom-everything", "pub_date": "2026-01-24T21:34:35+00:00", "label": false, "category": "other", "notes": "heuristic label"}
{"id": "4e7c353bb20e9625", "title": "DeepMath: A lightweight math reasoning Agent with smolagents", "description": "", "source_url": "https://huggingface.co/blog/feed.xml", "article_url": "https://huggingface.co/blog/intel-deepmath", "pub_date": "2025-12-04T00:00:00+00:00", "label": true, "category": "ai_ml", "notes": "heuristic label"}
{"id": "dee4a4d879de8dfb", "title": "Introducing Waypoint-1: Real-time interactive video diffusion from Overworld", "description": "", "source_url": "https://huggingface.co/blog/feed.xml", "article_url": "https://huggingface.co/blog/waypoint-1", "pub_date": "2026-01-20T00:00:00+00:00", "label": true, "category": "ai_ml", "notes": "heuristic label"}
{"id": "a803fa6781f6e3fa", "title": "Building for an Open Future - our new partnership with Google Cloud", "description": "", "source_url": "https://huggingface.co/blog/feed.xml", "article_url": "https://huggingface.co/blog/google-cloud", "pub_date": "2025-11-13T00:00:00+00:00", "label": true, "category": "technology", "notes": "heuristic label"}
{"id": "229e8d4830ad3878", "title": "Bringing powerful AI to millions across Europe with Deutsche Telekom", "description": "OpenAI is collaborating with Deutsche Telekom to bring advanced, multilingual AI experiences to millions of people across Europe. ChatGPT Enterprise will also be deployed to help employees at Deutsche Telekom improve workflows and accelerate innovation.", "source_url": "https://openai.com/blog/rss.xml", "article_url": "https://openai.com/index/deutsche-telekom-collaboration", "pub_date": "2025-12-09T00:00:00+00:00", "label": true, "category": "policy", "notes": "heuristic label"}
{"id": "741de1265ef15cbc", "title": "Anthropic invests $1.5 million in the Python Software Foundation and open source security", "description": "<p><strong><a href=\"https://pyfound.blogspot.com/2025/12/anthropic-invests-in-python.html?m=1\">Anthropic invests $1.5 million in the Python Software Foundation and open source security</a></strong></p>\nThis is outstanding news, especially given our decision to withdraw from that NSF grant application <a href=\"https://simonwillison.net/2025/Oct/27/psf-withdrawn-proposal/\">back in October</a>.</p>\n<blockquote>\n<p>We are thrilled to announce that Anthropic has entered into a two-year partnership with the Python Software Foundation (PSF) to contribute a landmark total of $1.5 million to support the foundation‚Äôs work, with an emphasis on Python ecosystem security. This investment will enable the PSF to make crucial security advances to CPython and the Python Package Index (PyPI) benefiting all users, and it will also sustain the foundation‚Äôs core work supporting the Python language, ecosystem, and global community.</p>\n</blockquote>\n<p>Note that while security is a focus these funds will also support other aspects of the PSF's work:</p>\n<blockquote>\n<p>Anthropic‚Äôs support will also go towards the PSF‚Äôs core work, including the Developer in Residence program driving contributions to CPython, community support through grants and other programs, running core infrastructure such as PyPI, and more.</p>\n</blockquote>\n\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/open-source\">open-source</a>, <a href=\"https://simonwillison.net/tags/python\">python</a>, <a href=\"https://simonwillison.net/tags/ai\">ai</a>, <a href=\"https://simonwillison.net/tags/psf\">psf</a>, <a href=\"https://simonwillison.net/tags/anthropic\">anthropic</a></p>", "source_url": "https://simonwillison.net/atom/everything/", "article_url": "https://simonwillison.net/2026/Jan/13/anthropic-invests-15-million-in-the-python-software-foundation-a/#atom-everything", "pub_date": "2026-01-13T23:58:17+00:00", "label": true, "category": "security", "notes": "heuristic label"}
{"id": "4f5651c61ebda59d", "title": "Inside GPT-5 for Work: How Businesses Use GPT-5", "description": "A data-driven report on how workers across industries use ChatGPT‚Äîcovering adoption trends, top tasks, departmental patterns, and the future of AI at work.", "source_url": "https://openai.com/blog/rss.xml", "article_url": "https://openai.com/business/guides-and-resources/chatgpt-usage-and-adoption-patterns-at-work", "pub_date": "2026-01-22T00:00:00+00:00", "label": true, "category": "ai_ml", "notes": "heuristic label"}
{"id": "28f07cb35dc46ecd", "title": "AI for self empowerment", "description": "How AI can expand human agency by closing the capability overhang‚Äîhelping people, businesses, and countries unlock real productivity, growth, and opportunity.", "source_url": "https://openai.com/blog/rss.xml", "article_url": "https://openai.com/index/ai-for-self-empowerment", "pub_date": "2026-01-18T12:00:00+00:00", "label": true, "category": "ai_ml", "notes": "heuristic label"}
{"id": "fc471a46aa53e8b9", "title": "Evaluating AI‚Äôs ability to perform scientific research tasks", "description": "OpenAI introduces FrontierScience, a benchmark testing AI reasoning in physics, chemistry, and biology to measure progress toward real scientific research.", "source_url": "https://openai.com/blog/rss.xml", "article_url": "https://openai.com/index/frontierscience", "pub_date": "2025-12-16T09:00:00+00:00", "label": true, "category": "ai_ml", "notes": "heuristic label"}
{"id": "822840c69cd95145", "title": "Fragments: January 22", "description": "<p>My colleagues here at Thoughtworks have announced <a href=\"https://www.thoughtworks.com/ai/works\">AI/works‚Ñ¢</a>, a platform for our work using AI-enabled software development. The platform is in its early days, and is currently intended to support Thoughtworks consultants in their client work. I‚Äôm looking forward to sharing what we learn from using and further developing the platform in future months.</p>\n\n<p>¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>\n\n<p>Simon Couch <a href=\"https://www.simonpcouch.com/blog/2026-01-20-cc-impact/\">examines the electricity consumption</a> of using AI. He‚Äôs a heavy user: ‚Äúusually programming for a few hours, and driving 2 or 3 Claude Code instances at a time‚Äù. He finds his usage of electricity is orders of magnitude more than typical estimates based on the ‚Äútypical query‚Äù.</p>\n\n<blockquote>\n  <p>On a median day, I estimate I consume 1,300 Wh through Claude Code‚Äî4,400 ‚Äútypical queries‚Äù worth.</p>\n</blockquote>\n\n<p>But it‚Äôs still not a massive amount of power - similar to that of running a dishwasher.</p>\n\n<p>A caveat to this is that this is ‚Äúnapkin math‚Äù because we don‚Äôt have decent data about how these models use resources. I agree with him that we ought to.</p>\n\n<p>¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>\n\n<p>My namesake Chad Fowler (no relation) considers that the movement to agentic coding creates a similar <a href=\"https://aicoding.leaflet.pub/3mbrvhyye4k2e\">shift in rigor and discipline</a> as appeared in Extreme Programming, dynamic languages, and continuous deployment.</p>\n\n<p>In Extreme Programming‚Äôs case, this meant a lot of discipline around testing, continuous integration, and keeping the code-base healthy. My current view is that with AI-enabled development we need to be rigorous about evaluating the software, both for its observable behavior and its internal quality.</p>\n\n<blockquote>\n  <p>The engineers who thrive in this environment will be the ones who relocate discipline rather than abandon it. They‚Äôll treat generation as a capability that demands more precision in specification, not less. They‚Äôll build evaluation systems that are harder to fool than the ones they replaced. They‚Äôll refuse the temptation to mistake velocity for progress.</p>\n</blockquote>\n\n<p>¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>\n\n<p>There‚Äôs been much written about the dreadful events in Minnesota, and I‚Äôve not felt I‚Äôve had anything useful to add to them. But I do want to pass on an excellent post from <a href=\"https://www.noahpinion.blog/p/why-are-federal-agents-gunning-down\">Noah Smith</a> that captures many of my thoughts. He points out that there is a ‚Äúconsistent record of brutality, aggression, dubious legality, and unprofessionalism‚Äù from ICE (and CBP) who seem to be turning into MAGA‚Äôs <a href=\"https://en.wikipedia.org/wiki/Sturmabteilung\">SD</a>.</p>\n\n<blockquote>\n  <p>Is this America now? A country where unaccountable and poorly trained government agents go door to door, arresting and beating people on pure suspicion, and shooting people who don‚Äôt obey their every order or who try to get away? ‚ÄúWhen a federal officer gives you instructions, you abide by them and then you get to keep your life‚Äù is a perfect description of an authoritarian police state. None of this is Constitutional, every bit of it is deeply antithetical to the American values we grew up taking for granted.</p>\n</blockquote>\n\n<p>My worries about these kinds of developments were what animated me to urge against voting for Trump in the <a href=\"https://martinfowler.com/articles/vote-against-trump.html\">2016 election</a>. Mostly those worries didn‚Äôt come to fruition because enough constitutional Republicans were in a position to stop them from happening, so even when Trump attempted a coup in 2020, he wasn‚Äôt able to get very far. But now those constitutional Republicans are absent or quiescent. I fear that what we‚Äôve seen in Minneapolis will be a harbinger of worse to come.</p>\n\n<p>I also second John Gruber‚Äôs <a href=\"https://daringfireball.net/2026/01/lets_call_a_murder_a_murder\">praise of bystander Caitlin Callenson</a>:</p>\n\n<blockquote>\n  <p>But then, after the murderous agent fired three shots‚Äâ‚Äî‚Äâjust 30 or 40 feet in front of Callenson‚Äâ‚Äî‚ÄâCallenson had the courage and conviction to stay with the scene and keep filming. Not to run away, but instead to follow the scene. To keep filming. To continue documenting with as best clarity as she could, what was unfolding.</p>\n</blockquote>\n\n<p>The recent activity in  Venezuala reminds me that I‚Äôve long felt that Trump is a Hugo Ch√°vez figure - a charismatic populist who‚Äôs keen on wrecking institutions and norms. Trump is old, so won‚Äôt be with us for that much longer - but the question is: ‚Äúwho is Trump‚Äôs Maduro?‚Äù</p>\n\n<p>¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>\n\n<p>With all the drama at home, we shouldn‚Äôt ignore the terrible things that happened in Iran. The people there again suffered again the consequences of an entrenched authoritarian police state.</p>", "source_url": "https://martinfowler.com/feed.atom", "article_url": "https://martinfowler.com/fragments/2026-01-22.html", "pub_date": "2026-01-22T14:30:00+00:00", "label": false, "category": "other", "notes": "manual: Newsletter roundup; low impact."}
{"id": "2d7083881d5b227c", "title": "The truth left out from Elon Musk‚Äôs recent court filing", "description": "The truth left out from Elon Musk‚Äôs recent court filing.", "source_url": "https://openai.com/blog/rss.xml", "article_url": "https://openai.com/index/the-truth-elon-left-out", "pub_date": "2026-01-16T12:00:00+00:00", "label": false, "category": "other", "notes": "heuristic label"}
{"id": "52a99a1ebbb1f932", "title": "Conversation: LLMs and the what/how loop", "description": "<p>A conversation between <b class=\"author\">Unmesh Joshi</b>, <b class=\"author\">Rebecca\n      Parsons</b>, and <b class=\"author\">Martin Fowler</b> on how LLMs help us\n      shape the abstractions in our software. We view our challenge as building\n      systems that survive change, requiring us to manage our cognitive load. We\n      can do this by mapping the &#x201c;what&#x201d; of we want our software to do into the\n      &#x201c;how&#x201d; of programming languages. This &#x201c;what&#x201d; and &#x201c;how&#x201d; are built up in a\n      feedback loop. TDD helps us operationalize that loop, and LLMs allow us to\n      explore that loop in an informal and more fluid manner.</p>\n\n<p><a class=\"more\" href=\"https://martinfowler.com/articles/convo-what-how.html\">more‚Ä¶</a></p>", "source_url": "https://martinfowler.com/feed.atom", "article_url": "https://martinfowler.com/articles/convo-what-how.html", "pub_date": "2026-01-21T14:40:00+00:00", "label": true, "category": "ai_ml", "notes": "heuristic label"}
{"id": "9336d3eba565cf62", "title": "Don't \"Trust the Process\"", "description": "<p><strong><a href=\"https://www.youtube.com/watch?v=4u94juYwLLM\">Don&#x27;t &quot;Trust the Process&quot;</a></strong></p>\nJenny Wen, Design Lead at Anthropic (and previously Director of Design at Figma) gave a provocative keynote at Hatch Conference in Berlin last September.</p>\n<p><img alt=\"Don't &quot;Trust the process&quot; slide, speaker shown on the left\" src=\"https://static.simonwillison.net/static/2026/dont-trust-process.jpg\" /></p>\n<p>Jenny argues that the Design Process - user research leading to personas leading to user journeys leading to wireframes... all before anything gets built - may be outdated for today's world.</p>\n<blockquote>\n<p><strong>Hypothesis</strong>: In a world where anyone can make anything ‚Äî what matters is your ability to choose and curate what you make.</p>\n</blockquote>\n<p>In place of the Process, designers should lean into prototypes. AI makes these much more accessible and less time-consuming than they used to be.</p>\n<p>Watching this talk made me think about how AI-assisted programming significantly reduces the cost of building the <em>wrong</em> thing. Previously if the design wasn't right you could waste months of development time building in the wrong direction, which was a very expensive mistake. If a wrong direction wastes just a few days instead we can take more risks and be much more proactive in exploring the problem space.</p>\n<p>I've always been a compulsive prototyper though, so this is very much playing into my own existing biases!\n\n    <p><small></small>Via <a href=\"https://twitter.com/jenny_wen/status/2014479445738893649\">@jenny_wen</a></small></p>\n\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/design\">design</a>, <a href=\"https://simonwillison.net/tags/prototyping\">prototyping</a>, <a href=\"https://simonwillison.net/tags/ai\">ai</a>, <a href=\"https://simonwillison.net/tags/generative-ai\">generative-ai</a>, <a href=\"https://simonwillison.net/tags/llms\">llms</a>, <a href=\"https://simonwillison.net/tags/ai-assisted-programming\">ai-assisted-programming</a>, <a href=\"https://simonwillison.net/tags/vibe-coding\">vibe-coding</a></p>", "source_url": "https://simonwillison.net/atom/everything/", "article_url": "https://simonwillison.net/2026/Jan/24/dont-trust-the-process/#atom-everything", "pub_date": "2026-01-24T23:31:03+00:00", "label": false, "category": "other", "notes": "manual: Opinion-style blog post."}
{"id": "88e39f15677c8138", "title": "Claude's new constitution", "description": "<p><strong><a href=\"https://www.anthropic.com/news/claude-new-constitution\">Claude&#x27;s new constitution</a></strong></p>\nLate last year Richard Weiss <a href=\"https://www.lesswrong.com/posts/vpNG99GhbBoLov9og/claude-4-5-opus-soul-document\">found something interesting</a> while poking around with the just-released Claude Opus 4.5: he was able to talk the model into regurgitating a document which was <em>not</em> part of the system prompt but appeared instead to be baked in during training, and which described Claude's core values at great length.</p>\n<p>He called this leak the <strong>soul document</strong>, and Amanda Askell from Anthropic <a href=\"https://simonwillison.net/2025/Dec/2/claude-soul-document/\">quickly confirmed</a> that it was indeed part of Claude's training procedures.</p>\n<p>Today Anthropic made this official, <a href=\"https://www.anthropic.com/news/claude-new-constitution\">releasing that full \"constitution\" document</a> under a CC0 (effectively public domain) license. There's a lot to absorb! It's over 35,000 tokens, more than 10x the length of the <a href=\"https://platform.claude.com/docs/en/release-notes/system-prompts#claude-opus-4-5\">published Opus 4.5 system prompt</a>.</p>\n<p>One detail that caught my eye is the acknowledgements at the end, which include a list of <a href=\"https://www.anthropic.com/constitution#acknowledgements\">external contributors</a> who helped review the document. I was intrigued to note that two of the fifteen listed names are Catholic members of the clergy - <a href=\"https://www.frbrendanmcguire.org/biography\">Father Brendan McGuire</a> is a pastor in Los Altos with a Master‚Äôs degree in Computer Science and Math and <a href=\"https://en.wikipedia.org/wiki/Paul_Tighe\">Bishop Paul Tighe</a> is an Irish Catholic bishop with a background in moral theology.\n\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/ai\">ai</a>, <a href=\"https://simonwillison.net/tags/generative-ai\">generative-ai</a>, <a href=\"https://simonwillison.net/tags/llms\">llms</a>, <a href=\"https://simonwillison.net/tags/anthropic\">anthropic</a>, <a href=\"https://simonwillison.net/tags/claude\">claude</a>, <a href=\"https://simonwillison.net/tags/amanda-askell\">amanda-askell</a>, <a href=\"https://simonwillison.net/tags/ai-ethics\">ai-ethics</a>, <a href=\"https://simonwillison.net/tags/ai-personality\">ai-personality</a></p>", "source_url": "https://simonwillison.net/atom/everything/", "article_url": "https://simonwillison.net/2026/Jan/21/claudes-new-constitution/#atom-everything", "pub_date": "2026-01-21T23:39:49+00:00", "label": true, "category": "ai_ml", "notes": "manual: Major model governance/policy update."}
{"id": "d5d963b8c07abecb", "title": "jordanhubbard/nanolang", "description": "<p><strong><a href=\"https://github.com/jordanhubbard/nanolang\">jordanhubbard/nanolang</a></strong></p>\nPlenty of people have mused about what a new programming language specifically designed to be used by LLMs might look like. Jordan Hubbard (<a href=\"https://en.wikipedia.org/wiki/Jordan_Hubbard\">co-founder of FreeBSD</a>, with serious stints at Apple and NVIDIA) just released exactly that.</p>\n<blockquote>\n<p>A minimal, LLM-friendly programming language with mandatory testing and unambiguous syntax.</p>\n<p>NanoLang transpiles to C for native performance while providing a clean, modern syntax optimized for both human readability and AI code generation.</p>\n</blockquote>\n<p>The syntax strikes me as an interesting mix between C, Lisp and Rust.</p>\n<p>I decided to see if an LLM could produce working code in it directly, given the necessary context. I started with this <a href=\"https://github.com/jordanhubbard/nanolang/blob/main/MEMORY.md\">MEMORY.md</a> file, which begins:</p>\n<blockquote>\n<p><strong>Purpose:</strong> This file is designed specifically for Large Language Model consumption. It contains the essential knowledge needed to generate, debug, and understand NanoLang code. Pair this with <code>spec.json</code> for complete language coverage.</p>\n</blockquote>\n<p>I ran that using <a href=\"https://llm.datasette.io/\">LLM</a> and <a href=\"https://github.com/simonw/llm-anthropic\">llm-anthropic</a> like this:</p>\n<pre><code>llm -m claude-opus-4.5 \\\n  -s https://raw.githubusercontent.com/jordanhubbard/nanolang/refs/heads/main/MEMORY.md \\\n  'Build me a mandelbrot fractal CLI tool in this language' \n  &gt; /tmp/fractal.nano\n</code></pre>\n<p>The <a href=\"https://gist.github.com/simonw/7847f022566d11629ec2139f1d109fb8#mandelbrot-fractal-cli-tool-in-nano\">resulting code</a>... <a href=\"https://gist.github.com/simonw/7847f022566d11629ec2139f1d109fb8?permalink_comment_id=5947465#gistcomment-5947465\">did not compile</a>.</p>\n<p>I may have been too optimistic expecting a one-shot working program for a new language like this. So I ran a clone of the actual project, copied in my program and had Claude Code take a look at the failing compiler output.</p>\n<p>... and it worked! Claude happily grepped its way through the various <code>examples/</code> and built me a working program.</p>\n<p>Here's <a href=\"https://gisthost.github.io/?9696da6882cb6596be6a9d5196e8a7a5/index.html\">the Claude Code transcript</a> - you can see it <a href=\"https://gisthost.github.io/?9696da6882cb6596be6a9d5196e8a7a5/page-001.html#msg-2026-01-19T23-43-09-675Z\">reading relevant examples here</a> - and here's <a href=\"https://gist.github.com/simonw/e7f3577adcfd392ab7fa23b1295d00f2\">the finished code plus its output</a>.</p>\n<p>I've suspected <a href=\"https://simonwillison.net/2025/Nov/7/llms-for-new-programming-languages/\">for a while</a> that LLMs and coding agents might significantly reduce the friction involved in launching a new language. This result reinforces my opinion.\n\n    <p><small></small>Via <a href=\"https://news.ycombinator.com/item?id=46684958\">Hacker News</a></small></p>\n\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/programming-languages\">programming-languages</a>, <a href=\"https://simonwillison.net/tags/ai\">ai</a>, <a href=\"https://simonwillison.net/tags/generative-ai\">generative-ai</a>, <a href=\"https://simonwillison.net/tags/llms\">llms</a>, <a href=\"https://simonwillison.net/tags/ai-assisted-programming\">ai-assisted-programming</a>, <a href=\"https://simonwillison.net/tags/llm\">llm</a>, <a href=\"https://simonwillison.net/tags/coding-agents\">coding-agents</a>, <a href=\"https://simonwillison.net/tags/claude-code\">claude-code</a></p>", "source_url": "https://simonwillison.net/atom/everything/", "article_url": "https://simonwillison.net/2026/Jan/19/nanolang/#atom-everything", "pub_date": "2026-01-19T23:58:56+00:00", "label": false, "category": "other", "notes": "manual: Small developer tool release."}
{"id": "233ce97d697e5b25", "title": "Scaling long-running autonomous coding", "description": "<p><strong><a href=\"https://cursor.com/blog/scaling-agents\">Scaling long-running autonomous coding</a></strong></p>\nWilson Lin at Cursor has been doing some experiments to see how far you can push a large fleet of \"autonomous\" coding agents:</p>\n<blockquote>\n<p>This post describes what we've learned from running hundreds of concurrent agents on a single project, coordinating their work, and watching them write over a million lines of code and trillions of tokens.</p>\n</blockquote>\n<p>They ended up running planners and sub-planners to create tasks, then having workers execute on those tasks - similar to how Claude Code uses sub-agents. Each cycle ended with a judge agent deciding if the project was completed or not.</p>\n<p>In my predictions for 2026 <a href=\"https://simonwillison.net/2026/Jan/8/llm-predictions-for-2026/#3-years-someone-will-build-a-new-browser-using-mainly-ai-assisted-coding-and-it-won-t-even-be-a-surprise\">the other day</a> I said that by 2029:</p>\n<blockquote>\n<p>I think somebody will have built a full web browser mostly using AI assistance, and it won‚Äôt even be surprising. Rolling a new web browser is one of the most complicated software projects I can imagine[...] the cheat code is the conformance suites. If there are existing tests that it‚Äôll get so much easier.</p>\n</blockquote>\n<p>I may have been off by three years, because Cursor chose \"building a web browser from scratch\" as their test case for their agent swarm approach:</p>\n<blockquote>\n<p>To test this system, we pointed it at an ambitious goal: building a web browser from scratch. The agents ran for close to a week, writing over 1 million lines of code across 1,000 files. You can explore <a href=\"https://github.com/wilsonzlin/fastrender\">the source code on GitHub</a>.</p>\n</blockquote>\n<p>But how well did they do? Their initial announcement a couple of days ago was met with <a href=\"https://embedding-shapes.github.io/cursor-implied-success-without-evidence/\">unsurprising skepticism</a>, especially when it became apparent that their GitHub Actions CI was failing and there were no build instructions in the repo.</p>\n<p>It looks like they addressed that within the past 24 hours. The <a href=\"https://github.com/wilsonzlin/fastrender/blob/main/README.md#build-requirements\">latest README</a> includes build instructions which I followed on macOS like this:</p>\n<pre><code>cd /tmp\ngit clone https://github.com/wilsonzlin/fastrender\ncd fastrender\ngit submodule update --init vendor/ecma-rs\ncargo run --release --features browser_ui --bin browser\n</code></pre>\n<p>This got me a working browser window! Here are screenshots I took of google.com and my own website:</p>\n<p><img alt=\"The browser chrome is neat but has a garbled tab name at the top. The Google homepage looks mostly correct but the buttons are not styled correctly and the Google Search one has a huge plus icon floating near it.\" src=\"https://static.simonwillison.net/static/2026/cursor-google.png\" /></p>\n<p><img alt=\"My blog looks mostly correct, but the right closing quotation mark on a quotation (which is implemented as a background image on the final paragraph) is displayed incorrectly multiple times.\" src=\"https://static.simonwillison.net/static/2026/cursor-simonwillison.jpg\" /></p>\n<p>Honestly those are very impressive! You can tell they're not just wrapping an existing rendering engine because of those very obvious rendering glitches, but the pages are legible and look mostly correct.</p>\n<p>The FastRender repo even uses Git submodules <a href=\"https://github.com/wilsonzlin/fastrender/tree/main/specs\">to include various WhatWG and CSS-WG specifications</a> in the repo, which is a smart way to make sure the agents have access to the reference materials that they might need.</p>\n<p>This is the second attempt I've seen at building a full web browser using AI-assisted coding in the past two weeks - the first was <a href=\"https://github.com/hiwavebrowser/hiwave\">HiWave browser</a>, a new browser engine in Rust first announced <a href=\"https://www.reddit.com/r/Anthropic/comments/1q4xfm0/over_christmas_break_i_wrote_a_fully_functional/\">in this Reddit thread</a>.</p>\n<p>When I made my 2029 prediction this is more-or-less the quality of result I had in mind. I don't think we'll see projects of this nature compete with Chrome or Firefox or WebKit any time soon but I have to admit I'm very surprised to see something this capable emerge so quickly.</p>\n<p><strong>Update 23rd January 2026</strong>: I recorded a 47 minute conversation with Wilson about this project and published it on YouTube. Here's <a href=\"https://simonwillison.net/2026/Jan/23/fastrender/\">the video and accompanying highlights</a>.\n\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/browsers\">browsers</a>, <a href=\"https://simonwillison.net/tags/ai\">ai</a>, <a href=\"https://simonwillison.net/tags/generative-ai\">generative-ai</a>, <a href=\"https://simonwillison.net/tags/llms\">llms</a>, <a href=\"https://simonwillison.net/tags/ai-assisted-programming\">ai-assisted-programming</a>, <a href=\"https://simonwillison.net/tags/coding-agents\">coding-agents</a>, <a href=\"https://simonwillison.net/tags/cursor\">cursor</a>, <a href=\"https://simonwillison.net/tags/parallel-agents\">parallel-agents</a>, <a href=\"https://simonwillison.net/tags/conformance-suites\">conformance-suites</a></p>", "source_url": "https://simonwillison.net/atom/everything/", "article_url": "https://simonwillison.net/2026/Jan/19/scaling-long-running-autonomous-coding/#atom-everything", "pub_date": "2026-01-19T05:12:51+00:00", "label": true, "category": "ai_ml", "notes": "manual: Agent scaling techniques with broader impact."}
{"id": "653e579ee89926a3", "title": "The Pulse #160: Why it‚Äôs so dramatic that ‚Äúwriting code by hand is dead‚Äù", "description": "Also: New trend of staff+ engineers and managers using AI a lot, replacing a $120/year micro-SaaS with LLM-generated code, and more", "source_url": "https://newsletter.pragmaticengineer.com/feed", "article_url": "https://newsletter.pragmaticengineer.com/p/the-pulse-160-why-its-so-dramatic", "pub_date": "2026-01-22T18:02:45+00:00", "label": true, "category": "ai_ml", "notes": "manual: AI-focused column with industry implications."}
{"id": "afbe4a42a40fda04", "title": "How AWS S3 is built", "description": "A behind-the-scenes look at how Amazon S3 is designed for durability and correctness at massive scale, drawing on over a decade of operating one of the world&#8217;s largest distributed systems with Mai-Lan Tomsen Bukovec at AWS.", "source_url": "https://newsletter.pragmaticengineer.com/feed", "article_url": "https://newsletter.pragmaticengineer.com/p/how-aws-s3-is-built", "pub_date": "2026-01-21T18:00:17+00:00", "label": true, "category": "technology", "notes": "manual: Infrastructure deep dive with broad relevance."}
{"id": "fceb519afc684fbd", "title": "Scaling PostgreSQL to power 800 million ChatGPT users", "description": "An inside look at how OpenAI scaled PostgreSQL to millions of queries per second using replicas, caching, rate limiting, and workload isolation.", "source_url": "https://openai.com/blog/rss.xml", "article_url": "https://openai.com/index/scaling-postgresql", "pub_date": "2026-01-22T12:00:00+00:00", "label": true, "category": "technology", "notes": "manual: Scaling database infrastructure for major product."}
{"id": "d20341d7197c1657", "title": "Inside Praktika's conversational approach to language learning", "description": "How Praktika uses GPT-4.1 and GPT-5.2 to build adaptive AI tutors that personalize lessons, track progress, and help learners achieve real-world language fluency", "source_url": "https://openai.com/blog/rss.xml", "article_url": "https://openai.com/index/praktika", "pub_date": "2026-01-22T05:00:00+00:00", "label": false, "category": "other", "notes": "manual: Customer case study; promotional."}
{"id": "0534826e5c0c23d9", "title": "Route leak incident on January 22, 2026", "description": "An automated routing policy configuration error caused us to leak some Border Gateway Protocol prefixes unintentionally from a router at our Miami data center. We discuss the impact and the changes we are implementing as a result.", "source_url": "https://blog.cloudflare.com/rss/", "article_url": "https://blog.cloudflare.com/route-leak-incident-january-22-2026/", "pub_date": "2026-01-23T14:00:00+00:00", "label": true, "category": "security", "notes": "manual: Internet infrastructure incident report."}
{"id": "9684d9b80c73f34b", "title": "How we mitigated a vulnerability in Cloudflare‚Äôs ACME validation logic", "description": "A vulnerability was recently identified in Cloudflare‚Äôs automation of certificate validation. Here we explain the vulnerability and outline the steps we‚Äôve taken to mitigate it.", "source_url": "https://blog.cloudflare.com/rss/", "article_url": "https://blog.cloudflare.com/acme-path-vulnerability/", "pub_date": "2026-01-19T14:00:00+00:00", "label": true, "category": "security", "notes": "manual: Security vulnerability mitigation details."}
